
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="tvm-tensorIR.html">
      
      
        <link rel="next" href="tvm-autotvm.html">
      
      <link rel="icon" href="../images/favicon.ico">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.11">
    
    
      
        <title>tvm-lowering - Cat538's Blog</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.85bb2934.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.a6bdf11c.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#tvm-lowering" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="Cat538&#39;s Blog" class="md-header__button md-logo" aria-label="Cat538's Blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Cat538's Blog
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              tvm-lowering
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
          
            
            
            
            <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
            
              <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
              </label>
            
          
            
            
            
            <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
            
              <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
              </label>
            
          
        </form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="Cat538&#39;s Blog" class="md-nav__button md-logo" aria-label="Cat538's Blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Cat538's Blog
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        Prologue
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../plan.html" class="md-nav__link">
        Exercise
      </a>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Cpp
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Cpp
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/toolchain.html" class="md-nav__link">
        toolchain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/move.html" class="md-nav__link">
        move
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/align.html" class="md-nav__link">
        align
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/ranges.html" class="md-nav__link">
        ranges
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/benchmark.html" class="md-nav__link">
        benchmark
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/profiler.html" class="md-nav__link">
        profiler
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/IO-multiplexing.html" class="md-nav__link">
        IO-multiplexing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/epoll.html" class="md-nav__link">
        epoll
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/signal.html" class="md-nav__link">
        signal
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../cpp/fork-safe.html" class="md-nav__link">
        fork-safe
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          Rust
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Rust
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rust/rust-toolchain.html" class="md-nav__link">
        rust-toolchain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rust/rust-study.html" class="md-nav__link">
        rust-study
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rust/rust-attributes.html" class="md-nav__link">
        rust-attributes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rust/rust-modules.html" class="md-nav__link">
        rust-modules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rust/rust-macro.html" class="md-nav__link">
        rust-macro
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rust/rust-network.html" class="md-nav__link">
        rust-network
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rust/rust-option.html" class="md-nav__link">
        rust-optin
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../rust/rust-str.html" class="md-nav__link">
        rust-str
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Tools
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Tools
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tools/cmake.html" class="md-nav__link">
        cmake
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tools/docker.html" class="md-nav__link">
        docker
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tools/git.html" class="md-nav__link">
        git
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tools/powershell.html" class="md-nav__link">
        powershell
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tools/regex.html" class="md-nav__link">
        regex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../tools/mkdocs.html" class="md-nav__link">
        mkdocs
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Courses
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Courses
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../courses/%E5%AF%86%E7%A0%81%E5%88%86%E6%9E%90%E5%AD%A6%E5%A4%8D%E4%B9%A0.html" class="md-nav__link">
        密码分析学
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../courses/%E5%AF%86%E7%A0%81%E5%AD%A6%E5%BC%95%E8%AE%BA%E5%A4%8D%E4%B9%A0.html" class="md-nav__link">
        密码学引论
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../courses/algorithm.html" class="md-nav__link">
        算法设计与分析
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../courses/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86.html" class="md-nav__link">
        编译原理
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          Crypto
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Crypto
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../crypto/ecdsa-pk-recovery.html" class="md-nav__link">
        ecdsa-pk-recovery
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../crypto/algo-rsa.html" class="md-nav__link">
        algo-rsa
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../crypto/algo-fft%26ntt.html" class="md-nav__link">
        algo-fft&ntt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../crypto/algo-reduction.html" class="md-nav__link">
        algo-reduction
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          Mlc
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Mlc
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="dlc-survey.html" class="md-nav__link">
        dlc-survey
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="tvm-install.html" class="md-nav__link">
        tvm-install
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="tvm-ffi.html" class="md-nav__link">
        tvm-ffi
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="tvm-type.html" class="md-nav__link">
        tvm-type
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="tvm-tensorIR.html" class="md-nav__link">
        tvm-tensorIR
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          tvm-lowering
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="tvm-lowering.html" class="md-nav__link md-nav__link--active">
        tvm-lowering
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-lower-tir" class="md-nav__link">
    1. Lower Tir
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-lower-te" class="md-nav__link">
    2. Lower TE
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-lower-relay" class="md-nav__link">
    3. Lower Relay
  </a>
  
    <nav class="md-nav" aria-label="3. Lower Relay">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-topi" class="md-nav__link">
    3.1. TOPI
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-relay-graphexecutor" class="md-nav__link">
    3.2. Relay-GraphExecutor
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-lower-relax" class="md-nav__link">
    4. Lower Relax
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="tvm-autotvm.html" class="md-nav__link">
        tvm-autotvm
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="tvm-ansor.html" class="md-nav__link">
        tvm-ansor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="mlir.html" class="md-nav__link">
        mlir
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="mlir-toy.html" class="md-nav__link">
        mlir-toy
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="mlir-onnx.html" class="md-nav__link">
        mlir-onnx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="mlir-iree.html" class="md-nav__link">
        mlir-iree
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="paper-torch.fx.html" class="md-nav__link">
        paper-torch.fx
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="paper-nimble.html" class="md-nav__link">
        paper-nimble
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="paper-disc.html" class="md-nav__link">
        paper-disc
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="paper-cortex.html" class="md-nav__link">
        paper-cortex
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="paper-freetensor.html" class="md-nav__link">
        paper-freetensor
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../about.html" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-lower-tir" class="md-nav__link">
    1. Lower Tir
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-lower-te" class="md-nav__link">
    2. Lower TE
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-lower-relay" class="md-nav__link">
    3. Lower Relay
  </a>
  
    <nav class="md-nav" aria-label="3. Lower Relay">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-topi" class="md-nav__link">
    3.1. TOPI
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-relay-graphexecutor" class="md-nav__link">
    3.2. Relay-GraphExecutor
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-lower-relax" class="md-nav__link">
    4. Lower Relax
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="tvm-lowering">TVM-lowering</h1>
<blockquote>
<p>Ref:</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/533161438">TVM 自底向上（二）：TIR 的概念和编译原理</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&amp;mid=2247494373&amp;idx=1&amp;sn=6eb14998e8cfe45a144ba1b8c61346ac&amp;chksm=9f835073a8f4d96555ed6382f25d8b5793e7b56da0f403867b7a38c48bbeedc23701c2e59721&amp;scene=178&amp;cur_album_id=1799364124980609027#rd">【从零开始学深度学习编译器】四，解析TVM算子</a></li>
<li><a href="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/%E3%80%90%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E3%80%91%E5%85%AD%EF%BC%8CTVM%E7%9A%84%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3/">【从零开始学深度学习编译器】六，TVM的编译流程详解</a></li>
<li><a href="https://mp.weixin.qq.com/s/NM5yvxW2JSbR06RmrR3ubw">TVM 学习指南（个人版）</a></li>
</ul>
</blockquote>
<h2 id="1-lower-tir">1. Lower Tir</h2>
<blockquote>
<p>TIR一般认为是 target ir，TensorIR是后面又提的新概念，做为tvm script的一部分。</p>
</blockquote>
<p>这节用一个例子追踪 tir 表示的 PrimFunc 是如何经过 target translation 被翻译为特定target的一个 <code>runtime.Module</code></p>
<p>使用的例子代码如下：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>
<span class="n">SIZE</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="c1"># 1. 使用 TE 声明计算</span>
<span class="c1"># mm</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;B&#39;</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;MM&#39;</span><span class="p">)</span>
<span class="c1"># relu</span>
<span class="n">D_IN</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;RELU_IN&#39;</span><span class="p">)</span>
<span class="n">D_OUT</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">D_IN</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;RELU_OUT&quot;</span><span class="p">)</span>

<span class="c1"># 2. 从 TE 创建 PrimFunc </span>
<span class="n">te_mm</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">PrimFunc</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span><span class="o">.</span><span class="n">with_attr</span><span class="p">({</span><span class="s2">&quot;global_symbol&quot;</span><span class="p">:</span> <span class="s2">&quot;mmult&quot;</span><span class="p">})</span>
<span class="n">te_relu</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">PrimFunc</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">D_IN</span><span class="p">,</span> <span class="n">D_OUT</span><span class="p">])</span><span class="o">.</span><span class="n">with_attr</span><span class="p">({</span><span class="s2">&quot;global_symbol&quot;</span><span class="p">:</span> <span class="s2">&quot;relu&quot;</span><span class="p">})</span>

<span class="c1"># 3. 将创建出来的 PrimFunc 添加到一个 IRModule 中</span>
<span class="n">ir_m</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">IRModule</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">IRModule</span><span class="p">({</span><span class="s1">&#39;mm&#39;</span><span class="p">:</span> <span class="n">te_mm</span><span class="p">})</span>
<span class="n">gv_relu</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">GlobalVar</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="n">ir_m</span><span class="o">.</span><span class="n">update_func</span><span class="p">(</span><span class="n">gv_relu</span><span class="p">,</span> <span class="n">te_relu</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ir_m</span><span class="o">.</span><span class="n">get_global_vars</span><span class="p">())</span>
<span class="c1"># 这里也可以使用 先构建 Schedule， 再调用 `tvm.lower` 的方法 得到一个 IRModule</span>
<span class="c1"># te_sch: te.Schedule = te.create_schedule(C.op) # C.op: te.tensor.ComputeOp</span>
<span class="c1"># ir_m: tvm.IRModule = tvm.lower(te_sch, [A, B, C], simple_mode=True,name=&#39;mmult&#39;)</span>

<span class="c1"># 4. Lower and Build</span>
<span class="sd">&#39;&#39;&#39; 在这一步将 IRModule ==&gt; runtime.Module&#39;&#39;&#39;</span>
<span class="n">rt_m</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">ir_m</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s1">&#39;llvm&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;func_set&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[31mtvm.build</span><span class="se">\033</span><span class="s2">[0m: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">ir_m</span><span class="p">)</span><span class="si">}</span><span class="s2"> </span><span class="se">\033</span><span class="s2">[31m==&gt;</span><span class="se">\033</span><span class="s2">[0m </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">rt_m</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># print IRModule， 其中包含一个 PrimFunc</span>
<span class="c1"># print(f&quot;\033[31mAfter lowering, the IRModule\033[0m:\n{ir_m}&quot;)</span>
<span class="n">ir_m</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># # print source code</span>
<span class="c1"># print(f&quot;\033[31msource code\033[0m:\n{rt_m.get_source()}&quot;)</span>
</code></pre></div>
<p><code>tvm.build</code> 将代码从 IRModule 转换成 runtime.Module； </p>
<ul>
<li>其中 IRModule 定义在 <code>include/tvm/ir/module.h</code> 中； 可以简单视作一个 <code>&lt;name, BaseFunc&gt;</code> 的哈希表(即可同时包含<code>PrimFunc</code> 和 <code>Func</code>)</li>
<li>其中 runtime.Module 定义对应在 <code>include/tvm/runtime/module.h</code> 中； 可以简单视作一个 <code>&lt;name, PackedFunc&gt;</code> 的哈希表。 <code>ModuleNode</code> 是一个接口， 对应有多种不同的实现， 如 <code>CUDAModuleNode</code>， <code>LLVMModuleNode</code> 等</li>
</ul>
<p>以使用te声明 计算过程 为例，展示一个 <code>te.tensor.ComputeOp</code> 是如何被编译成 <code>runtime.Module</code> 的：</p>
<ol>
<li>使用te声明计算过程</li>
<li>
<p>使用 <code>te.create_schedule</code> 创建一个Schedule 对象， 接着使用 <code>te.lower()</code> 将一个 <code>te.Schedule</code> 转成一个IRModule；</p>
<p>或者使用 <code>te.create_prim_func</code> 创建一个 <code>tvm.tir.PrimFunc</code> 对象(即 MLC 课程 中所说的一个<strong>元张量函数</strong>)， 然后使用 IRModule 的构造函数把这个 PrimFunc 放到IRModule 中</p>
</li>
<li>
<p>接着调用 <code>tvm.build()</code> 对 IRModule进行构建： 在这一步中， IRModule 最终将被转换成一个 <code>runtime.Module</code>； </p>
<p>这里需要注意 <code>tvm.build</code> 可以接受 <code>Schedule</code>, <code>PrimFunc</code>, <code>IRModule</code> 等多种类型（实际上都会被转成 IRModule 然后再编译）作为输入， 如果传入的是一个 <code>Schedule</code> 的话， 则需要传入函数相应的输入参数列表</p>
</li>
</ol>
<p>💡<strong>因此接下来主要看 位于 <code>python/driver/build_module.py</code> 中的 <code>tvm.build</code> 方法</strong></p>
<hr />
<p><code>tvm.build()</code> 包括以下几个主要步骤</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">build</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">te</span><span class="o">.</span><span class="n">Schedule</span><span class="p">,</span> <span class="n">PrimFunc</span><span class="p">,</span> <span class="n">IRModule</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">IRModule</span><span class="p">]],</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Buffer</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Var</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Target</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">target_host</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Target</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">runtime</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;tvm.relay.backend.Runtime&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;default_function&quot;</span><span class="p">,</span>
    <span class="n">binds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Buffer</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">IRModule</span><span class="p">):</span>
      <span class="n">input_mod</span> <span class="o">=</span> <span class="n">lower</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">dict</span><span class="p">,</span> <span class="n">container</span><span class="o">.</span><span class="n">Map</span><span class="p">)):</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">Target</span><span class="o">.</span><span class="n">current</span><span class="p">()</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">target</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">target</span> <span class="k">if</span> <span class="n">target</span> <span class="k">else</span> <span class="s2">&quot;llvm&quot;</span>
      <span class="n">target_input_mod</span> <span class="o">=</span> <span class="p">{</span><span class="n">target</span><span class="p">:</span> <span class="n">input_mod</span><span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">target_input_mod</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="n">annotated_mods</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">tar</span><span class="p">,</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">target_input_mod</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">annotated_mods</span><span class="p">[</span><span class="n">tar</span><span class="p">]</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&quot;runtime&quot;</span><span class="p">,</span> <span class="n">runtime</span><span class="p">)</span>

    <span class="c1"># 省略 get target_host 的代码</span>
    <span class="n">target_host</span> <span class="o">=</span> <span class="s2">&quot;llvm&quot;</span>

    <span class="n">annotated_mods</span><span class="p">,</span> <span class="n">target_host</span> <span class="o">=</span> <span class="n">Target</span><span class="o">.</span><span class="n">canon_target_map_and_host</span><span class="p">(</span><span class="n">annotated_mods</span><span class="p">,</span> <span class="n">target_host</span><span class="p">)</span>
    <span class="n">rt_mod_host</span> <span class="o">=</span> <span class="n">_driver_ffi</span><span class="o">.</span><span class="n">tir_to_runtime</span><span class="p">(</span><span class="n">annotated_mods</span><span class="p">,</span> <span class="n">target_host</span><span class="p">)</span>
    <span class="n">annotated_mods</span><span class="p">,</span> <span class="n">target_host</span> <span class="o">=</span> <span class="n">Target</span><span class="o">.</span><span class="n">canon_target_map_and_host</span><span class="p">(</span><span class="n">annotated_mods</span><span class="p">,</span> <span class="n">target_host</span><span class="p">)</span>

    <span class="n">to_return</span> <span class="o">=</span> <span class="n">rt_mod_host</span>
    <span class="k">return</span> <span class="n">OperatorModule</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">to_return</span><span class="p">,</span> <span class="n">ir_module_by_target</span><span class="o">=</span><span class="n">annotated_mods</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</code></pre></div>
<ol>
<li>
<p>调用 <code>python/driver/build_module.py</code> 中 <code>lower()</code>， 这是一步作用是执行 IRModule（or schedule等） 到 IRModule 的变换，执行 tir 层级的 target 无关优化。 具体来说，<code>lower()</code> 中会调用 <code>ffi.lower_module(inp, simple_mode)</code>，这里对应到 C++ 代码 <code>src/dirver/driver_api.cc</code> 中的 <code>LowerModule</code> 方法：</p>
<div class="highlight"><pre><span></span><code><span class="n">IRModule</span><span class="w"> </span><span class="nf">LowerModule</span><span class="p">(</span><span class="n">IRModule</span><span class="w"> </span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">simple_mode</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">transform</span><span class="o">::</span><span class="n">Pass</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pass_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreatePassList</span><span class="p">(</span><span class="n">simple_mode</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">LowerWithPassList</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">mod</span><span class="p">),</span><span class="w"> </span><span class="n">pass_list</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">&quot;driver.lower_module&quot;</span><span class="p">).</span><span class="n">set_body_typed</span><span class="p">([](</span><span class="n">IRModule</span><span class="w"> </span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">simple_mode</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">LowerModule</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">mod</span><span class="p">),</span><span class="w"> </span><span class="n">simple_mode</span><span class="p">);</span>
<span class="p">});</span>
</code></pre></div>
<p>可以看到 在 <code>LowerModule</code> 逻辑中， 首先创建了一个pass 列表， 包含了<code>tir::transform</code>中的一些变换， 然后调用 <code>LowerWithPassList</code> 把列表中的 pass 应用到该 IRModule 中：即首先将 <code>pass_list</code> 构造成一个 <code>Sequential</code> 对象， 然后调用 <code>SequentialNode::operator()</code> ， </p>
</li>
<li>
<p>在第一步的 lowering 之后，<code>tvm.build()</code> 接收的inputs参数 <code>Union[te.Schedule, PrimFunc, IRModule, Mapping[str, IRModule]]</code> 被转成了一个 经过初步优化的 IRModule ， 并执行了 tir 级别的 target independent 优化。 <strong>接下来就是执行代码生成的工作</strong>， 首先检查 target 相关信息，会根据 <code>tvm.build()</code> 的输入设置相应的 target, target_host 等变量</p>
</li>
<li>
<p>在拿到 target 等规范格式的信息之后，会执行<code>rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)</code> ，这里调用的 <code>tir_to_runtime</code> 对应到C++中的 <code>TIRToRuntime</code>:</p>
<div class="highlight"><pre><span></span><code><span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">&quot;driver.tir_to_runtime&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">set_body_typed</span><span class="p">([](</span><span class="k">const</span><span class="w"> </span><span class="n">Map</span><span class="o">&lt;</span><span class="n">Target</span><span class="p">,</span><span class="w"> </span><span class="n">IRModule</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">inputs_arg</span><span class="p">,</span><span class="w"> </span><span class="n">Target</span><span class="w"> </span><span class="n">host_target</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">TIRToRuntime</span><span class="p">(</span><span class="n">inputs_arg</span><span class="p">,</span><span class="w"> </span><span class="n">host_target</span><span class="p">);</span>
<span class="w">    </span><span class="p">});</span>
</code></pre></div>
<p>比较关键的 <code>TIRToRuntime</code> 逻辑如下：</p>
<div class="highlight"><pre><span></span><code><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="nf">TIRToRuntime</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Map</span><span class="o">&lt;</span><span class="n">Target</span><span class="p">,</span><span class="w"> </span><span class="n">IRModule</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">inputs_arg</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Target</span><span class="o">&amp;</span><span class="w"> </span><span class="n">target_host_arg</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device_modules</span><span class="p">;</span>
<span class="w">  </span><span class="n">Map</span><span class="o">&lt;</span><span class="n">Target</span><span class="p">,</span><span class="w"> </span><span class="n">IRModule</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs_arg</span><span class="p">;</span>
<span class="w">  </span><span class="n">Target</span><span class="w"> </span><span class="n">target_host</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">target_host_arg</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// 1. 检查并设置target_host</span>
<span class="w">  </span><span class="c1">// ...</span>
<span class="w">  </span><span class="c1">// 2. 遍历输入的 {Target: IRModule}</span>
<span class="w">  </span><span class="c1">//  SplitMixedModule 将一个 IRModule 分为 host 与 device 两部分</span>
<span class="w">  </span><span class="c1">//  根据 host 与 device 信息更新 mhost_all</span>
<span class="w">  </span><span class="n">IRModule</span><span class="w"> </span><span class="n">mhost_all</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">IRModule</span><span class="p">(</span><span class="n">Map</span><span class="o">&lt;</span><span class="n">GlobalVar</span><span class="p">,</span><span class="w"> </span><span class="n">BaseFunc</span><span class="o">&gt;</span><span class="p">(),{},{},{},(</span><span class="o">*</span><span class="n">inputs</span><span class="p">.</span><span class="n">begin</span><span class="p">()).</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">attrs</span><span class="p">);</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="p">[</span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="n">ir_module</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">it</span><span class="p">;</span>
<span class="w">      </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="p">[</span><span class="n">host_mod</span><span class="p">,</span><span class="w"> </span><span class="n">device_mod</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SplitMixedModule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="p">,</span><span class="w"> </span><span class="n">target_host</span><span class="p">);</span>

<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">overrides_host_target</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">target</span><span class="o">-&gt;</span><span class="n">GetTargetDeviceType</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">target_host</span><span class="o">-&gt;</span><span class="n">GetTargetDeviceType</span><span class="p">();</span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_host_target_kind</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">target</span><span class="o">-&gt;</span><span class="n">kind</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">target_host</span><span class="o">-&gt;</span><span class="n">kind</span><span class="p">;</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">overrides_host_target</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">non_host_target_kind</span><span class="p">)</span>
<span class="w">        </span><span class="n">device_modules</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">codegen</span><span class="o">::</span><span class="n">Build</span><span class="p">(</span><span class="n">host_mod</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="p">));</span>
<span class="w">      </span><span class="k">else</span>
<span class="w">        </span><span class="n">mhost_all</span><span class="o">-&gt;</span><span class="n">Update</span><span class="p">(</span><span class="n">host_mod</span><span class="p">);</span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_mod</span><span class="o">-&gt;</span><span class="n">functions</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span>
<span class="w">        </span><span class="n">device_modules</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">codegen</span><span class="o">::</span><span class="n">Build</span><span class="p">(</span><span class="n">device_mod</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// 3. Build host module</span>
<span class="w">  </span><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="n">mhost</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">codegen</span><span class="o">::</span><span class="n">Build</span><span class="p">(</span><span class="n">mhost_all</span><span class="p">,</span><span class="w"> </span><span class="n">target_host</span><span class="p">);</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">device_modules</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="k">operator</span><span class="o">-&gt;</span><span class="p">())</span>
<span class="w">      </span><span class="n">mhost</span><span class="p">.</span><span class="n">Import</span><span class="p">(</span><span class="n">it</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">mhost</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<p>这里比较关键的 <code>codegen::build</code> 在<code>src/target/codegen.cc</code> 中， 实现如下：</p>
<div class="highlight"><pre><span></span><code><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="nf">Build</span><span class="p">(</span><span class="n">IRModule</span><span class="w"> </span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="n">Target</span><span class="w"> </span><span class="n">target</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">target_attr_map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">TargetKind</span><span class="o">::</span><span class="n">GetAttrMap</span><span class="o">&lt;</span><span class="n">FTVMTIRToRuntime</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;TIRToRuntime&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">target_attr_map</span><span class="p">.</span><span class="n">count</span><span class="p">(</span><span class="n">target</span><span class="o">-&gt;</span><span class="n">kind</span><span class="p">))</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">target_attr_map</span><span class="p">[</span><span class="n">target</span><span class="o">-&gt;</span><span class="n">kind</span><span class="p">](</span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="p">);</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">build_f_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;target.build.&quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">target</span><span class="o">-&gt;</span><span class="n">kind</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">PackedFunc</span><span class="o">*</span><span class="w"> </span><span class="n">bf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runtime</span><span class="o">::</span><span class="n">Registry</span><span class="o">::</span><span class="n">Get</span><span class="p">(</span><span class="n">build_f_name</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">bf</span><span class="p">)(</span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>
<p>可以看到 通过使用字符串拼接 拿到 <code>build_f_name</code> (例如 target 是 llvm， 这里得到就是 "target.build.llvm"， cuda 就是 "target.build.cuda")， 接着用这个名字查找到已注册的函数，进行 build</p>
</li>
<li>
<p>接下来分别以 llvm 后端（不设置runtime） 和 cuda 后端为例 看一下如何生成的 llvm IR or CUDA 代码</p>
<div class="highlight"><pre><span></span><code><span class="c1">// `src/target/llvm/llvm_module.cc`</span>
<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">&quot;target.build.llvm&quot;</span><span class="p">)</span>
<span class="p">.</span><span class="n">set_body_typed</span><span class="p">([](</span><span class="n">IRModule</span><span class="w"> </span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="n">Target</span><span class="w"> </span><span class="n">target</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_object</span><span class="o">&lt;</span><span class="n">LLVMModuleNode</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="n">n</span><span class="o">-&gt;</span><span class="n">Init</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span><span class="p">(</span><span class="n">n</span><span class="p">);</span>
<span class="p">});</span>
<span class="c1">// `src/target/opt/build_cuda_on.cc`</span>
<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">&quot;target.build.cuda&quot;</span><span class="p">).</span><span class="n">set_body_typed</span><span class="p">(</span><span class="n">BuildCUDA</span><span class="p">);</span>
</code></pre></div>
<p>首先看CUDA的 tir =&gt; bin 编译流程：</p>
<div class="highlight"><pre><span></span><code><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span><span class="w"> </span><span class="nf">BuildCUDA</span><span class="p">(</span><span class="n">IRModule</span><span class="w"> </span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="n">Target</span><span class="w"> </span><span class="n">target</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Step 1: Initialize CodeGen for CUDA  </span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">output_ssa</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">  </span><span class="n">CodeGenCUDA</span><span class="w"> </span><span class="n">cg</span><span class="p">;</span>
<span class="w">  </span><span class="n">cg</span><span class="p">.</span><span class="n">Init</span><span class="p">(</span><span class="n">output_ssa</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Step 2: Add all tir::PrimFunc in IRModule to compile list</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">kv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">mod</span><span class="o">-&gt;</span><span class="n">functions</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Downcast</span><span class="o">&lt;</span><span class="n">PrimFunc</span><span class="o">&gt;</span><span class="p">(</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">);</span>
<span class="w">    </span><span class="n">cg</span><span class="p">.</span><span class="n">AddFunction</span><span class="p">(</span><span class="n">f</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Step 3: Lower IRModule to CUDA source code</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">code</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">Finish</span><span class="p">();</span>

<span class="w">  </span><span class="c1">// Step 4: Compile CUDA source code using NVCC and create runtime::Module</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">fmt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;ptx&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">ptx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">NVRTCCompile</span><span class="p">(</span><span class="n">code</span><span class="p">,</span><span class="w"> </span><span class="n">cg</span><span class="p">.</span><span class="n">need_include_path</span><span class="p">());</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">CUDAModuleCreate</span><span class="p">(</span><span class="n">ptx</span><span class="p">,</span><span class="w"> </span><span class="n">fmt</span><span class="p">,</span><span class="w"> </span><span class="n">ExtractFuncInfo</span><span class="p">(</span><span class="n">mod</span><span class="p">),</span><span class="w"> </span><span class="n">code</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>
<p><strong>并行线程执行（Parallel Thread eXecution，PTX）代码是编译后的GPU代码的一种 IR ， 它可以再次编译为原生的GPU指令</strong>。PTX提供了一个稳定的编程模型和指令集，这个ISA能够跨越多种GPU，并且能够优化代码的编译等等。</p>
<p>接下来看 LLVM，值得注意的是，如果 target="llvm"，由于 LLVM IR 仍然只是一种中间表示，还需要根据 target 当中更详细的硬件参数，找到目标编译硬件，然后调用相应的 CodeGen（省略部分辅助代码）：</p>
<div class="highlight"><pre><span></span><code><span class="kt">void</span><span class="w"> </span><span class="nf">Init</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">IRModule</span><span class="o">&amp;</span><span class="w"> </span><span class="n">mod</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Target</span><span class="o">&amp;</span><span class="w"> </span><span class="n">target</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Step 1: Initialize CodeGen for LLVM with different target</span>
<span class="w">  </span><span class="n">InitializeLLVM</span><span class="p">();</span>
<span class="w">  </span><span class="n">tm_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GetLLVMTargetMachine</span><span class="p">(</span><span class="n">target</span><span class="p">);</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">CodeGenLLVM</span><span class="o">&gt;</span><span class="w"> </span><span class="n">cg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CodeGenLLVM</span><span class="o">::</span><span class="n">Create</span><span class="p">(</span><span class="n">tm_</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>

<span class="w">  </span><span class="c1">// Step 2: Add all tir::PrimFunc in IRModule to compile list</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">PrimFunc</span><span class="o">&gt;</span><span class="w"> </span><span class="n">funcs</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">kv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">mod</span><span class="o">-&gt;</span><span class="n">functions</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">IsInstance</span><span class="o">&lt;</span><span class="n">PrimFuncNode</span><span class="o">&gt;</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// (@jroesch): we relax constraints here, Relay functions will just be ignored.</span>
<span class="w">      </span><span class="n">DLOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Can only lower IR Module with PrimFuncs, but got &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">GetTypeKey</span><span class="p">();</span>
<span class="w">      </span><span class="k">continue</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Downcast</span><span class="o">&lt;</span><span class="n">PrimFunc</span><span class="o">&gt;</span><span class="p">(</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">);</span>
<span class="w">    </span><span class="n">funcs</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">f</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Step 3: Lower IRModule to LLVM IR code</span>
<span class="w">  </span><span class="n">module_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="o">-&gt;</span><span class="n">Finish</span><span class="p">();</span>
<span class="p">}</span>
</code></pre></div>
<p>可以看到这里构造了一个 <code>LLVMModuleNode</code> 实例， 调用了它的 <code>Init</code> 方法， 因此可知该方法就是将 IRModule lower 为 LLVMModuleNode 的核心函数。 关于 <code>LLVMModuleNode</code> 的定义， 可参考 <a href="tvm-type.html">TVM-type</a>，下面是 <code>Init</code> 方法的具体实现：</p>
</li>
<li>
<p>从上一步中可知， TIR 能 lower 成目标源代码，关键是 CodeGen。上面提到的 CodeGenCUDA，以及 CodeGenLLVM 是完成 TIR lower 为 C++ 代码的相关结构。 其中 <code>CodeGenCUDA</code>继承自 <code>CodeGenC</code>，以其为例子说明（<code>tvm/src/target/source/codegen_c.[h, cc]</code>）：</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CodeGenC</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">ExprFunctor</span><span class="o">&lt;</span><span class="kt">void</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">PrimExpr</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span>
<span class="w">             </span><span class="k">public</span><span class="w"> </span><span class="n">StmtFunctor</span><span class="o">&lt;</span><span class="kt">void</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Stmt</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span>
<span class="w">             </span><span class="k">public</span><span class="w"> </span><span class="n">CodeGenSourceBase</span><span class="w"> </span><span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="c1">// expression</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">VarNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span><span class="w">         </span><span class="c1">// NOLINT(*)</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">LoadNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span><span class="w">        </span><span class="c1">// NOLINT(*)</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">BufferLoadNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span><span class="w">  </span><span class="c1">// NOLINT(*)</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">LetNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span><span class="w">         </span><span class="c1">// NOLINT(*)</span>
<span class="w">  </span><span class="p">...</span>
<span class="w">  </span><span class="c1">// statment</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">VisitStmt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">LetStmtNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">VisitStmt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">StoreNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">VisitStmt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">BufferStoreNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">VisitStmt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">ForNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">VisitStmt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">WhileNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">VisitStmt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">IfThenElseNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</code></pre></div>
<p>可以看到， <code>CodeGenC</code> 会遍历到两种 TIR Node：Expression（表达式） 和 Statement（语句）。Expression（表达式）中包含了常见的变量声明、运算、判断、函数调用，而 Statement（语句）中包含了控制流（if-else，Loop 等）、内存管理、赋值等操作。</p>
<p>例如，遇到四则运算的 Expression，CodeGenC 直接翻译为 " a OP b "的代码：</p>
<div class="highlight"><pre><span></span><code><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">PrintBinaryExpr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">T</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">opstr</span><span class="p">,</span>
<span class="w">                            </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">,</span><span class="w"> </span><span class="n">CodeGenC</span><span class="o">*</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// If both a and b are scalars</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">.</span><span class="n">lanes</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// If OP is an alphabet string, then lower it as &quot;OP(a, b)&quot;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">isalpha</span><span class="p">(</span><span class="n">opstr</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">opstr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">&#39;(&#39;</span><span class="p">;</span>
<span class="w">      </span><span class="n">p</span><span class="o">-&gt;</span><span class="n">PrintExpr</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">);</span>
<span class="w">      </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;, &quot;</span><span class="p">;</span>
<span class="w">      </span><span class="n">p</span><span class="o">-&gt;</span><span class="n">PrintExpr</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">);</span>
<span class="w">      </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">&#39;)&#39;</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// If OP is a symbol, like + - * / %, then lower it as &quot;a OP b&quot;</span>
<span class="w">    </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">&#39;(&#39;</span><span class="p">;</span>
<span class="w">      </span><span class="n">p</span><span class="o">-&gt;</span><span class="n">PrintExpr</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">);</span>
<span class="w">      </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">&#39; &#39;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">opstr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">&#39; &#39;</span><span class="p">;</span>
<span class="w">      </span><span class="n">p</span><span class="o">-&gt;</span><span class="n">PrintExpr</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">);</span>
<span class="w">      </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="sc">&#39;)&#39;</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// If both a and b are vectors</span>
<span class="w">  </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">p</span><span class="o">-&gt;</span><span class="n">PrintVecBinaryOp</span><span class="p">(</span><span class="n">opstr</span><span class="p">,</span><span class="w"> </span><span class="n">op</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">op</span><span class="o">-&gt;</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">op</span><span class="o">-&gt;</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="n">CodeGenC</span><span class="o">::</span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">AddNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// NOLINT(*)</span>
<span class="w">  </span><span class="n">PrintBinaryExpr</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;+&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">,</span><span class="w"> </span><span class="k">this</span><span class="p">);</span>
<span class="p">}</span>
<span class="kt">void</span><span class="w"> </span><span class="n">CodeGenC</span><span class="o">::</span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">SubNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// NOLINT(*)</span>
<span class="w">  </span><span class="n">PrintBinaryExpr</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;-&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">,</span><span class="w"> </span><span class="k">this</span><span class="p">);</span>
<span class="p">}</span>
<span class="kt">void</span><span class="w"> </span><span class="n">CodeGenC</span><span class="o">::</span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">MulNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// NOLINT(*)</span>
<span class="w">  </span><span class="n">PrintBinaryExpr</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;*&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">,</span><span class="w"> </span><span class="k">this</span><span class="p">);</span>
<span class="p">}</span>
<span class="kt">void</span><span class="w"> </span><span class="n">CodeGenC</span><span class="o">::</span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">DivNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// NOLINT(*)</span>
<span class="w">  </span><span class="n">PrintBinaryExpr</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;/&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">,</span><span class="w"> </span><span class="k">this</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>
<p>如果遇到选择 SelectNode，CodeGenC 则翻译为 "(c ? a : b)" 的代码：</p>
<div class="highlight"><pre><span></span><code><span class="kt">void</span><span class="w"> </span><span class="nf">CodeGenC::VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">SelectNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ostream</span><span class="o">&amp;</span><span class="w"> </span><span class="n">os</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;(&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">PrintExpr</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">condition</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">);</span>
<span class="w">  </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; ? &quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">PrintExpr</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">true_value</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">);</span>
<span class="w">  </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; : &quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">PrintExpr</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">false_value</span><span class="p">,</span><span class="w"> </span><span class="n">os</span><span class="p">);</span>
<span class="w">  </span><span class="n">os</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;)&quot;</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
</li>
</ol>
<h2 id="2-lower-te">2. Lower TE</h2>
<p>上一节中我们使用 TE(Tensor Expression) 声明计算过程， 构造了一个 PrimFunc。 在TVM 中， PrimFunc 还可以通过编写 TVMScript， 或者通过 relay/relax lower 而来。 </p>
<p>TE是位于 Relay IR / TOPI 和 TIR 之间概念， 是TVM 提供的一种用于编写 Primfunc 的 EDSL， 其抽象程度比 TIR 更高，无法直接被编译为硬件源代码，必须先 lower 为 TIR 的 Primitive Function 再进行编译。</p>
<p>在这一节中我们关注使用 te 描述的计算过程是如何被转换成一个 PrimFunc 的，即对应到 <a href="#1-lower-tir">Lower Tir</a> 中提到的 <code>te.create_prim_func</code>， 也就是官方给出的 编译流程中的红框中的部分：</p>
<div class="autocb" style="text-align:center;"><img src="./tvm-lowering.assets\autocb_0.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

<p>测试代码与第一节中代码相同，只不过此时我们关注更上层的部分：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>
<span class="n">SIZE</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="c1"># 1. 使用 TE 声明计算</span>
<span class="c1"># mm</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">)</span>  <span class="c1"># te.tensor.Tensor</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;B&#39;</span><span class="p">)</span>  <span class="c1"># te.tensor.Tensor</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>          <span class="c1"># tir.expr.IterVar</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">SIZE</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;MM&#39;</span><span class="p">)</span> <span class="c1"># te.tensor.Tensor</span>

<span class="c1"># 2. 从 TE 创建 PrimFunc </span>
<span class="n">te_mm</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">PrimFunc</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span><span class="o">.</span><span class="n">with_attr</span><span class="p">({</span><span class="s2">&quot;global_symbol&quot;</span><span class="p">:</span> <span class="s2">&quot;mmult&quot;</span><span class="p">})</span>

<span class="c1"># 3. 将创建出来的 PrimFunc 添加到一个 IRModule 中</span>
<span class="n">ir_m</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">IRModule</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">IRModule</span><span class="p">({</span><span class="s1">&#39;mm&#39;</span><span class="p">:</span> <span class="n">te_mm</span><span class="p">})</span>

<span class="c1"># 这里也可以使用 先构建 Schedule， 再调用 `tvm.lower` 的方法 得到一个 IRModule</span>
<span class="n">te_sch</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">Schedule</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">op</span><span class="p">)</span> <span class="c1"># C.op: te.tensor.ComputeOp</span>
<span class="n">ir_m</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">IRModule</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">te_sch</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;mmult&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ir_m</span><span class="o">.</span><span class="n">get_global_vars</span><span class="p">())</span>
</code></pre></div>
<p>在关注 lowering 之前首先看一下一个 TE 的构成</p>
<ol>
<li>
<p>首先是 <code>te.placeholder</code> 将返回一个 <code>te.tensor.Tensor</code>。 其数据结构定义位于 <code>include/tvm/te/operation.h</code>中， 具体如下：</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PlaceholderOpNode</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">OperationNode</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">PrimExpr</span><span class="o">&gt;</span><span class="w"> </span><span class="n">shape</span><span class="p">;</span>
<span class="w">  </span><span class="n">DataType</span><span class="w"> </span><span class="n">dtype</span><span class="p">;</span>

<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="nf">num_outputs</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="p">{</span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;};</span>
<span class="w">  </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">PrimExpr</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_shape</span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="p">{</span><span class="k">return</span><span class="w"> </span><span class="n">shape</span><span class="p">;};</span>
<span class="w">  </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">InputTensors</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="p">{</span><span class="k">return</span><span class="w"> </span><span class="p">{};};</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">_type_key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;PlaceholderOp&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">TVM_DECLARE_BASE_OBJECT_INFO</span><span class="p">(</span><span class="n">PlaceholderOpNode</span><span class="p">,</span><span class="w"> </span><span class="n">OperationNode</span><span class="p">);</span>
<span class="p">};</span>
</code></pre></div>
<p>placeholder 通常用于计算图的 Input 节点使用，没有前序节点，可能也是te中最常用的Op。 除了<code>PlaceholderOp</code> 之外， TE 中还有一些其他的 Op 定义，例如后面会提到的 <code>ComputeOp</code>， 以及 <code>ExternOp</code> 等， 这些 Op 是 TE AST 的组成部分。</p>
</li>
<li>
<p>接下来是 <code>te.compute</code> ， compute 从一个或者多个前序节点接收数据，并按初始化的时候传入的 lambda 表达式计算 Tensor 内的数据。其API 第一个传入参数是 Tensor 的 shape，第二个参数是一个 lambda 表达式，表明 Tensor 的数据是如何计算来的，返回一个<code>te.tensor.Tensor</code> 。 其 C++ 部分数据结构定义具体如下：</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TVM_DLL</span><span class="w"> </span><span class="n">ComputeOpNode</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">BaseComputeOpNode</span><span class="w"> </span><span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">PrimExpr</span><span class="o">&gt;</span><span class="w"> </span><span class="n">body</span><span class="p">;</span><span class="w"> </span><span class="c1">//compute expression</span>
<span class="w">  </span><span class="c1">// override functions</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="nf">num_outputs</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="p">;</span>
<span class="w">  </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">InputTensors</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">final</span><span class="p">;</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">_type_key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;ComputeOp&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">TVM_DECLARE_FINAL_OBJECT_INFO</span><span class="p">(</span><span class="n">ComputeOpNode</span><span class="p">,</span><span class="w"> </span><span class="n">BaseComputeOpNode</span><span class="p">);</span>
<span class="p">};</span>
</code></pre></div>
<p>可以看到其关键成员只有一个 body，是一个 PrimExpr 数组，因此接下来看 python 端是怎样把一个 lambda 表达式表达的计算转换成了 PrimExpr：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">fcompute</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;compute&quot;</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">attrs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">varargs_names</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="n">out_ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

  <span class="n">argspec</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="n">fcompute</span><span class="p">)</span>
  <span class="n">arg_names</span> <span class="o">=</span> <span class="n">argspec</span><span class="o">.</span><span class="n">args</span>

  <span class="c1"># 1. 将传入的 shape 转成 tir.IterVar 列表</span>
  <span class="n">dim_var</span> <span class="o">=</span> <span class="p">[</span><span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">IterVar</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">arg_names</span><span class="p">,</span> <span class="n">shape</span><span class="p">[:</span><span class="n">out_ndim</span><span class="p">])]</span>

  <span class="c1"># 2. 构造计算的 AST</span>
  <span class="n">body</span> <span class="o">=</span> <span class="n">fcompute</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">var</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dim_var</span><span class="p">])</span>

  <span class="c1"># 3. 调用 C++ API 获取 compute_op_node</span>
  <span class="n">body</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">body</span><span class="p">)</span> <span class="c1"># 将List[PrimExpr] 转成 Array&lt;PrimExpr&gt;</span>
  <span class="n">op_node</span> <span class="o">=</span> <span class="n">_ffi_api</span><span class="o">.</span><span class="n">ComputeOp</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">tag</span><span class="p">,</span> <span class="n">attrs</span><span class="p">,</span> <span class="n">dim_var</span><span class="p">,</span> <span class="n">body</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">op_node</span><span class="o">.</span><span class="n">output</span>
</code></pre></div>
<p>这里的关键点在于 <code>IterVar</code> 继承了 <code>ExprOp</code>（位于 <code>python/tvm/tir/expr.py</code>）， 而 <code>ExprOp</code> 重载了四则运算，移位运算，比较运算等操作符，因此可以直接被 lambda 表达式操作，调用重载的运算符返回表示运算之后的表达式，例如：</p>
<div class="highlight"><pre><span></span><code><span class="n">Z</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div>
<p>首先对于 <code>X[i]</code>和<code>Y[i]</code> 会分别构造一个 <code>tir.expr.ProducerLoad</code> 表达式，接着这两个表达式的乘法会调用到 <code>ExprOp</code> 重载的乘法运算符：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">span</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_ffi_api</span><span class="o">.</span><span class="n">_OpMul</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">span</span><span class="p">)</span>
</code></pre></div>
<p>返回一个 <code>tir.Mul</code> Expr；其它的运算类似。<strong>值得注意的是</strong> TE 中也提供了控制类 PrimExpr 的封装，例如 <code>te.if_then_else</code>；以及 <code>te.extern_primfunc</code> 用其它PrimFunc 来创建 PrimExpr</p>
</li>
<li>
<p>在构造表示计算的 AST 之后，我们把这个从lambda构造出来的 PrimExpr 数组传入了<code>_ffi_api.ComputeOp</code>， 返回一个 ComputeOp 类型， <code>ComputeOp</code> 是 <code>te::Operation</code> 的一个子类，因此可以从其<code>output</code>方法拿到计算结果 <code>Tensor</code> 作为 <code>te.compute</code> 的返回结果</p>
</li>
</ol>
<p>💡<strong>在这里总结一下 TE 的构成关键元素</strong>： 即 <code>Tensor</code> 和 <code>Operation</code> 两个类型。前者表征一个张量的形状、元素类型、source operation等；后者表示产生 Tensor 的一种操作，包括表示输入的操作 PlaceholderOp， 表示计算逻辑的操作 ComputeOp 等。</p>
<div class="autocb" style="text-align:center;"><img src="./tvm-lowering.assets\autocb_1.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

<hr />
<p>接下来我们开始关注 <code>te.create_prim_func</code> 是如何 将 TE lower 到 PrimFunc 的，对应到<code>src/te/operation/create_prim_func.cc</code> ：</p>
<div class="highlight"><pre><span></span><code><span class="n">PrimFunc</span><span class="w"> </span><span class="nf">CreatePrimFuncWithConstants</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">te</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">arg_list</span><span class="p">,</span>
<span class="w">                                     </span><span class="k">const</span><span class="w"> </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">runtime</span><span class="o">::</span><span class="n">NDArray</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">constants</span><span class="p">,</span>
<span class="w">                                     </span><span class="k">const</span><span class="w"> </span><span class="n">Optional</span><span class="o">&lt;</span><span class="n">Array</span><span class="o">&lt;</span><span class="n">tir</span><span class="o">::</span><span class="n">Var</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">tir_var_list</span><span class="p">,</span>
<span class="w">                                     </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">DataType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">index_dtype_override</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Infomations used in CreatePrimFunc and its sub-functions.</span>
<span class="w">  </span><span class="n">CreateFuncInfo</span><span class="w"> </span><span class="n">info</span><span class="p">(</span><span class="n">arg_list</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// Root body stmts.</span>
<span class="w">  </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">Stmt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">root_stmts</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// Analyzer</span>
<span class="w">  </span><span class="n">arith</span><span class="o">::</span><span class="n">Analyzer</span><span class="w"> </span><span class="n">analyzer</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Step 1. 后续遍历计算图： placeholder被放到前面，最终computeOp在最后</span>
<span class="w">  </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">te</span><span class="o">::</span><span class="n">Operation</span><span class="o">&gt;</span><span class="w"> </span><span class="n">order</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CollectOrderedOps</span><span class="p">(</span><span class="n">arg_list</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Step 2. 这段逻辑是处理 TE 中后来引入的 ExternOp，在本例中无需考虑</span>
<span class="w">  </span><span class="n">InitializeBufferBinds</span><span class="p">(</span><span class="n">order</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">info</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Step 3. 如果是 placeholder，创建对应的 tir.Buffer；如果是compute，创建对应的 tir.Stmt；如果是 extern，则创建一个 tir.Stmt 节点</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">te</span><span class="o">::</span><span class="n">Operation</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">order</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">RewriteStageToBlock</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">info</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">root_stmts</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">analyzer</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// 创建并返回 PrimFunc</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">func</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GenerateAndCompletePrimFunc</span><span class="p">(</span><span class="n">arg_list</span><span class="p">,</span><span class="w"> </span><span class="n">root_stmts</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">info</span><span class="p">,</span><span class="w"> </span><span class="n">tir_var_list</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">func</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<p>回顾下 PrimFunc 的结构：</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">PrimFuncNode</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">BaseFuncNode</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">tir</span><span class="o">::</span><span class="n">Var</span><span class="o">&gt;</span><span class="w"> </span><span class="n">params</span><span class="p">;</span>
<span class="w">  </span><span class="n">tir</span><span class="o">::</span><span class="n">Stmt</span><span class="w"> </span><span class="n">body</span><span class="p">;</span>
<span class="w">  </span><span class="n">Type</span><span class="w"> </span><span class="n">ret_type</span><span class="p">;</span>
<span class="w">  </span><span class="n">Map</span><span class="o">&lt;</span><span class="n">tir</span><span class="o">::</span><span class="n">Var</span><span class="p">,</span><span class="w"> </span><span class="n">Buffer</span><span class="o">&gt;</span><span class="w"> </span><span class="n">buffer_map</span><span class="p">;</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">_type_key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;tir.PrimFunc&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">TVM_DECLARE_FINAL_OBJECT_INFO</span><span class="p">(</span><span class="n">PrimFuncNode</span><span class="p">,</span><span class="w"> </span><span class="n">BaseFuncNode</span><span class="p">);</span>
<span class="p">};</span>
</code></pre></div>
<p><code>GenerateAndCompletePrimFunc</code> 会将 <code>arg_list</code>(<code>Array&lt;te::Tensor&gt;</code>) 转成一个 <code>Array&lt;tir::Var&gt;</code> 作为函数的 params； 而函数 body 和 <code>buffer_map</code> 已从建立的计算图中得到；因此到这里我们就完成了从 TE 到 <code>PrimFunc</code> 的转换。</p>
<p>💡总结整个过程，最核心的部分在于后续遍历 TE 表示的计算图（Operation作为节点，通过 <code>InputTensors</code> 方法拿到所有入边，做 DFS），转换成对应的 <code>tir.Buffer</code> 和 <code>tir.Stmt</code> 即 tir 中的 AST</p>
<h2 id="3-lower-relay">3. Lower Relay</h2>
<p>这一节关注一个更高层次的抽象， TVM 中图级别 IR Relay 的 lowering</p>
<div class="autocb" style="text-align:center;"><img src="./tvm-lowering.assets\autocb_2.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

<p>从这张图可以看到， Relay 要到 TIR 有2条路径，第一条就是直接到 TIR， 比如 PrimExpr 派生的节点 IntImmNode 可以直接映射到 TIR ，另外一条就是 Relay 里面类似 Conv 的 Op 的计算逻辑是用 TOPI 来表达的， TOPI 是 TVM 中用 TE 来表示常用算子的预定义库。</p>
<h3 id="31-topi">3.1. TOPI</h3>
<p>考虑这个例子：</p>
<div class="highlight"><pre><span></span><code><span class="n">n</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int32&quot;</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;j&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int32&quot;</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;A&quot;</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;B&quot;</span><span class="p">)</span>

<span class="c1"># 使用 TOPI: 这一句与上面两句等价</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 

<span class="c1"># 可以利用上一节的代码， 将 TE lower 到 tir 查看表示：</span>
<span class="n">prim_func1</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">])</span>
<span class="n">prim_func2</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span><span class="n">C</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[31m</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">20</span><span class="si">}</span><span class="s2">prim_func1</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">20</span><span class="si">}</span><span class="se">\033</span><span class="s2">[0m</span><span class="se">\n</span><span class="si">{</span><span class="n">prim_func1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[31m</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">20</span><span class="si">}</span><span class="s2">prim_func2</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">20</span><span class="si">}</span><span class="se">\033</span><span class="s2">[0m</span><span class="se">\n</span><span class="si">{</span><span class="n">prim_func2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Build 之后运行结果当然也是相同的：</span>
<span class="n">prim_func1</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">])</span>
<span class="n">prim_func2</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span><span class="n">C</span><span class="p">])</span>

<span class="n">lib1</span><span class="p">,</span> <span class="n">lib2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">prim_func1</span><span class="p">),</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">prim_func2</span><span class="p">)</span>

<span class="n">a_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">expected_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">a_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">a_nd1</span><span class="p">,</span> <span class="n">a_nd2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a_np</span><span class="p">),</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a_np</span><span class="p">)</span>
<span class="n">test_res1</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">test_res2</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>

<span class="n">lib1</span><span class="p">(</span><span class="n">a_nd1</span><span class="p">,</span> <span class="n">test_res1</span><span class="p">)</span>
<span class="n">lib2</span><span class="p">(</span><span class="n">a_nd2</span><span class="p">,</span> <span class="n">test_res2</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">expected_res</span><span class="p">,</span> <span class="n">test_res1</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">expected_res</span><span class="p">,</span> <span class="n">test_res2</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;🎉</span><span class="se">\033</span><span class="s2">[32mTest Pass...</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
</code></pre></div>
<p>可以看到，对于求和这样常见的简单操作，如果只使用 te 的话，也需要先定义一个 reduce_axis, 再用 lambda 风格去声明，比较繁琐，更不必说在神经网络中大量使用的其它更复杂一点的算子，如果用 te 去写的话，会比较麻烦。 因此 TVM 提供了一些使用 TE 定义的现成的算子，把他们放到一起（以及一些 schedule 操作），称作 TOPI(Tensor operator inventory)</p>
<p>TOPI 中包含了如 卷积， softmax， 矩阵乘 等常见的算子，可以帮助我们更好的表达 DL 中常见的计算过程。</p>
<p>在看具体实现前可以首先观察一下 TOPI 的目录结构，包含了 <code>nn.cc</code>，<code>elemwise.cc</code>，<code>schedule.cc</code>等，分别对应了机器学习常见算子，element wise 运算（如上面例子中的 <code>topi.sum</code>）， 算子调度；在这些文件中，使用 <code>TVM_REGISTER_GLOBAL</code> 注册了 TOPI 中的具体实现。</p>
<p>💡这里我们以 <code>topi.matmul</code> 为例看一下实现。</p>
<p><code>topi.matmul</code> 对应 C++ 注册在<code>src/topi/nn.cc</code> 中，实现在<code>include/tvm/topi/nn/dense.h</code>中，简化后如下：</p>
<div class="highlight"><pre><span></span><code><span class="kr">inline</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">te</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">matmul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">te</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">te</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">B</span><span class="p">,</span><span class="kt">bool</span><span class="w"> </span><span class="n">trans_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">trans_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;T_matmul&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">tag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kMatMul</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">tvm</span><span class="o">::</span><span class="n">Array</span><span class="o">&lt;</span><span class="n">tvm</span><span class="o">::</span><span class="n">PrimExpr</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_shape</span><span class="p">{</span><span class="n">A</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">[</span><span class="n">trans_a</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">B</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">[</span><span class="n">trans_b</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="p">]};</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">te</span><span class="o">::</span><span class="n">reduce_axis</span><span class="p">(</span><span class="n">tvm</span><span class="o">::</span><span class="n">Range</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">[</span><span class="n">trans_a</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="p">]},</span><span class="w"> </span><span class="s">&quot;k&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tvm</span><span class="o">::</span><span class="n">tir</span><span class="o">::</span><span class="n">Var</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">tir</span><span class="o">::</span><span class="n">Var</span><span class="w"> </span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">sum</span><span class="p">((</span><span class="n">trans_a</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">trans_b</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">]),</span><span class="w"> </span><span class="p">{</span><span class="n">k</span><span class="p">});</span>
<span class="w">  </span><span class="p">};</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">te</span><span class="o">::</span><span class="n">compute</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span><span class="w"> </span><span class="n">l</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">tag</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>
<p>可以看到其实 <code>dense</code> 跟我们手写的没有什么不同，但是对于 logsoftmax 之类有实现技巧的算子，TOPI 有很好的实现并且有相应的调度可以使用。</p>
<blockquote>
<p>TOPI 中有些算子在C++ 和 python 端分别实现了一次，这样做的原因见： https://discuss.tvm.apache.org/t/why-topi-is-implemented-in-both-c-and-python/50/7</p>
<p>简单来说，当一个算子地调度方式稳定之后，会被放在C++部分， 正在dev地放在python实现；而对于一些简单的算子，就保留了在 C++ 和 py 两部分的实现</p>
</blockquote>
<h3 id="32-relay-graphexecutor">3.2. Relay-GraphExecutor</h3>
<p>本节使用如下例子：</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">relay</span>

<span class="c1"># 1. 定义 Relay Var</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">weight1</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;weight1&quot;</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">784</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">bias1</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;bias1&quot;</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,),</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">weight2</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;weight2&quot;</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">128</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">bias2</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&quot;bias2&quot;</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,),</span><span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>

<span class="c1"># 2. 使用 Relay Op 定义网络结构</span>
<span class="n">dense1</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">weight1</span><span class="p">)</span>
<span class="n">bias_add1</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">dense1</span><span class="p">,</span> <span class="n">bias1</span><span class="p">)</span>
<span class="n">relu1</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">bias_add1</span><span class="p">)</span>
<span class="n">dense2</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">relu1</span><span class="p">,</span><span class="n">weight2</span><span class="p">)</span>
<span class="n">bias_add2</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">dense2</span><span class="p">,</span> <span class="n">bias2</span><span class="p">)</span>
<span class="n">relu2</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">bias_add2</span><span class="p">)</span>

<span class="c1"># 3. 构建网络</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">Function</span><span class="p">([</span><span class="n">data</span><span class="p">,</span><span class="n">weight1</span><span class="p">,</span><span class="n">bias1</span><span class="p">,</span><span class="n">weight2</span><span class="p">,</span><span class="n">bias2</span><span class="p">],</span><span class="n">relu2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[31m</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">20</span><span class="si">}</span><span class="s2">relay func</span><span class="si">{</span><span class="s1">&#39;=&#39;</span><span class="o">*</span><span class="mi">20</span><span class="si">}</span><span class="se">\033</span><span class="s2">[0m</span><span class="se">\n</span><span class="si">{</span><span class="n">func</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 4. 构建 IRModule</span>
<span class="n">ir_mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">IRModule</span><span class="o">.</span><span class="n">from_expr</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>

<span class="c1"># 5. 调用 relay.build 进行构建</span>
<span class="c1"># relay.build 目前暂时支持第一个参数传入 `relay.Function`， 但是之后将只支持 IRModule</span>
<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
  <span class="n">rt_lib</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">ir_mod</span><span class="o">=</span><span class="n">ir_mod</span><span class="p">,</span><span class="n">target</span><span class="o">=</span><span class="s2">&quot;llvm&quot;</span><span class="p">)</span>

<span class="c1"># 6. 使用 graph_executor 加载 build 好的计算图并执行</span>
<span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">dtype</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span>
<span class="n">graph_module</span> <span class="o">=</span> <span class="n">graph_executor</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">rt_lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">dev</span><span class="p">))</span>
<span class="c1"># Set inputs: 这里只设置一部分参数，因为params 的内存已经在 relay.build 阶段 分配好内存包含在 graphmodule 中， 因此这里的代码也能跑通</span>

<span class="c1"># Execute &amp;&amp; Get outputs</span>
<span class="n">graph_module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">graph_exec_output</span> <span class="o">=</span> <span class="n">graph_module</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">graph_exec_output</span><span class="p">)</span>
</code></pre></div>
<p>这里以 <code>dense</code> 算子为例， 首先关注 <code>dense1 = relay.nn.dense(data,weight1)</code> ：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">dense</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">_make</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">units</span><span class="p">,</span> <span class="n">out_dtype</span><span class="p">)</span>
</code></pre></div>
<p>在 C++ 端使用 RELAY_REGISTER_OP 机制统一管理 Relay Op， <code>dense</code>算子的注册位于 <code>src/relay/op/nn/nn.cc</code> 中：</p>
<div class="highlight"><pre><span></span><code><span class="n">Expr</span><span class="w"> </span><span class="nf">MakeDense</span><span class="p">(</span><span class="n">Expr</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Expr</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">IndexExpr</span><span class="w"> </span><span class="n">units</span><span class="p">,</span><span class="w"> </span><span class="n">DataType</span><span class="w"> </span><span class="n">out_dtype</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">attrs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_object</span><span class="o">&lt;</span><span class="n">DenseAttrs</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="n">attrs</span><span class="o">-&gt;</span><span class="n">units</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">units</span><span class="p">;</span>
<span class="w">  </span><span class="n">attrs</span><span class="o">-&gt;</span><span class="n">out_dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">out_dtype</span><span class="p">;</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Op</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Op</span><span class="o">::</span><span class="n">Get</span><span class="p">(</span><span class="s">&quot;nn.dense&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Call</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">},</span><span class="w"> </span><span class="n">Attrs</span><span class="p">(</span><span class="n">attrs</span><span class="p">),</span><span class="w"> </span><span class="p">{});</span>
<span class="p">}</span>

<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">&quot;relay.op.nn._make.dense&quot;</span><span class="p">).</span><span class="n">set_body_typed</span><span class="p">(</span><span class="n">MakeDense</span><span class="p">);</span>

<span class="n">RELAY_REGISTER_OP</span><span class="p">(</span><span class="s">&quot;nn.dense&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">describe</span><span class="p">(</span><span class="cm">/*省略*/</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">set_attrs_type</span><span class="o">&lt;</span><span class="n">DenseAttrs</span><span class="o">&gt;</span><span class="p">()</span>
<span class="w">    </span><span class="p">.</span><span class="n">set_num_inputs</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&quot;data&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;nD Tensor&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Input data.&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&quot;weight&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;2D Tensor&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Weight matrix.&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">set_support_level</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">set_attr</span><span class="o">&lt;</span><span class="n">FInferCorrectLayout</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;FInferCorrectLayout&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">DenseInferCorrectLayout</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">add_type_rel</span><span class="p">(</span><span class="s">&quot;Dense&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">MatmulRel</span><span class="o">&lt;</span><span class="n">DenseAttrs</span><span class="o">&gt;</span><span class="p">)</span>
<span class="w">    </span><span class="p">.</span><span class="n">set_attr</span><span class="o">&lt;</span><span class="n">TOpPattern</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;TOpPattern&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">kOutEWiseFusable</span><span class="p">);</span>
</code></pre></div>
<p>可以看到最终返回的就是一个 <code>CallNode</code>， Op 是 <code>dense</code>。</p>
<p>同时注意到 <code>RELAY_REGISTER_OP("nn.dense")</code> 只是设置了输入输出 type 关系，以及其它的一些相关属性， 没有指定 <code>dense</code> 这个算子具体的计算和调度， 那么一个 Relay 算子的计算和调度在哪里指定呢？</p>
<p>答案是在 python 端， 以<code>dense</code>为例， 其调度和计算的指定位于 <code>python/tvm/relay/op/nn/_nn.py</code> 中， 该文件会在 <code>import relay</code> 时被自动加载：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># dense</span>
<span class="n">reg</span><span class="o">.</span><span class="n">register_strategy</span><span class="p">(</span><span class="s2">&quot;nn.dense&quot;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">.</span><span class="n">dense_strategy</span><span class="p">)</span>
</code></pre></div>
<p><code>strategy.dense_strategy</code> 定义如下：</p>
<div class="highlight"><pre><span></span><code><span class="nd">@override_native_generic_func</span><span class="p">(</span><span class="s2">&quot;dense_strategy&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">dense_strategy</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;dense generic strategy&quot;&quot;&quot;</span>
  <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;dense is not optimized for this platform.&quot;</span><span class="p">)</span>
  <span class="n">strategy</span> <span class="o">=</span> <span class="n">_op</span><span class="o">.</span><span class="n">OpStrategy</span><span class="p">()</span>
  <span class="n">strategy</span><span class="o">.</span><span class="n">add_implementation</span><span class="p">(</span>
    <span class="n">wrap_compute_dense</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dense</span><span class="p">),</span>
    <span class="n">wrap_topi_schedule</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">generic</span><span class="o">.</span><span class="n">schedule_dense</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;dense.generic&quot;</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="k">return</span> <span class="n">strategy</span>
</code></pre></div>
<p>其中 <code>topi.nn.dense</code> 和 <code>topi.generic.schedule_dense</code> 就是 上一节 TOPI 中预定义好的 TE 表示的算子库中的算子。 而 <code>_op.OpStrategy</code> 对应 C++ 中的数据结构位于 <code>include/tvm/relay/op_strategy.h</code> 中：</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">OpImplementationNode</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Object</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">FTVMCompute</span><span class="w"> </span><span class="n">fcompute</span><span class="p">;</span>
<span class="w">  </span><span class="n">FTVMSchedule</span><span class="w"> </span><span class="n">fschedule</span><span class="p">;</span>
<span class="w">  </span><span class="n">String</span><span class="w"> </span><span class="n">name</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">plevel</span><span class="p">;</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="o">*</span><span class="w"> </span><span class="n">_type_key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;relay.OpImplementation&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">TVM_DECLARE_FINAL_OBJECT_INFO</span><span class="p">(</span><span class="n">OpImplementationNode</span><span class="p">,</span><span class="w"> </span><span class="n">Object</span><span class="p">);</span>
<span class="p">};</span>
</code></pre></div>
<p>其中 <code>FTVMCompute</code> 是 <code>runtime::TypedPackedFunc&lt;Array&lt;te::Tensor&gt;(const Attrs&amp; attrs, const Array&lt;te::Tensor&gt;&amp; inputs, const Type&amp; out_type)&gt;;</code> ， FTVMSchedule 是 <code>runtime::TypedPackedFunc&lt;te::Schedule(const Attrs&amp; attrs, const Array&lt;te::Tensor&gt;&amp; outs, const Target&amp; target)&gt;;</code>；</p>
<p>💡<strong>这样就将 TOPI 和 Relay Op 建立了联系</strong></p>
<p>接下来关注 <code>relay.build</code></p>
<hr />
<p>先暂时忽略 <code>PassContext(opt_level=2)</code>， 直接看 <code>relay.build</code>：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="n">ir_mod</span><span class="p">,</span><span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">executor</span><span class="o">=</span><span class="n">Executor</span><span class="p">(</span><span class="s2">&quot;graph&quot;</span><span class="p">),</span> <span class="n">runtime</span><span class="o">=</span><span class="n">Runtime</span><span class="p">(</span><span class="s2">&quot;cpp&quot;</span><span class="p">),</span>
          <span class="n">workspace_memory_pools</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">constant_memory_pools</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
          <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mod_name</span><span class="o">=</span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
<span class="p">):</span>
  <span class="n">raw_targets</span> <span class="o">=</span> <span class="n">Target</span><span class="o">.</span><span class="n">canon_multi_target_and_host</span><span class="p">(</span><span class="n">Target</span><span class="o">.</span><span class="n">target_or_current</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="n">target_host</span><span class="p">)</span>
  <span class="n">target_host</span> <span class="o">=</span> <span class="n">raw_targets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">host</span>

  <span class="c1"># 如果当前 dispatch context 是 fallback context (the default root context),</span>
  <span class="c1"># 从 TopHub 中 load pre-tuned parameters</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">autotvm</span><span class="o">.</span><span class="n">DispatchContext</span><span class="o">.</span><span class="n">current</span><span class="p">,</span> <span class="n">autotvm</span><span class="o">.</span><span class="n">FallbackContext</span><span class="p">):</span>
    <span class="n">tophub_context</span> <span class="o">=</span> <span class="n">autotvm</span><span class="o">.</span><span class="n">tophub</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">raw_targets</span><span class="p">))</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">tophub_context</span> <span class="o">=</span> <span class="n">autotvm</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">EmptyContext</span><span class="p">()</span>

  <span class="k">with</span> <span class="n">tophub_context</span><span class="p">:</span>
    <span class="n">bld_mod</span> <span class="o">=</span> <span class="n">BuildModule</span><span class="p">()</span>
    <span class="n">graph_json</span><span class="p">,</span> <span class="n">runtime_mod</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">bld_mod</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
      <span class="n">mod</span><span class="o">=</span><span class="n">ir_mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">raw_targets</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
      <span class="n">executor</span><span class="o">=</span><span class="n">executor</span><span class="p">,</span> <span class="n">runtime</span><span class="o">=</span><span class="n">runtime</span><span class="p">,</span>
      <span class="n">workspace_memory_pools</span><span class="o">=</span><span class="n">workspace_memory_pools</span><span class="p">,</span>
      <span class="n">constant_memory_pools</span><span class="o">=</span><span class="n">constant_memory_pools</span><span class="p">,</span>
      <span class="n">mod_name</span><span class="o">=</span><span class="n">mod_name</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># build 之后 bld_mod 中包含 IRModule（其中为 PrimFunc）</span>
    <span class="n">func_metadata</span> <span class="o">=</span> <span class="n">bld_mod</span><span class="o">.</span><span class="n">get_function_metadata</span><span class="p">()</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">bld_mod</span><span class="o">.</span><span class="n">get_devices</span><span class="p">()</span>
    <span class="n">lowered_ir_mods</span> <span class="o">=</span> <span class="n">bld_mod</span><span class="o">.</span><span class="n">get_irmodule</span><span class="p">()</span>
    <span class="n">executor_codegen_metadata</span> <span class="o">=</span> <span class="n">bld_mod</span><span class="o">.</span><span class="n">get_executor_codegen_metadata</span><span class="p">()</span>
    <span class="c1"># 这里只看 graph_executor 去掉了aot 相关</span>
    <span class="k">if</span> <span class="n">executor</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;graph&quot;</span><span class="p">:</span>
      <span class="n">executor_factory</span> <span class="o">=</span> <span class="n">_executor_factory</span><span class="o">.</span><span class="n">GraphExecutorFactoryModule</span><span class="p">(</span>
        <span class="n">ir_mod</span><span class="p">,</span> <span class="n">raw_targets</span><span class="p">,</span> <span class="c1"># 原始(Relay)IRModule; build target</span>
        <span class="n">executor</span><span class="p">,</span> <span class="n">graph_json</span><span class="p">,</span> <span class="n">runtime_mod</span><span class="p">,</span>  <span class="c1"># executor-config; graph信息; build后的 runtime module</span>
        <span class="n">mod_name</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">func_metadata</span><span class="p">,</span>  <span class="c1"># runtime.module name; 模型参数; 函数信息</span>
      <span class="p">)</span>
    <span class="k">return</span> <span class="n">executor_factory</span>
</code></pre></div>
<p>函数最终返回一个 <code>relay.backend.executor_factory.ExecutorFactoryModule</code> ， 是relay的 graph executor factory（ <code>ExecutorFactoryModule</code> 目前有 graph 和 aot 两种， 在本例子中为 <code>graph_executor</code>）</p>
<p>整个构建流程如下：</p>
<ol>
<li>
<p>TopHub 寻找历史优化信息:</p>
<blockquote>
<p>首先 Relay 会寻找是否有 AutoTVM 预先 Fintune 的记录，如果没有那么就使用autotvm.FallbackContext这个环境上下文信息，如果有那么接下来的所有操作都在 tophub_context 的 scope 之下 (with tophub_context:)。值得一提的是 Relay 考虑了异构情景下的代码生成，用户可以指定多个生成代码的目标 (target)。</p>
</blockquote>
<p>TODO:</p>
</li>
<li>
<p>在 <code>tophub_context</code>中，创建了一个 <code>BuildModule</code> ，调用 <code>build</code> 。 <code>bld_mod.build</code> 在 python 端的返回值有三个: <code>executor_config, mod, params</code>； 其中 <code>executor_config</code> 是一个 json-like form 的config， 用于给后续生成给 graph_executor 提供信息； <code>mod</code> 是包含各种必需运行时库的 <code>runtime.module</code>； <code>params</code> 是优化后的计算图的参数。</p>
</li>
<li>
<p><code>BuildModule()</code> 对应在 C++ 中的 <code>src/relay/backend/build_module.cc</code> 中:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">RelayBuildModule</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">runtime</span><span class="o">::</span><span class="n">ModuleNode</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">protected</span><span class="o">:</span>
<span class="w">  </span><span class="c1">// return The updated Relay IR module after optimization.</span>
<span class="w">  </span><span class="n">IRModule</span><span class="w"> </span><span class="n">Optimize</span><span class="p">(</span><span class="n">IRModule</span><span class="w"> </span><span class="n">relay_module</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Array</span><span class="o">&lt;</span><span class="n">Target</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">raw_targets</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Compile a Relay IR module to runtime module. 结果保存在 `this-&gt;ret_`</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">BuildRelay</span><span class="p">(</span><span class="n">IRModule</span><span class="w"> </span><span class="n">relay_module</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">String</span><span class="o">&amp;</span><span class="w"> </span><span class="n">mod_name</span><span class="p">);</span>

<span class="w"> </span><span class="k">protected</span><span class="o">:</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">ExecutorCodegen</span><span class="o">&gt;</span><span class="w"> </span><span class="n">executor_codegen_</span><span class="p">;</span>
<span class="w">  </span><span class="n">Executor</span><span class="w"> </span><span class="n">executor_</span><span class="p">;</span><span class="w">     </span><span class="c1">// Executor to build for</span>
<span class="w">  </span><span class="n">Runtime</span><span class="w"> </span><span class="n">runtime_</span><span class="p">;</span><span class="w">       </span><span class="c1">// Runtime to codegen for</span>
<span class="w">  </span><span class="n">WorkspaceMemoryPools</span><span class="w"> </span><span class="n">workspace_memory_pools_</span><span class="p">;</span>
<span class="w">  </span><span class="n">ConstantMemoryPools</span><span class="w"> </span><span class="n">constant_memory_pools_</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">runtime</span><span class="o">::</span><span class="n">NDArray</span><span class="o">&gt;</span><span class="w"> </span><span class="n">params_</span><span class="p">;</span><span class="w"> </span><span class="c1">// parameters</span>
<span class="w">  </span><span class="n">BuildOutput</span><span class="w"> </span><span class="n">ret_</span><span class="p">;</span><span class="w"> </span><span class="c1">// building output</span>
<span class="w">  </span><span class="n">CompilationConfig</span><span class="w"> </span><span class="n">config_</span><span class="p">;</span>
<span class="p">};</span>
</code></pre></div>
<p><code>bld_mod.build</code> 直接对应上述 <code>Build</code> 函数，该函简单地把函数参数如 runtime， executor， config 等赋值给对应的fileds， 接着调用上述代码中的 <code>BuildRelay</code> 函数，这是整个 build 中的关键部分</p>
</li>
<li>
<p><code>BuildRelay</code> 函数主要逻辑如下：</p>
<div class="highlight"><pre><span></span><code><span class="kt">void</span><span class="w"> </span><span class="nf">BuildRelay</span><span class="p">(</span><span class="n">IRModule</span><span class="w"> </span><span class="n">relay_module</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">String</span><span class="o">&amp;</span><span class="w"> </span><span class="n">mod_name</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// 1. Relay IRModule -&gt; IRModule 优化 (TODO: 展开)</span>
<span class="w">  </span><span class="n">relay_module</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">OptimizeImpl</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">relay_module</span><span class="p">));</span>

<span class="w">  </span><span class="n">Function</span><span class="w"> </span><span class="n">func</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Downcast</span><span class="o">&lt;</span><span class="n">Function</span><span class="o">&gt;</span><span class="p">(</span><span class="n">relay_module</span><span class="o">-&gt;</span><span class="n">Lookup</span><span class="p">(</span><span class="s">&quot;main&quot;</span><span class="p">));</span>
<span class="w">  </span><span class="n">LOG</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">)</span><span class="o">&lt;&lt;</span><span class="s">&quot;After `OptimizeImpl`: relay_module-&gt;Lookup(&#39;main&#39;)&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">PrettyPrint</span><span class="p">(</span><span class="n">func</span><span class="p">);</span><span class="w"> </span><span class="c1">// my print</span>

<span class="w">  </span><span class="n">IRModule</span><span class="w"> </span><span class="n">func_module</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">WithAttrs</span><span class="p">(</span><span class="n">IRModule</span><span class="o">::</span><span class="n">FromExpr</span><span class="p">(</span><span class="n">func</span><span class="p">),{</span><span class="cm">/*executor 等 attrs...*/</span><span class="p">});</span>

<span class="w">  </span><span class="c1">// 2. 为优化后的函数执行 codegen</span>
<span class="w">  </span><span class="c1">// 2.1. 如果是 graph_executor 创建一个 `GraphCodegen` 对象</span>
<span class="w">  </span><span class="n">executor_codegen_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MakeExecutorCodegen</span><span class="p">(</span><span class="n">executor_</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">);</span>
<span class="w">  </span><span class="n">executor_codegen_</span><span class="o">-&gt;</span><span class="n">Init</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">config_</span><span class="o">-&gt;</span><span class="n">primitive_targets</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// 这里的codegen 对应 GraphExecutorCodegen::Codegen</span>
<span class="w">  </span><span class="c1">//  a) 设置 memory_plan_</span>
<span class="w">  </span><span class="c1">//  b) LowerTE, lower 之后的 IR 在 tir level </span>
<span class="w">  </span><span class="n">executor_codegen_</span><span class="o">-&gt;</span><span class="n">Codegen</span><span class="p">(</span><span class="n">func_module</span><span class="p">,</span><span class="w"> </span><span class="n">func</span><span class="p">,</span><span class="w"> </span><span class="n">mod_name</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// 为 ret_ 设置 graph_json</span>
<span class="w">  </span><span class="n">executor_codegen_</span><span class="o">-&gt;</span><span class="n">UpdateOutput</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ret_</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// ret_ 包括: 1) graph_json 字符串; 2) runtime.mod; 3)[string: NDArray] params 字典;</span>
<span class="w">  </span><span class="c1">// 这里设置 executor_codegen_ 在 codegen 阶段构造的 params </span>
<span class="w">  </span><span class="n">ret_</span><span class="p">.</span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">executor_codegen_</span><span class="o">-&gt;</span><span class="n">GetParams</span><span class="p">();</span>

<span class="w">  </span><span class="c1">// 2.2. 拿到 lowerd_funcs （此时Relay函数已经下降为 PrimFunc）</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">lowered_funcs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">executor_codegen_</span><span class="o">-&gt;</span><span class="n">GetIRModule</span><span class="p">();</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Target</span><span class="o">&amp;</span><span class="w"> </span><span class="n">host_target</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">config_</span><span class="o">-&gt;</span><span class="n">host_virtual_device</span><span class="o">-&gt;</span><span class="n">target</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">runtime</span><span class="o">::</span><span class="n">PackedFunc</span><span class="o">*</span><span class="w"> </span><span class="n">pf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">runtime</span><span class="o">::</span><span class="n">Registry</span><span class="o">::</span><span class="n">Get</span><span class="p">(</span><span class="s">&quot;codegen.LLVMModuleCreate&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// 2.3. 如果由于优化等原因， lowered_funcs 集合为空时，返回空 module</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">lowered_funcs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="cm">/* 省略...*/</span><span class="p">}</span><span class="w"> </span>
<span class="w">  </span><span class="c1">// 3. 否则执行 TIRToRuntime， 可以参考 tir 下降一节中的步骤</span>
<span class="w">  </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="n">ret_</span><span class="p">.</span><span class="n">mod</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">TIRToRuntime</span><span class="p">(</span><span class="n">lowered_funcs</span><span class="p">,</span><span class="w"> </span><span class="n">host_target</span><span class="p">);}</span>

<span class="w">  </span><span class="c1">// 4. 接下来是对 external module 的处理</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</code></pre></div>
<p><code>BuildRelay</code> 包含了 Optimize ， Codegen 两个过程：</p>
<ol>
<li>
<p>在 <code>Build</code> 之前： Relay 阶段的 IRModule：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nd">@main</span><span class="p">(</span><span class="o">%</span><span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">float32</span><span class="p">],</span> <span class="o">%</span><span class="n">weight1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">float32</span><span class="p">],</span> <span class="o">%</span><span class="n">bias1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[(</span><span class="mi">128</span><span class="p">),</span> <span class="n">float32</span><span class="p">],</span> <span class="o">%</span><span class="n">weight2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">float32</span><span class="p">],</span> <span class="o">%</span><span class="n">bias2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">[(</span><span class="mi">10</span><span class="p">),</span> <span class="n">float32</span><span class="p">])</span> <span class="p">{</span>
  <span class="o">%</span><span class="mi">0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="o">%</span><span class="n">data</span><span class="p">,</span> <span class="o">%</span><span class="n">weight1</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="kc">None</span><span class="p">);</span>
  <span class="o">%</span><span class="mi">1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="o">%</span><span class="mi">0</span><span class="p">,</span> <span class="o">%</span><span class="n">bias1</span><span class="p">);</span>
  <span class="o">%</span><span class="mi">2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="o">%</span><span class="mi">1</span><span class="p">);</span>
  <span class="o">%</span><span class="mi">3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="o">%</span><span class="mi">2</span><span class="p">,</span> <span class="o">%</span><span class="n">weight2</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="kc">None</span><span class="p">);</span>
  <span class="o">%</span><span class="mi">4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="o">%</span><span class="mi">3</span><span class="p">,</span> <span class="o">%</span><span class="n">bias2</span><span class="p">);</span>
  <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="o">%</span><span class="mi">4</span><span class="p">)</span>
<span class="p">}</span>
</code></pre></div>
</li>
<li>
<p>在 <code>Optmize</code> 之后， 于打印出这时的 IR 发现是以 Tensor 为操作单位，即转为了 TE的表示，并且在TE层级进行了算子融合，可以看到，在Relay中的 <code>nn.dense</code> Op, 以及 <code>nn.bias_add</code> Op, <code>nn.relu</code> Op 在这里的 IR 中都被转为TE表示，并且三个算子被包裹到了一个函数中，在接下来的 codegen 中， 三个TE函数将会被融合成一个计算过程：</p>
<div class="highlight"><pre><span></span><code><span class="n">fn</span><span class="w"> </span><span class="p">(</span>
<span class="w">  </span><span class="o">%</span><span class="n">data</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="n">weight1</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="n">bias1</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">128</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="n">weight2</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="n">bias2</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">10</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span>
<span class="w">  </span><span class="n">dst_layout</span><span class="o">=</span><span class="s">&quot;NC5n&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">executor</span><span class="o">=</span><span class="n">meta</span><span class="p">[</span><span class="n">Executor</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">runtime</span><span class="o">=</span><span class="n">meta</span><span class="p">[</span><span class="n">Runtime</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">hash</span><span class="o">=</span><span class="s">&quot;e88b28184aebb4db&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="n">src_layout</span><span class="o">=</span><span class="s">&quot;NC&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">virtual_device</span><span class="o">=</span><span class="n">VirtualDevice</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">virtual_device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="o">=</span><span class="n">Target</span><span class="p">(</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="n">d4d3b79920</span><span class="p">,</span><span class="w"> </span><span class="n">kind</span><span class="o">=</span><span class="err">&#39;</span><span class="n">llvm</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">keys</span><span class="o">=</span><span class="p">{</span><span class="err">&#39;</span><span class="n">cpu</span><span class="err">&#39;</span><span class="p">},</span><span class="w"> </span><span class="n">host</span><span class="o">=</span><span class="n">Target</span><span class="p">(</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="n">d4d3b79a00</span><span class="p">,</span><span class="w"> </span><span class="n">kind</span><span class="o">=</span><span class="err">&#39;</span><span class="n">llvm</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">keys</span><span class="o">=</span><span class="p">{</span><span class="err">&#39;</span><span class="n">cpu</span><span class="err">&#39;</span><span class="p">})))</span>
<span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="c1">// 数据 layout 转换</span>
<span class="w">  </span><span class="o">%</span><span class="mi">6</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fn</span><span class="w"> </span><span class="p">(</span><span class="o">%</span><span class="n">p02</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="n">Primitive</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">hash</span><span class="o">=</span><span class="s">&quot;e9662aa5b8e67b96&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">src_layout</span><span class="o">=</span><span class="s">&quot;NC&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">dst_layout</span><span class="o">=</span><span class="s">&quot;NC8n&quot;</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">layout_transform</span><span class="p">(</span><span class="o">%</span><span class="n">p02</span><span class="p">,</span><span class="w"> </span><span class="n">src_layout</span><span class="o">=</span><span class="s">&quot;NC&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">dst_layout</span><span class="o">=</span><span class="s">&quot;NC8n&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span><span class="w">  </span>
<span class="w">  </span><span class="o">%</span><span class="mi">7</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">%</span><span class="mi">6</span><span class="p">(</span><span class="o">%</span><span class="n">weight1</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// 对应 Relay function IR 中的</span>
<span class="w">  </span><span class="c1">// %0 = nn.dense(%data, %weight1, units=None);</span>
<span class="w">  </span><span class="c1">// %1 = nn.bias_add(%0, %bias1);</span>
<span class="w">  </span><span class="c1">// %2 = nn.relu(%1);</span>
<span class="w">  </span><span class="o">%</span><span class="mi">8</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fn</span><span class="w"> </span><span class="p">(</span><span class="o">%</span><span class="n">p01</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span><span class="o">%</span><span class="n">p11</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span><span class="o">%</span><span class="n">p21</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">128</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span><span class="n">Primitive</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">hash</span><span class="o">=</span><span class="s">&quot;f360b4c42be956c4&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">weight_layout</span><span class="o">=</span><span class="s">&quot;NC8n&quot;</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="o">%</span><span class="mi">3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">contrib_dense_pack</span><span class="p">(</span><span class="o">%</span><span class="n">p01</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="n">p11</span><span class="p">,</span><span class="w"> </span><span class="n">units</span><span class="o">=</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">out_dtype</span><span class="o">=</span><span class="s">&quot;float32&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">weight_layout</span><span class="o">=</span><span class="s">&quot;NC8n&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="o">%</span><span class="mi">4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expand_dims</span><span class="p">(</span><span class="o">%</span><span class="n">p21</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="o">%</span><span class="mi">5</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">add</span><span class="p">(</span><span class="o">%</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">4</span><span class="p">);</span>
<span class="w">    </span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="o">%</span><span class="mi">5</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span>

<span class="w">  </span><span class="c1">// 数据 Layout 转换</span>
<span class="w">  </span><span class="o">%</span><span class="mi">9</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fn</span><span class="w"> </span><span class="p">(</span><span class="o">%</span><span class="n">p03</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span><span class="n">Primitive</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">hash</span><span class="o">=</span><span class="s">&quot;86451ec737a6a453&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">src_layout</span><span class="o">=</span><span class="s">&quot;NC&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">dst_layout</span><span class="o">=</span><span class="s">&quot;NC5n&quot;</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">layout_transform</span><span class="p">(</span><span class="o">%</span><span class="n">p03</span><span class="p">,</span><span class="w"> </span><span class="n">src_layout</span><span class="o">=</span><span class="s">&quot;NC&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">dst_layout</span><span class="o">=</span><span class="s">&quot;NC5n&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="cm">/* ty=fn (Tensor[(10, 128), float32]) -&gt; Tensor[(2, 128, 5), float32] */</span><span class="p">;</span>
<span class="w">  </span><span class="o">%</span><span class="mi">10</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">%</span><span class="mi">8</span><span class="p">(</span><span class="o">%</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="n">bias1</span><span class="p">);</span>
<span class="w">  </span><span class="o">%</span><span class="mi">11</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">%</span><span class="mi">9</span><span class="p">(</span><span class="o">%</span><span class="n">weight2</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// 对应 Relay function IR 中的</span>
<span class="w">  </span><span class="c1">// %3 = nn.dense(%2, %weight2, units=None);</span>
<span class="w">  </span><span class="c1">// %4 = nn.bias_add(%3, %bias2);</span>
<span class="w">  </span><span class="c1">// nn.relu(%4)</span>
<span class="w">  </span><span class="o">%</span><span class="mi">12</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fn</span><span class="w"> </span><span class="p">(</span><span class="o">%</span><span class="n">p0</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span><span class="o">%</span><span class="n">p1</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span><span class="o">%</span><span class="n">p2</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">10</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">],</span><span class="w"> </span><span class="n">Primitive</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">hash</span><span class="o">=</span><span class="s">&quot;32a532a5919d3a8b&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">weight_layout</span><span class="o">=</span><span class="s">&quot;NC5n&quot;</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="o">%</span><span class="mi">0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="n">contrib_dense_pack</span><span class="p">(</span><span class="o">%</span><span class="n">p0</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="n">p1</span><span class="p">,</span><span class="w"> </span><span class="n">units</span><span class="o">=</span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">out_dtype</span><span class="o">=</span><span class="s">&quot;float32&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">weight_layout</span><span class="o">=</span><span class="s">&quot;NC5n&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="o">%</span><span class="mi">1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">expand_dims</span><span class="p">(</span><span class="o">%</span><span class="n">p2</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="o">%</span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">add</span><span class="p">(</span><span class="o">%</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="o">%</span><span class="mi">2</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="mi">12</span><span class="p">(</span><span class="o">%</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">11</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="n">bias2</span><span class="p">)</span>
<span class="p">}</span><span class="w"> </span>
</code></pre></div>
</li>
<li>
<p>接着是 <code>CodeGen</code> (<code>GraphExecutorCodegen</code> 的方法)中对 TE 的 lower， 对应到 <code>tec::LowerTE</code> 函数的调用:</p>
<div class="highlight"><pre><span></span><code><span class="c1">// In GraphExecutorCodegen:</span>
<span class="c1">// relay::backend::LoweredOutput Codegen(tvm::IRModule mod, relay::Function func,tvm::runtime::String mod_name)</span>

<span class="p">...</span>

<span class="n">IRModule</span><span class="w"> </span><span class="n">lowered_mod</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tec</span><span class="o">::</span><span class="n">LowerTE</span><span class="p">(</span><span class="n">mod_name_</span><span class="p">,</span><span class="w"> </span><span class="n">config_</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="k">this</span><span class="p">](</span><span class="n">BaseFunc</span><span class="w"> </span><span class="n">func</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">func</span><span class="o">-&gt;</span><span class="n">GetAttr</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span><span class="p">(</span><span class="n">attr</span><span class="o">::</span><span class="n">kCompiler</span><span class="p">).</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">UpdateConstants</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">params_</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">tec</span><span class="o">::</span><span class="n">UpdateFunctionMetadata</span><span class="p">(</span><span class="n">func</span><span class="p">,</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">function_metadata_</span><span class="p">);</span>
<span class="p">})(</span><span class="n">mod</span><span class="p">);</span>
<span class="n">LOG</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;after complile TE:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">PrettyPrint</span><span class="p">(</span><span class="n">lowered_mod</span><span class="p">);</span><span class="w"> </span><span class="c1">// dmhj</span>

<span class="p">...</span>
</code></pre></div>
<p>TE 被 lower 之后的 IR 由 tir Stmt 组成， TE 函数被转换成 PrimFunc(元张量函数)， 在 下面的 IR 中 可以看到出现了 <code>Pointer</code>, <code>Buffer</code>, <code>AllocateNode</code> 等 Stmt 节点，这是在 TE 和 Relay 层级不会出现的抽象</p>
<div class="highlight"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="err">@</span><span class="n">main</span><span class="p">(</span>
<span class="w">  </span><span class="o">%</span><span class="n">data</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="n">weight1</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="n">bias1</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">128</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="n">weight2</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="o">%</span><span class="n">bias2</span><span class="o">:</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">10</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">,</span><span class="w"> </span>

<span class="w">  </span><span class="n">dst_layout</span><span class="o">=</span><span class="s">&quot;NC5n&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">executor</span><span class="o">=</span><span class="n">meta</span><span class="p">[</span><span class="n">Executor</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">runtime</span><span class="o">=</span><span class="n">meta</span><span class="p">[</span><span class="n">Runtime</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">hash</span><span class="o">=</span><span class="s">&quot;e88b28184aebb4db&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">src_layout</span><span class="o">=</span><span class="s">&quot;NC&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">  </span><span class="n">virtual_device</span><span class="o">=</span><span class="n">VirtualDevice</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">virtual_device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">target</span><span class="o">=</span><span class="n">Target</span><span class="p">(</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="n">d233c937a0</span><span class="p">,</span><span class="w"> </span><span class="n">kind</span><span class="o">=</span><span class="err">&#39;</span><span class="n">llvm</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">keys</span><span class="o">=</span><span class="p">{</span><span class="err">&#39;</span><span class="n">cpu</span><span class="err">&#39;</span><span class="p">},</span><span class="w"> </span><span class="n">host</span><span class="o">=</span><span class="n">Target</span><span class="p">(</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="n">d233c94220</span><span class="p">,</span><span class="w"> </span><span class="n">kind</span><span class="o">=</span><span class="err">&#39;</span><span class="n">llvm</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">keys</span><span class="o">=</span><span class="p">{</span><span class="err">&#39;</span><span class="n">cpu</span><span class="err">&#39;</span><span class="p">})))</span>
<span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="p">[(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="o">%</span><span class="mi">0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">%</span><span class="n">weight1</span><span class="p">,)</span><span class="w"> </span><span class="p">;</span>
<span class="w">  </span><span class="o">%</span><span class="mi">1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">call_lowered</span><span class="p">(</span><span class="err">@</span><span class="n">tvmgen_default_fused_layout_transform</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;relay_attrs&quot;</span><span class="o">=</span><span class="p">{</span><span class="n">__dict__</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;Primitive&quot;</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;hash&quot;</span><span class="o">=</span><span class="s">&quot;e9662aa5b8e67b96&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;src_layout&quot;</span><span class="o">=</span><span class="s">&quot;NC&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;dst_layout&quot;</span><span class="o">=</span><span class="s">&quot;NC8n&quot;</span><span class="p">}},</span><span class="w"> </span><span class="s">&quot;all_prim_fn_vars&quot;</span><span class="o">=</span><span class="p">[</span><span class="err">&#39;</span><span class="n">tvmgen_default_fused_layout_transform</span><span class="err">&#39;</span><span class="p">]})</span><span class="w"> </span><span class="p">;</span>
<span class="w">  </span><span class="o">%</span><span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">%</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="n">bias1</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
<span class="w">  </span><span class="o">%</span><span class="mi">3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">%</span><span class="n">weight2</span><span class="p">,)</span><span class="w"> </span><span class="p">;</span>
<span class="w">  </span><span class="o">%</span><span class="mi">4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">call_lowered</span><span class="p">(</span><span class="err">@</span><span class="n">tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;relay_attrs&quot;</span><span class="o">=</span><span class="p">{</span><span class="n">__dict__</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;Primitive&quot;</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;hash&quot;</span><span class="o">=</span><span class="s">&quot;f360b4c42be956c4&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;weight_layout&quot;</span><span class="o">=</span><span class="s">&quot;NC8n&quot;</span><span class="p">}},</span><span class="w"> </span><span class="s">&quot;all_prim_fn_vars&quot;</span><span class="o">=</span><span class="p">[</span><span class="err">&#39;</span><span class="n">tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu</span><span class="err">&#39;</span><span class="p">]})</span><span class="w"> </span><span class="p">;</span>
<span class="w">  </span><span class="o">%</span><span class="mi">5</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">call_lowered</span><span class="p">(</span><span class="err">@</span><span class="n">tvmgen_default_fused_layout_transform_1</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;relay_attrs&quot;</span><span class="o">=</span><span class="p">{</span><span class="n">__dict__</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;Primitive&quot;</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;hash&quot;</span><span class="o">=</span><span class="s">&quot;86451ec737a6a453&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;src_layout&quot;</span><span class="o">=</span><span class="s">&quot;NC&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;dst_layout&quot;</span><span class="o">=</span><span class="s">&quot;NC5n&quot;</span><span class="p">}},</span><span class="w"> </span><span class="s">&quot;all_prim_fn_vars&quot;</span><span class="o">=</span><span class="p">[</span><span class="err">&#39;</span><span class="n">tvmgen_default_fused_layout_transform_1</span><span class="err">&#39;</span><span class="p">]})</span><span class="w"> </span><span class="p">;</span>
<span class="w">  </span><span class="o">%</span><span class="mi">6</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">%</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="n">bias2</span><span class="p">)</span><span class="w"> </span><span class="p">;</span>
<span class="w">  </span><span class="n">call_lowered</span><span class="p">(</span><span class="err">@</span><span class="n">tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu_1</span><span class="p">,</span><span class="w"> </span><span class="o">%</span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;relay_attrs&quot;</span><span class="o">=</span><span class="p">{</span><span class="n">__dict__</span><span class="o">=</span><span class="p">{</span><span class="s">&quot;Primitive&quot;</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;hash&quot;</span><span class="o">=</span><span class="s">&quot;32a532a5919d3a8b&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;weight_layout&quot;</span><span class="o">=</span><span class="s">&quot;NC5n&quot;</span><span class="p">}},</span><span class="w"> </span><span class="s">&quot;all_prim_fn_vars&quot;</span><span class="o">=</span><span class="p">[</span><span class="err">&#39;</span><span class="n">tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu_1</span><span class="err">&#39;</span><span class="p">]})</span><span class="w"> </span>
<span class="p">}</span>

<span class="err">@</span><span class="n">tvmgen_default_fused_layout_transform</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">primfn</span><span class="p">(</span><span class="n">p0_1</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">T_layout_trans_1</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">()</span>
<span class="w">  </span><span class="n">buffers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nl">p0</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p0_2</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">],</span><span class="w"> </span><span class="p">[]),</span>
<span class="w">    </span><span class="nl">T_layout_trans</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">T_layout_trans_2</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">],</span><span class="w"> </span><span class="p">[])</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">buffer_map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">p0_1</span><span class="o">:</span><span class="w"> </span><span class="n">p0</span><span class="p">,</span><span class="w"> </span><span class="n">T_layout_trans_1</span><span class="o">:</span><span class="w"> </span><span class="n">T_layout_trans</span><span class="p">}</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">ax0</span><span class="p">.</span><span class="n">ax1</span><span class="p">.</span><span class="n">fused</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">12544</span><span class="p">)</span><span class="w"> </span><span class="s">&quot;parallel&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nl">T_layout_trans_3</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">T_layout_trans_2</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">100352</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">ramp</span><span class="p">((</span><span class="n">ax0</span><span class="p">.</span><span class="n">ax1</span><span class="p">.</span><span class="n">fused</span><span class="o">*</span><span class="mi">8</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">)]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>
<span class="w">      </span><span class="nl">p0_3</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p0_2</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">100352</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span>
<span class="w">        </span><span class="n">ramp</span><span class="p">(((</span><span class="n">floordiv</span><span class="p">(</span><span class="n">ax0</span><span class="p">.</span><span class="n">ax1</span><span class="p">.</span><span class="n">fused</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">)</span><span class="o">*</span><span class="mi">6272</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">floormod</span><span class="p">(</span><span class="n">ax0</span><span class="p">.</span><span class="n">ax1</span><span class="p">.</span><span class="n">fused</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">)),</span><span class="w"> </span><span class="mi">784</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">)</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="err">@</span><span class="n">tvmgen_default_fused_layout_transform_1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">primfn</span><span class="p">(</span><span class="n">p0_5</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">T_layout_trans_5</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">()</span>
<span class="w">  </span><span class="n">buffers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nl">p0_4</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p0_6</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[]),</span>
<span class="w">    </span><span class="nl">T_layout_trans_4</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">T_layout_trans_6</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">],</span><span class="w"> </span><span class="p">[])</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">buffer_map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">p0_5</span><span class="o">:</span><span class="w"> </span><span class="n">p0_4</span><span class="p">,</span><span class="w"> </span><span class="n">T_layout_trans_5</span><span class="o">:</span><span class="w"> </span><span class="n">T_layout_trans_4</span><span class="p">}</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">ax0</span><span class="p">.</span><span class="n">ax1</span><span class="p">.</span><span class="n">fused_1</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">)</span><span class="w"> </span><span class="s">&quot;parallel&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nl">T_layout_trans_7</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">T_layout_trans_6</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1280</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">ramp</span><span class="p">((</span><span class="n">ax0</span><span class="p">.</span><span class="n">ax1</span><span class="p">.</span><span class="n">fused_1</span><span class="o">*</span><span class="mi">5</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>
<span class="w">      </span><span class="nl">p0_7</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p0_6</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1280</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span>
<span class="w">        </span><span class="n">ramp</span><span class="p">(((</span><span class="n">floordiv</span><span class="p">(</span><span class="n">ax0</span><span class="p">.</span><span class="n">ax1</span><span class="p">.</span><span class="n">fused_1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">)</span><span class="o">*</span><span class="mi">640</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">floormod</span><span class="p">(</span><span class="n">ax0</span><span class="p">.</span><span class="n">ax1</span><span class="p">.</span><span class="n">fused_1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">)),</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="err">@</span><span class="n">tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">primfn</span><span class="p">(</span><span class="n">p0_9</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">p1_1</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">p2_1</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">T_relu_1</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">()</span>
<span class="w">  </span><span class="n">attr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;from_legacy_te_schedule&quot;</span><span class="o">:</span><span class="w"> </span><span class="n">True</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;target&quot;</span><span class="o">:</span><span class="w"> </span><span class="n">Target</span><span class="p">(</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="n">d233c937a0</span><span class="p">,</span><span class="w"> </span><span class="n">kind</span><span class="o">=</span><span class="err">&#39;</span><span class="n">llvm</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">keys</span><span class="o">=</span><span class="p">{</span><span class="err">&#39;</span><span class="n">cpu</span><span class="err">&#39;</span><span class="p">},</span><span class="w"> </span><span class="n">host</span><span class="o">=</span><span class="n">Target</span><span class="p">(</span><span class="n">id</span><span class="o">=</span><span class="mi">1</span><span class="n">d233c94220</span><span class="p">,</span><span class="w"> </span><span class="n">kind</span><span class="o">=</span><span class="err">&#39;</span><span class="n">llvm</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">keys</span><span class="o">=</span><span class="p">{</span><span class="err">&#39;</span><span class="n">cpu</span><span class="err">&#39;</span><span class="p">})),</span><span class="w"> </span><span class="s">&quot;tir.noalias&quot;</span><span class="o">:</span><span class="w"> </span><span class="n">True</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;hash&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;f360b4c42be956c4&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;global_symbol&quot;</span><span class="o">:</span><span class="w"> </span><span class="s">&quot;tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu&quot;</span><span class="p">}</span>
<span class="w">  </span><span class="n">buffers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nl">p0_8</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p0_10</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">],</span><span class="w"> </span><span class="p">[]),</span>
<span class="w">    </span><span class="nl">p1</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p1_2</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">],</span><span class="w"> </span><span class="p">[]),</span>
<span class="w">    </span><span class="nl">p2</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p2_2</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[]),</span>
<span class="w">    </span><span class="nl">T_relu</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">T_relu_2</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[])</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">buffer_map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">p0_9</span><span class="o">:</span><span class="w"> </span><span class="n">p0_8</span><span class="p">,</span><span class="w"> </span><span class="n">p1_1</span><span class="o">:</span><span class="w"> </span><span class="n">p1</span><span class="p">,</span><span class="w"> </span><span class="n">p2_1</span><span class="o">:</span><span class="w"> </span><span class="n">p2</span><span class="p">,</span><span class="w"> </span><span class="n">T_relu_1</span><span class="o">:</span><span class="w"> </span><span class="n">T_relu</span><span class="p">}</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">ax1</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">ax0</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="s">&quot;parallel&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">allocate</span><span class="p">(</span><span class="n">compute</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">global</span><span class="w"> </span><span class="n">float32x8</span><span class="p">),</span><span class="w"> </span><span class="n">float32x8</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span><span class="w"> </span><span class="n">storage_scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">global</span><span class="p">;</span>
<span class="w">    </span><span class="n">allocate</span><span class="p">(</span><span class="n">compute</span><span class="p">.</span><span class="n">global</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">global</span><span class="w"> </span><span class="n">float32x8</span><span class="p">),</span><span class="w"> </span><span class="n">float32x8</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="w"> </span><span class="n">storage_scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">global</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">compute</span><span class="p">.</span><span class="n">global_1</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">compute</span><span class="p">.</span><span class="n">global</span><span class="p">,</span><span class="w"> </span><span class="n">float32x8</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="p">[],</span><span class="w"> </span><span class="n">align</span><span class="o">=</span><span class="mi">32</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">broadcast</span><span class="p">(</span><span class="mf">0f</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">outer</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">784</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="n">compute</span><span class="p">.</span><span class="n">global_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">compute</span><span class="p">.</span><span class="n">global_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">p0_11</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p0_10</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">784</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">k</span><span class="p">.</span><span class="n">outer</span><span class="p">],</span><span class="w"> </span><span class="mi">8</span><span class="p">)</span><span class="o">*</span><span class="n">p1_3</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p1_2</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">100352</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">ramp</span><span class="p">((((</span><span class="n">ax1</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">ax0</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused</span><span class="o">*</span><span class="mi">25088</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused</span><span class="o">*</span><span class="mi">6272</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">outer</span><span class="o">*</span><span class="mi">8</span><span class="p">)),</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">)]))</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="nl">compute_1</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">compute</span><span class="p">,</span><span class="w"> </span><span class="n">float32x8</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">4</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">y</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">compute</span><span class="p">.</span><span class="n">global_1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">ax1</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">let</span><span class="w"> </span><span class="n">cse_var_1</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">((</span><span class="n">ax1</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">ax0</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused</span><span class="o">*</span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">ax1</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="o">*</span><span class="mi">8</span><span class="p">))</span>
<span class="w">        </span><span class="nl">T_relu_3</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">T_relu_2</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">ramp</span><span class="p">(</span><span class="n">cse_var_1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">)]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max</span><span class="p">((</span><span class="n">compute_1</span><span class="p">[</span><span class="n">ax1</span><span class="p">.</span><span class="n">inner</span><span class="p">.</span><span class="n">outer</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">p2_3</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p2_2</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">ramp</span><span class="p">(</span><span class="n">cse_var_1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">)]),</span><span class="w"> </span><span class="n">broadcast</span><span class="p">(</span><span class="mf">0f</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">))</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="err">@</span><span class="n">tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu_1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">primfn</span><span class="p">(</span>
<span class="w">  </span><span class="nl">p0_13</span><span class="p">:</span><span class="w"> </span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">p1_5</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">p2_5</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span><span class="p">,</span><span class="w"> </span><span class="n">T_relu_5</span><span class="o">:</span><span class="w"> </span><span class="n">handle</span>
<span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="p">()</span>
<span class="w">  </span><span class="n">buffers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nl">p0_12</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p0_14</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[]),</span>
<span class="w">    </span><span class="nl">p1_4</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p1_6</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">],</span><span class="w"> </span><span class="p">[]),</span>
<span class="w">    </span><span class="nl">p2_4</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p2_6</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">10</span><span class="p">],</span><span class="w"> </span><span class="p">[]),</span>
<span class="w">    </span><span class="nl">T_relu_4</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">T_relu_6</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">float32</span><span class="p">),</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">],</span><span class="w"> </span><span class="p">[])</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">buffer_map</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">p0_13</span><span class="o">:</span><span class="w"> </span><span class="n">p0_12</span><span class="p">,</span><span class="w"> </span><span class="n">p1_5</span><span class="o">:</span><span class="w"> </span><span class="n">p1_4</span><span class="p">,</span><span class="w"> </span><span class="n">p2_5</span><span class="o">:</span><span class="w"> </span><span class="n">p2_4</span><span class="p">,</span><span class="w"> </span><span class="n">T_relu_5</span><span class="o">:</span><span class="w"> </span><span class="n">T_relu_4</span><span class="p">}</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">ax1</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">ax0</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused_1</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="s">&quot;parallel&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">let</span><span class="w"> </span><span class="n">cse_var_1_1</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">ax1</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">ax0</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused_1</span><span class="o">*</span><span class="mi">5</span><span class="p">)</span>
<span class="w">    </span><span class="n">allocate</span><span class="p">(</span><span class="n">compute</span><span class="p">.</span><span class="n">global_2</span><span class="o">:</span><span class="w"> </span><span class="n">Pointer</span><span class="p">(</span><span class="n">global</span><span class="w"> </span><span class="n">float32x5</span><span class="p">),</span><span class="w"> </span><span class="n">float32x5</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span><span class="w"> </span><span class="n">storage_scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">global</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">compute</span><span class="p">.</span><span class="n">global_3</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">compute</span><span class="p">.</span><span class="n">global_2</span><span class="p">,</span><span class="w"> </span><span class="n">float32x5</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="p">[],</span><span class="w"> </span><span class="n">align</span><span class="o">=</span><span class="mi">16</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">broadcast</span><span class="p">(</span><span class="mf">0f</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">outer_1</span><span class="o">:</span><span class="w"> </span><span class="n">int32</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">compute</span><span class="p">.</span><span class="n">global_3</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">compute</span><span class="p">.</span><span class="n">global_3</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">broadcast</span><span class="p">(</span><span class="n">p0_15</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p0_14</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">128</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">k</span><span class="p">.</span><span class="n">outer_1</span><span class="p">],</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span><span class="o">*</span><span class="n">p1_7</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p1_6</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1280</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">ramp</span><span class="p">(((</span><span class="n">ax1</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">ax0</span><span class="p">.</span><span class="n">outer</span><span class="p">.</span><span class="n">fused_1</span><span class="o">*</span><span class="mi">640</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="n">outer_1</span><span class="o">*</span><span class="mi">5</span><span class="p">)),</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)]))</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="nl">T_relu_7</span><span class="p">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">T_relu_6</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">10</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">ramp</span><span class="p">(</span><span class="n">cse_var_1_1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">max</span><span class="p">((</span><span class="n">compute</span><span class="p">.</span><span class="n">global_4</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">compute</span><span class="p">.</span><span class="n">global_2</span><span class="p">,</span><span class="w"> </span><span class="n">float32x5</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="p">[],</span><span class="w"> </span><span class="n">align</span><span class="o">=</span><span class="mi">16</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">p2_7</span><span class="o">:</span><span class="w"> </span><span class="n">Buffer</span><span class="p">(</span><span class="n">p2_6</span><span class="p">,</span><span class="w"> </span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="mi">10</span><span class="p">],</span><span class="w"> </span><span class="p">[])[</span><span class="n">ramp</span><span class="p">(</span><span class="n">cse_var_1_1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)]),</span><span class="w"> </span><span class="n">broadcast</span><span class="p">(</span><span class="mf">0f</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">))</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<p>图中 的 IR 省略了 attributes 以及 shape 信息等</p>
</li>
<li>
<p>接下来通过 Visitor 的方式DFS遍历刚才得到的 IR ， 将 IR 中的 CallNode, VarNode 和 ConstantNode 等按照相应的规则转换成对应的 GraphNode; <strong>这里值得注意的是 Relay GraphExecutor 是不支持控制流的，因此如果 Relay IR 中含有 If, Match 等， 在这里会构建失败</strong>：</p>
<div class="highlight"><pre><span></span><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">GraphNodeRef</span><span class="o">&gt;</span><span class="w"> </span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">VarNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Expr</span><span class="w"> </span><span class="n">expr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GetRef</span><span class="o">&lt;</span><span class="n">Expr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">var_map_</span><span class="p">[</span><span class="n">expr</span><span class="p">.</span><span class="n">get</span><span class="p">()];</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">GraphNodeRef</span><span class="o">&gt;</span><span class="w"> </span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">IfNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Graph executor does not support control flow (found IfNode)&quot;</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">GraphNodeRef</span><span class="o">&gt;</span><span class="w"> </span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">ConstructorNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Graph executor does not support ADTs (found ConstructorNode)&quot;</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">GraphNodeRef</span><span class="o">&gt;</span><span class="w"> </span><span class="n">VisitExpr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">GlobalVarNode</span><span class="o">*</span><span class="w"> </span><span class="n">op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">LOG</span><span class="p">(</span><span class="n">FATAL</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;All GlobalVarNodes should be removed before graph executor&#39;s Codegen is called&quot;</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<p>接下来， GraphNode 会被组织在 <code>GraphExecutorCodegen</code> 的 <code>std::vector&lt;GraphObjectPtr&gt; nodes_</code> 中。 这个图最终被写入 graph_json 中:</p>
<div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;nodes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;null&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;data&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;null&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;weight1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;null&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;bias1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;null&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;weight2&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;null&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;bias2&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvm_op&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvmgen_default_fused_layout_transform&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;attrs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;hash&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;e9662aa5b8e67b96&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;src_layout&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NC&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;dst_layout&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NC8n&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;func_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvmgen_default_fused_layout_transform&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;flatten_data&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvm_op&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;attrs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;hash&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;f360b4c42be956c4&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;weight_layout&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NC8n&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;func_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;flatten_data&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvm_op&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvmgen_default_fused_layout_transform_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;attrs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;hash&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;86451ec737a6a453&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;src_layout&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NC&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;dst_layout&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NC5n&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;func_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvmgen_default_fused_layout_transform_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;flatten_data&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">      </span><span class="nt">&quot;op&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvm_op&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu_1&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;attrs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;hash&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;32a532a5919d3a8b&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;weight_layout&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NC5n&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;func_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;tvmgen_default_fused_nn_contrib_dense_pack_expand_dims_add_nn_relu_1&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;flatten_data&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;0&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;num_outputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1&quot;</span>
<span class="w">      </span><span class="p">},</span>
<span class="w">      </span><span class="nt">&quot;inputs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="nt">&quot;arg_nodes&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
<span class="w">  </span><span class="nt">&quot;heads&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">[</span><span class="mi">8</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="w">  </span><span class="p">],</span>
<span class="w">  </span><span class="nt">&quot;attrs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;storage_id&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;list_int&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;shape&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;list_shape&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="p">[</span>
<span class="w">        </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">128</span><span class="p">,</span><span class="mi">784</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">128</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">128</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">10</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">784</span><span class="p">,</span><span class="mi">8</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>
<span class="w">        </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span>
<span class="w">      </span><span class="p">]</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;device_index&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;list_int&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;dltype&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">      </span><span class="s2">&quot;list_str&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="p">[</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span><span class="s2">&quot;float32&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="p">]</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;node_row_ptr&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div>
</li>
</ol>
</li>
<li>
<p>最终 <code>executor_factory = _executor_factory.GraphExecutorFactoryModule()</code> 会将</p>
<ol>
<li>输入的 Relay IR</li>
<li>json表示的 计算执行图</li>
<li>对应的执行器</li>
<li>TIRToRuntime build 出的 runtime.module</li>
<li>代码生成的 target</li>
<li>优化后的计算图的输入参数</li>
<li>mod_name, func_metadata </li>
</ol>
<p>打包成一个module， 可以根据该module中的信息构建一个 graph_executor， 并 利用 graph_executor 加载执行 graph</p>
</li>
</ol>
<p>💡<u><strong>总结一下</strong></u>: </p>
<ol>
<li>
<p>c++ 端的 <code>BuildRelay</code> 函数是通用接口 <code>relay.build</code> 的核心， 在上面过程中， 我们打出了 Relay, TE, TIR, graph_json 等几种不同的中间表示， 从 Relay 到 TE， 从TE 到 TIR， 再从 TIR 中的元张量函数被翻译成机器码， 每一步都会执行相应部分的优化。 至于具体做了哪些优化， TODO:</p>
</li>
<li>
<p>但是需要注意的是通过这条路径，我们只能编译 静态模型， 无论是控制流还是 动态 shape， 支持的都不是很好； Relay 的后续工作 nimble 在这一方面做出了改进，可以参考: <a href="paper-nimble.html">nimble</a>。 简单来讲，我们不再依赖这个简单地 graph_executor, 而是构建了一个虚拟机进行运行时的分析、内存分配、算子派发等，在 Relax 中也是这样做的，因此接下来的一节以 Relax 为例， 看一下 TVM 如何支持动态shape， 动态控制流， 如何使用 VM 进行相应支持。</p>
</li>
</ol>
<h2 id="4-lower-relax">4. Lower Relax</h2>
<p><code>gen_call_tir_inputs</code> 中 <code>_convert_te_arg</code> 注释：</p>
<blockquote>
<p>在动态形状的情况下，传入的参数可能包含 TIR 变量。 例如，参数可以是具有符号形状的 TensorStructInfo 的 Relax Var，或者参数可以是具有符号变量的 ShapeExpr。 为了使生成的 PrimFunc 与调用者 Relax 函数具有自变量，我们将用新的变量替换输入参数中的 TIR 变量，这是通过维护 TIR 变量映射来完成的。</p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.indexes"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.fac441b0.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>
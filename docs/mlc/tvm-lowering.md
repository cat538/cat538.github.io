# TVM-lowering
> Ref:
>
> - [TVM è‡ªåº•å‘ä¸Šï¼ˆäºŒï¼‰ï¼šTIR çš„æ¦‚å¿µå’Œç¼–è¯‘åŸç†](https://zhuanlan.zhihu.com/p/533161438)
> - [ã€ä»é›¶å¼€å§‹å­¦æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ã€‘å››ï¼Œè§£æTVMç®—å­](https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&mid=2247494373&idx=1&sn=6eb14998e8cfe45a144ba1b8c61346ac&chksm=9f835073a8f4d96555ed6382f25d8b5793e7b56da0f403867b7a38c48bbeedc23701c2e59721&scene=178&cur_album_id=1799364124980609027#rd)
> - [ã€ä»é›¶å¼€å§‹å­¦æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨ã€‘å…­ï¼ŒTVMçš„ç¼–è¯‘æµç¨‹è¯¦è§£](http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/%E3%80%90%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8%E3%80%91%E5%85%AD%EF%BC%8CTVM%E7%9A%84%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3/)
> - [TVM å­¦ä¹ æŒ‡å—ï¼ˆä¸ªäººç‰ˆï¼‰](https://mp.weixin.qq.com/s/NM5yvxW2JSbR06RmrR3ubw)


## 1. Lower Tir

> TIRä¸€èˆ¬è®¤ä¸ºæ˜¯ target irï¼ŒTensorIRæ˜¯åé¢åˆæçš„æ–°æ¦‚å¿µï¼Œåšä¸ºtvm scriptçš„ä¸€éƒ¨åˆ†ã€‚

è¿™èŠ‚ç”¨ä¸€ä¸ªä¾‹å­è¿½è¸ª tir è¡¨ç¤ºçš„ PrimFunc æ˜¯å¦‚ä½•ç»è¿‡ target translation è¢«ç¿»è¯‘ä¸ºç‰¹å®štargetçš„ä¸€ä¸ª `runtime.Module`

ä½¿ç”¨çš„ä¾‹å­ä»£ç å¦‚ä¸‹ï¼š

```py
import tvm
from tvm import te
SIZE = 1024

# 1. ä½¿ç”¨ TE å£°æ˜è®¡ç®—
# mm
k = te.reduce_axis((0, SIZE), 'k')
A = te.placeholder((SIZE, SIZE), name='A')
B = te.placeholder((SIZE, SIZE), name='B')
C = te.compute((SIZE, SIZE), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name='MM')
# relu
D_IN = te.placeholder((SIZE, SIZE), name='RELU_IN')
D_OUT = te.compute((SIZE, SIZE), lambda i, j: te.max(D_IN[i, j], 0), "RELU_OUT")

# 2. ä» TE åˆ›å»º PrimFunc 
te_mm: tvm.tir.PrimFunc = te.create_prim_func([A, B, C]).with_attr({"global_symbol": "mmult"})
te_relu: tvm.tir.PrimFunc = te.create_prim_func([D_IN, D_OUT]).with_attr({"global_symbol": "relu"})

# 3. å°†åˆ›å»ºå‡ºæ¥çš„ PrimFunc æ·»åŠ åˆ°ä¸€ä¸ª IRModule ä¸­
ir_m: tvm.IRModule = tvm.IRModule({'mm': te_mm})
gv_relu = tvm.ir.GlobalVar("relu")
ir_m.update_func(gv_relu, te_relu)
print(ir_m.get_global_vars())
# è¿™é‡Œä¹Ÿå¯ä»¥ä½¿ç”¨ å…ˆæ„å»º Scheduleï¼Œ å†è°ƒç”¨ `tvm.lower` çš„æ–¹æ³• å¾—åˆ°ä¸€ä¸ª IRModule
# te_sch: te.Schedule = te.create_schedule(C.op) # C.op: te.tensor.ComputeOp
# ir_m: tvm.IRModule = tvm.lower(te_sch, [A, B, C], simple_mode=True,name='mmult')

# 4. Lower and Build
''' åœ¨è¿™ä¸€æ­¥å°† IRModule ==> runtime.Module'''
rt_m: tvm.runtime.Module = tvm.build(ir_m, target='llvm', name='func_set')

print(f"\033[31mtvm.build\033[0m: {type(ir_m)} \033[31m==>\033[0m {type(rt_m)}")

# print IRModuleï¼Œ å…¶ä¸­åŒ…å«ä¸€ä¸ª PrimFunc
# print(f"\033[31mAfter lowering, the IRModule\033[0m:\n{ir_m}")
ir_m.show()

# # print source code
# print(f"\033[31msource code\033[0m:\n{rt_m.get_source()}")
```


`tvm.build` å°†ä»£ç ä» IRModule è½¬æ¢æˆ runtime.Moduleï¼› 

- å…¶ä¸­ IRModule å®šä¹‰åœ¨ `include/tvm/ir/module.h` ä¸­ï¼› å¯ä»¥ç®€å•è§†ä½œä¸€ä¸ª `<name, BaseFunc>` çš„å“ˆå¸Œè¡¨(å³å¯åŒæ—¶åŒ…å«`PrimFunc` å’Œ `Func`)
- å…¶ä¸­ runtime.Module å®šä¹‰å¯¹åº”åœ¨ `include/tvm/runtime/module.h` ä¸­ï¼› å¯ä»¥ç®€å•è§†ä½œä¸€ä¸ª `<name, PackedFunc>` çš„å“ˆå¸Œè¡¨ã€‚ `ModuleNode` æ˜¯ä¸€ä¸ªæ¥å£ï¼Œ å¯¹åº”æœ‰å¤šç§ä¸åŒçš„å®ç°ï¼Œ å¦‚ `CUDAModuleNode`ï¼Œ `LLVMModuleNode` ç­‰


ä»¥ä½¿ç”¨teå£°æ˜ è®¡ç®—è¿‡ç¨‹ ä¸ºä¾‹ï¼Œå±•ç¤ºä¸€ä¸ª `te.tensor.ComputeOp` æ˜¯å¦‚ä½•è¢«ç¼–è¯‘æˆ `runtime.Module` çš„ï¼š

1. ä½¿ç”¨teå£°æ˜è®¡ç®—è¿‡ç¨‹
2. ä½¿ç”¨ `te.create_schedule` åˆ›å»ºä¸€ä¸ªSchedule å¯¹è±¡ï¼Œ æ¥ç€ä½¿ç”¨ `te.lower()` å°†ä¸€ä¸ª `te.Schedule` è½¬æˆä¸€ä¸ªIRModuleï¼›

    æˆ–è€…ä½¿ç”¨ `te.create_prim_func` åˆ›å»ºä¸€ä¸ª `tvm.tir.PrimFunc` å¯¹è±¡(å³ MLC è¯¾ç¨‹ ä¸­æ‰€è¯´çš„ä¸€ä¸ª**å…ƒå¼ é‡å‡½æ•°**)ï¼Œ ç„¶åä½¿ç”¨ IRModule çš„æ„é€ å‡½æ•°æŠŠè¿™ä¸ª PrimFunc æ”¾åˆ°IRModule ä¸­

3. æ¥ç€è°ƒç”¨ `tvm.build()` å¯¹ IRModuleè¿›è¡Œæ„å»ºï¼š åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œ IRModule æœ€ç»ˆå°†è¢«è½¬æ¢æˆä¸€ä¸ª `runtime.Module`ï¼› 

    è¿™é‡Œéœ€è¦æ³¨æ„ `tvm.build` å¯ä»¥æ¥å— `Schedule`, `PrimFunc`, `IRModule` ç­‰å¤šç§ç±»å‹ï¼ˆå®é™…ä¸Šéƒ½ä¼šè¢«è½¬æˆ IRModule ç„¶åå†ç¼–è¯‘ï¼‰ä½œä¸ºè¾“å…¥ï¼Œ å¦‚æœä¼ å…¥çš„æ˜¯ä¸€ä¸ª `Schedule` çš„è¯ï¼Œ åˆ™éœ€è¦ä¼ å…¥å‡½æ•°ç›¸åº”çš„è¾“å…¥å‚æ•°åˆ—è¡¨

ğŸ’¡**å› æ­¤æ¥ä¸‹æ¥ä¸»è¦çœ‹ ä½äº `python/driver/build_module.py` ä¸­çš„ `tvm.build` æ–¹æ³•**

---

`tvm.build()` åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤

```py
def build(
    inputs: Union[te.Schedule, PrimFunc, IRModule, Mapping[str, IRModule]],
    args: Optional[List[Union[Buffer, tensor.Tensor, Var]]] = None,
    target: Optional[Union[str, Target]] = None,
    target_host: Optional[Union[str, Target]] = None,
    runtime: Optional["tvm.relay.backend.Runtime"] = None,
    name: Optional[str] = "default_function",
    binds: Optional[Mapping[tensor.Tensor, Buffer]] = None,
):
    if isinstance(inputs, tvm.IRModule):
      input_mod = lower(inputs)
    if not isinstance(inputs, (dict, container.Map)):
      target = Target.current() if target is None else target
      target = target if target else "llvm"
      target_input_mod = {target: input_mod}
    else:
      target_input_mod = inputs

    annotated_mods = {}
    for tar, mod in target_input_mod.items():
      annotated_mods[tar] = mod.with_attr("runtime", runtime)
    
    # çœç•¥ get target_host çš„ä»£ç 
    target_host = "llvm"

    annotated_mods, target_host = Target.canon_target_map_and_host(annotated_mods, target_host)
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
    annotated_mods, target_host = Target.canon_target_map_and_host(annotated_mods, target_host)

    to_return = rt_mod_host
    return OperatorModule.from_module(to_return, ir_module_by_target=annotated_mods, name=name)
```

1. è°ƒç”¨ `python/driver/build_module.py` ä¸­ `lower()`ï¼Œ è¿™æ˜¯ä¸€æ­¥ä½œç”¨æ˜¯æ‰§è¡Œ IRModuleï¼ˆor scheduleç­‰ï¼‰ åˆ° IRModule çš„å˜æ¢ï¼Œæ‰§è¡Œ tir å±‚çº§çš„ target æ— å…³ä¼˜åŒ–ã€‚ å…·ä½“æ¥è¯´ï¼Œ`lower()` ä¸­ä¼šè°ƒç”¨ `ffi.lower_module(inp, simple_mode)`ï¼Œè¿™é‡Œå¯¹åº”åˆ° C++ ä»£ç  `src/dirver/driver_api.cc` ä¸­çš„ `LowerModule` æ–¹æ³•ï¼š

    ```c++
    IRModule LowerModule(IRModule mod, bool simple_mode) {
        Array<transform::Pass> pass_list = CreatePassList(simple_mode);
        return LowerWithPassList(std::move(mod), pass_list);
    }
    TVM_REGISTER_GLOBAL("driver.lower_module").set_body_typed([](IRModule mod, bool simple_mode) {
        return LowerModule(std::move(mod), simple_mode);
    });
    ```

    å¯ä»¥çœ‹åˆ° åœ¨ `LowerModule` é€»è¾‘ä¸­ï¼Œ é¦–å…ˆåˆ›å»ºäº†ä¸€ä¸ªpass åˆ—è¡¨ï¼Œ åŒ…å«äº†`tir::transform`ä¸­çš„ä¸€äº›å˜æ¢ï¼Œ ç„¶åè°ƒç”¨ `LowerWithPassList` æŠŠåˆ—è¡¨ä¸­çš„ pass åº”ç”¨åˆ°è¯¥ IRModule ä¸­ï¼šå³é¦–å…ˆå°† `pass_list` æ„é€ æˆä¸€ä¸ª `Sequential` å¯¹è±¡ï¼Œ ç„¶åè°ƒç”¨ `SequentialNode::operator()` ï¼Œ 

2. åœ¨ç¬¬ä¸€æ­¥çš„ lowering ä¹‹åï¼Œ`tvm.build()` æ¥æ”¶çš„inputså‚æ•° `Union[te.Schedule, PrimFunc, IRModule, Mapping[str, IRModule]]` è¢«è½¬æˆäº†ä¸€ä¸ª ç»è¿‡åˆæ­¥ä¼˜åŒ–çš„ IRModule ï¼Œ å¹¶æ‰§è¡Œäº† tir çº§åˆ«çš„ target independent ä¼˜åŒ–ã€‚ **æ¥ä¸‹æ¥å°±æ˜¯æ‰§è¡Œä»£ç ç”Ÿæˆçš„å·¥ä½œ**ï¼Œ é¦–å…ˆæ£€æŸ¥ target ç›¸å…³ä¿¡æ¯ï¼Œä¼šæ ¹æ® `tvm.build()` çš„è¾“å…¥è®¾ç½®ç›¸åº”çš„ target, target_host ç­‰å˜é‡

3. åœ¨æ‹¿åˆ° target ç­‰è§„èŒƒæ ¼å¼çš„ä¿¡æ¯ä¹‹åï¼Œä¼šæ‰§è¡Œ`rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)` ï¼Œè¿™é‡Œè°ƒç”¨çš„ `tir_to_runtime` å¯¹åº”åˆ°C++ä¸­çš„ `TIRToRuntime`:

    ```c++
    TVM_REGISTER_GLOBAL("driver.tir_to_runtime")
        .set_body_typed([](const Map<Target, IRModule>& inputs_arg, Target host_target) {
            return TIRToRuntime(inputs_arg, host_target);
        });
    ```

    æ¯”è¾ƒå…³é”®çš„ `TIRToRuntime` é€»è¾‘å¦‚ä¸‹ï¼š

    ```c++
    runtime::Module TIRToRuntime(const Map<Target, IRModule>& inputs_arg, const Target& target_host_arg) {
      std::vector<runtime::Module> device_modules;
      Map<Target, IRModule> inputs = inputs_arg;
      Target target_host = target_host_arg;
      // 1. æ£€æŸ¥å¹¶è®¾ç½®target_host
      // ...
      // 2. éå†è¾“å…¥çš„ {Target: IRModule}
      //  SplitMixedModule å°†ä¸€ä¸ª IRModule åˆ†ä¸º host ä¸ device ä¸¤éƒ¨åˆ†
      //  æ ¹æ® host ä¸ device ä¿¡æ¯æ›´æ–° mhost_all
      IRModule mhost_all = IRModule(Map<GlobalVar, BaseFunc>(),{},{},{},(*inputs.begin()).second->attrs);
      for (const auto& it : inputs) {
        if (it.second.defined()) {
          const auto& [target, ir_module] = it;
          auto& [host_mod, device_mod] = SplitMixedModule(ir_module, target, target_host);

          bool overrides_host_target = target->GetTargetDeviceType() == target_host->GetTargetDeviceType();
          bool non_host_target_kind  = target->kind != target_host->kind;
          if (overrides_host_target && non_host_target_kind)
            device_modules.push_back(codegen::Build(host_mod, target));
          else
            mhost_all->Update(host_mod);

          if (device_mod->functions.size() != 0)
            device_modules.push_back(codegen::Build(device_mod, target));
        }
      }
      // 3. Build host module
      runtime::Module mhost = codegen::Build(mhost_all, target_host);
      for (const auto& it : device_modules)
        if (it.operator->())
          mhost.Import(it);
      return mhost;
    }
    ```

    è¿™é‡Œæ¯”è¾ƒå…³é”®çš„ `codegen::build` åœ¨`src/target/codegen.cc` ä¸­ï¼Œ å®ç°å¦‚ä¸‹ï¼š

    ```c++
    runtime::Module Build(IRModule mod, Target target) {
      auto target_attr_map = tvm::TargetKind::GetAttrMap<FTVMTIRToRuntime>("TIRToRuntime");
      if (target_attr_map.count(target->kind))
        return target_attr_map[target->kind](mod, target);
      
      std::string build_f_name = "target.build." + target->kind->name;
      const PackedFunc* bf = runtime::Registry::Get(build_f_name);
      return (*bf)(mod, target);
    }
    ```

    å¯ä»¥çœ‹åˆ° é€šè¿‡ä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥ æ‹¿åˆ° `build_f_name` (ä¾‹å¦‚ target æ˜¯ llvmï¼Œ è¿™é‡Œå¾—åˆ°å°±æ˜¯ "target.build.llvm"ï¼Œ cuda å°±æ˜¯ "target.build.cuda")ï¼Œ æ¥ç€ç”¨è¿™ä¸ªåå­—æŸ¥æ‰¾åˆ°å·²æ³¨å†Œçš„å‡½æ•°ï¼Œè¿›è¡Œ build

4. æ¥ä¸‹æ¥åˆ†åˆ«ä»¥ llvm åç«¯ï¼ˆä¸è®¾ç½®runtimeï¼‰ å’Œ cuda åç«¯ä¸ºä¾‹ çœ‹ä¸€ä¸‹å¦‚ä½•ç”Ÿæˆçš„ llvm IR or CUDA ä»£ç 

    ```c++
    // `src/target/llvm/llvm_module.cc`
    TVM_REGISTER_GLOBAL("target.build.llvm")
    .set_body_typed([](IRModule mod, Target target) -> runtime::Module {
      auto n = make_object<LLVMModuleNode>();
      n->Init(mod, target);
      return runtime::Module(n);
    });
    // `src/target/opt/build_cuda_on.cc`
    TVM_REGISTER_GLOBAL("target.build.cuda").set_body_typed(BuildCUDA);
    ```

    é¦–å…ˆçœ‹CUDAçš„ tir => bin ç¼–è¯‘æµç¨‹ï¼š
    
    ```c++
    runtime::Module BuildCUDA(IRModule mod, Target target) {
      // Step 1: Initialize CodeGen for CUDA  
      bool output_ssa = false;
      CodeGenCUDA cg;
      cg.Init(output_ssa);
      
      // Step 2: Add all tir::PrimFunc in IRModule to compile list
      for (auto kv : mod->functions) {
        auto f = Downcast<PrimFunc>(kv.second);
        cg.AddFunction(f);
      }

      // Step 3: Lower IRModule to CUDA source code
      std::string code = cg.Finish();

      // Step 4: Compile CUDA source code using NVCC and create runtime::Module
      std::string fmt = "ptx";
      std::string ptx = NVRTCCompile(code, cg.need_include_path());
      return CUDAModuleCreate(ptx, fmt, ExtractFuncInfo(mod), code);
    }
    ```

    **å¹¶è¡Œçº¿ç¨‹æ‰§è¡Œï¼ˆParallel Thread eXecutionï¼ŒPTXï¼‰ä»£ç æ˜¯ç¼–è¯‘åçš„GPUä»£ç çš„ä¸€ç§ IR ï¼Œ å®ƒå¯ä»¥å†æ¬¡ç¼–è¯‘ä¸ºåŸç”Ÿçš„GPUæŒ‡ä»¤**ã€‚PTXæä¾›äº†ä¸€ä¸ªç¨³å®šçš„ç¼–ç¨‹æ¨¡å‹å’ŒæŒ‡ä»¤é›†ï¼Œè¿™ä¸ªISAèƒ½å¤Ÿè·¨è¶Šå¤šç§GPUï¼Œå¹¶ä¸”èƒ½å¤Ÿä¼˜åŒ–ä»£ç çš„ç¼–è¯‘ç­‰ç­‰ã€‚
    
    æ¥ä¸‹æ¥çœ‹ LLVMï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœ target="llvm"ï¼Œç”±äº LLVM IR ä»ç„¶åªæ˜¯ä¸€ç§ä¸­é—´è¡¨ç¤ºï¼Œè¿˜éœ€è¦æ ¹æ® target å½“ä¸­æ›´è¯¦ç»†çš„ç¡¬ä»¶å‚æ•°ï¼Œæ‰¾åˆ°ç›®æ ‡ç¼–è¯‘ç¡¬ä»¶ï¼Œç„¶åè°ƒç”¨ç›¸åº”çš„ CodeGenï¼ˆçœç•¥éƒ¨åˆ†è¾…åŠ©ä»£ç ï¼‰ï¼š
    
    ```c++
    void Init(const IRModule& mod, const Target& target) {
      // Step 1: Initialize CodeGen for LLVM with different target
      InitializeLLVM();
      tm_ = GetLLVMTargetMachine(target);
      std::unique_ptr<CodeGenLLVM> cg = CodeGenLLVM::Create(tm_.get());

      // Step 2: Add all tir::PrimFunc in IRModule to compile list
      std::vector<PrimFunc> funcs;
      for (auto kv : mod->functions) {
        if (!kv.second->IsInstance<PrimFuncNode>()) {
          // (@jroesch): we relax constraints here, Relay functions will just be ignored.
          DLOG(INFO) << "Can only lower IR Module with PrimFuncs, but got " << kv.second->GetTypeKey();
          continue;
        }
        auto f = Downcast<PrimFunc>(kv.second);
        funcs.push_back(f);
      }

      // Step 3: Lower IRModule to LLVM IR code
      module_ = cg->Finish();
    }
    ```
    
    å¯ä»¥çœ‹åˆ°è¿™é‡Œæ„é€ äº†ä¸€ä¸ª `LLVMModuleNode` å®ä¾‹ï¼Œ è°ƒç”¨äº†å®ƒçš„ `Init` æ–¹æ³•ï¼Œ å› æ­¤å¯çŸ¥è¯¥æ–¹æ³•å°±æ˜¯å°† IRModule lower ä¸º LLVMModuleNode çš„æ ¸å¿ƒå‡½æ•°ã€‚ å…³äº `LLVMModuleNode` çš„å®šä¹‰ï¼Œ å¯å‚è€ƒ [TVM-type](./tvm-type.md)ï¼Œä¸‹é¢æ˜¯ `Init` æ–¹æ³•çš„å…·ä½“å®ç°ï¼š


5. ä»ä¸Šä¸€æ­¥ä¸­å¯çŸ¥ï¼Œ TIR èƒ½ lower æˆç›®æ ‡æºä»£ç ï¼Œå…³é”®æ˜¯ CodeGenã€‚ä¸Šé¢æåˆ°çš„ CodeGenCUDAï¼Œä»¥åŠ CodeGenLLVM æ˜¯å®Œæˆ TIR lower ä¸º C++ ä»£ç çš„ç›¸å…³ç»“æ„ã€‚ å…¶ä¸­ `CodeGenCUDA`ç»§æ‰¿è‡ª `CodeGenC`ï¼Œä»¥å…¶ä¸ºä¾‹å­è¯´æ˜ï¼ˆ`tvm/src/target/source/codegen_c.[h, cc]`ï¼‰ï¼š
    
    ```c++
    class CodeGenC : public ExprFunctor<void(const PrimExpr&, std::ostream&)>,
                 public StmtFunctor<void(const Stmt&)>,
                 public CodeGenSourceBase {
    public:
      // expression
      void VisitExpr_(const VarNode* op, std::ostream& os) override;         // NOLINT(*)
      void VisitExpr_(const LoadNode* op, std::ostream& os) override;        // NOLINT(*)
      void VisitExpr_(const BufferLoadNode* op, std::ostream& os) override;  // NOLINT(*)
      void VisitExpr_(const LetNode* op, std::ostream& os) override;         // NOLINT(*)
      ...
      // statment
      void VisitStmt_(const LetStmtNode* op) override;
      void VisitStmt_(const StoreNode* op) override;
      void VisitStmt_(const BufferStoreNode* op) override;
      void VisitStmt_(const ForNode* op) override;
      void VisitStmt_(const WhileNode* op) override;
      void VisitStmt_(const IfThenElseNode* op) override;
      ...
    }
    ```

    å¯ä»¥çœ‹åˆ°ï¼Œ `CodeGenC` ä¼šéå†åˆ°ä¸¤ç§ TIR Nodeï¼šExpressionï¼ˆè¡¨è¾¾å¼ï¼‰ å’Œ Statementï¼ˆè¯­å¥ï¼‰ã€‚Expressionï¼ˆè¡¨è¾¾å¼ï¼‰ä¸­åŒ…å«äº†å¸¸è§çš„å˜é‡å£°æ˜ã€è¿ç®—ã€åˆ¤æ–­ã€å‡½æ•°è°ƒç”¨ï¼Œè€Œ Statementï¼ˆè¯­å¥ï¼‰ä¸­åŒ…å«äº†æ§åˆ¶æµï¼ˆif-elseï¼ŒLoop ç­‰ï¼‰ã€å†…å­˜ç®¡ç†ã€èµ‹å€¼ç­‰æ“ä½œã€‚

    ä¾‹å¦‚ï¼Œé‡åˆ°å››åˆ™è¿ç®—çš„ Expressionï¼ŒCodeGenC ç›´æ¥ç¿»è¯‘ä¸º " a OP b "çš„ä»£ç ï¼š

    ```c++
    template <typename T>
    inline void PrintBinaryExpr(const T* op, const char* opstr,
                                std::ostream& os, CodeGenC* p) {
      // If both a and b are scalars
      if (op->dtype.lanes() == 1) {
        // If OP is an alphabet string, then lower it as "OP(a, b)"
        if (isalpha(opstr[0])) {
          os << opstr << '(';
          p->PrintExpr(op->a, os);
          os << ", ";
          p->PrintExpr(op->b, os);
          os << ')';
        }
        // If OP is a symbol, like + - * / %, then lower it as "a OP b"
        else {
          os << '(';
          p->PrintExpr(op->a, os);
          os << ' ' << opstr << ' ';
          p->PrintExpr(op->b, os);
          os << ')';
        }
      }
      // If both a and b are vectors
      else {
        p->PrintVecBinaryOp(opstr, op->dtype, op->a, op->b, os);
      }
    }

    void CodeGenC::VisitExpr_(const AddNode* op, std::ostream& os) {  // NOLINT(*)
      PrintBinaryExpr(op, "+", os, this);
    }
    void CodeGenC::VisitExpr_(const SubNode* op, std::ostream& os) {  // NOLINT(*)
      PrintBinaryExpr(op, "-", os, this);
    }
    void CodeGenC::VisitExpr_(const MulNode* op, std::ostream& os) {  // NOLINT(*)
      PrintBinaryExpr(op, "*", os, this);
    }
    void CodeGenC::VisitExpr_(const DivNode* op, std::ostream& os) {  // NOLINT(*)
      PrintBinaryExpr(op, "/", os, this);
    }
    ```

    å¦‚æœé‡åˆ°é€‰æ‹© SelectNodeï¼ŒCodeGenC åˆ™ç¿»è¯‘ä¸º "(c ? a : b)" çš„ä»£ç ï¼š

    ```c++
    void CodeGenC::VisitExpr_(const SelectNode* op, std::ostream& os) {
      os << "(";
      PrintExpr(op->condition, os);
      os << " ? ";
      PrintExpr(op->true_value, os);
      os << " : ";
      PrintExpr(op->false_value, os);
      os << ")";
    }
    ```

## 2. Lower TE
ä¸Šä¸€èŠ‚ä¸­æˆ‘ä»¬ä½¿ç”¨ TE(Tensor Expression) å£°æ˜è®¡ç®—è¿‡ç¨‹ï¼Œ æ„é€ äº†ä¸€ä¸ª PrimFuncã€‚ åœ¨TVM ä¸­ï¼Œ PrimFunc è¿˜å¯ä»¥é€šè¿‡ç¼–å†™ TVMScriptï¼Œ æˆ–è€…é€šè¿‡ relay/relax lower è€Œæ¥ã€‚ 

TEæ˜¯ä½äº Relay IR / TOPI å’Œ TIR ä¹‹é—´æ¦‚å¿µï¼Œ æ˜¯TVM æä¾›çš„ä¸€ç§ç”¨äºç¼–å†™ Primfunc çš„ EDSLï¼Œ å…¶æŠ½è±¡ç¨‹åº¦æ¯” TIR æ›´é«˜ï¼Œæ— æ³•ç›´æ¥è¢«ç¼–è¯‘ä¸ºç¡¬ä»¶æºä»£ç ï¼Œå¿…é¡»å…ˆ lower ä¸º TIR çš„ Primitive Function å†è¿›è¡Œç¼–è¯‘ã€‚

åœ¨è¿™ä¸€èŠ‚ä¸­æˆ‘ä»¬å…³æ³¨ä½¿ç”¨ te æè¿°çš„è®¡ç®—è¿‡ç¨‹æ˜¯å¦‚ä½•è¢«è½¬æ¢æˆä¸€ä¸ª PrimFunc çš„ï¼Œå³å¯¹åº”åˆ° [Lower Tir](#1-lower-tir) ä¸­æåˆ°çš„ `te.create_prim_func`ï¼Œ ä¹Ÿå°±æ˜¯å®˜æ–¹ç»™å‡ºçš„ ç¼–è¯‘æµç¨‹ä¸­çš„çº¢æ¡†ä¸­çš„éƒ¨åˆ†ï¼š

<div class="autocb" style="text-align:center;"><img src="./tvm-lowering.assets\autocb_0.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

æµ‹è¯•ä»£ç ä¸ç¬¬ä¸€èŠ‚ä¸­ä»£ç ç›¸åŒï¼Œåªä¸è¿‡æ­¤æ—¶æˆ‘ä»¬å…³æ³¨æ›´ä¸Šå±‚çš„éƒ¨åˆ†ï¼š

```py
import tvm
from tvm import te
SIZE = 1024

# 1. ä½¿ç”¨ TE å£°æ˜è®¡ç®—
# mm
A = te.placeholder((SIZE, SIZE), name='A')  # te.tensor.Tensor
B = te.placeholder((SIZE, SIZE), name='B')  # te.tensor.Tensor
k = te.reduce_axis((0, SIZE), 'k')          # tir.expr.IterVar
C = te.compute((SIZE, SIZE), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name='MM') # te.tensor.Tensor

# 2. ä» TE åˆ›å»º PrimFunc 
te_mm: tvm.tir.PrimFunc = te.create_prim_func([A, B, C]).with_attr({"global_symbol": "mmult"})

# 3. å°†åˆ›å»ºå‡ºæ¥çš„ PrimFunc æ·»åŠ åˆ°ä¸€ä¸ª IRModule ä¸­
ir_m: tvm.IRModule = tvm.IRModule({'mm': te_mm})

# è¿™é‡Œä¹Ÿå¯ä»¥ä½¿ç”¨ å…ˆæ„å»º Scheduleï¼Œ å†è°ƒç”¨ `tvm.lower` çš„æ–¹æ³• å¾—åˆ°ä¸€ä¸ª IRModule
te_sch: te.Schedule = te.create_schedule(C.op) # C.op: te.tensor.ComputeOp
ir_m: tvm.IRModule = tvm.lower(te_sch, [A, B, C], simple_mode=True,name='mmult')

print(ir_m.get_global_vars())
```

åœ¨å…³æ³¨ lowering ä¹‹å‰é¦–å…ˆçœ‹ä¸€ä¸‹ä¸€ä¸ª TE çš„æ„æˆ

1. é¦–å…ˆæ˜¯ `te.placeholder` å°†è¿”å›ä¸€ä¸ª `te.tensor.Tensor`ã€‚ å…¶æ•°æ®ç»“æ„å®šä¹‰ä½äº `include/tvm/te/operation.h`ä¸­ï¼Œ å…·ä½“å¦‚ä¸‹ï¼š

    ```c++
    class PlaceholderOpNode : public OperationNode {
     public:
      Array<PrimExpr> shape;
      DataType dtype;

      int num_outputs() const final{return 1;};
      Array<PrimExpr> output_shape(size_t i) const final{return shape;};
      Array<Tensor> InputTensors() const final{return {};};

      static constexpr const char* _type_key = "PlaceholderOp";
      TVM_DECLARE_BASE_OBJECT_INFO(PlaceholderOpNode, OperationNode);
    };
    ```

    placeholder é€šå¸¸ç”¨äºè®¡ç®—å›¾çš„ Input èŠ‚ç‚¹ä½¿ç”¨ï¼Œæ²¡æœ‰å‰åºèŠ‚ç‚¹ï¼Œå¯èƒ½ä¹Ÿæ˜¯teä¸­æœ€å¸¸ç”¨çš„Opã€‚ é™¤äº†`PlaceholderOp` ä¹‹å¤–ï¼Œ TE ä¸­è¿˜æœ‰ä¸€äº›å…¶ä»–çš„ Op å®šä¹‰ï¼Œä¾‹å¦‚åé¢ä¼šæåˆ°çš„ `ComputeOp`ï¼Œ ä»¥åŠ `ExternOp` ç­‰ï¼Œ è¿™äº› Op æ˜¯ TE AST çš„ç»„æˆéƒ¨åˆ†ã€‚

2. æ¥ä¸‹æ¥æ˜¯ `te.compute` ï¼Œ compute ä»ä¸€ä¸ªæˆ–è€…å¤šä¸ªå‰åºèŠ‚ç‚¹æ¥æ”¶æ•°æ®ï¼Œå¹¶æŒ‰åˆå§‹åŒ–çš„æ—¶å€™ä¼ å…¥çš„ lambda è¡¨è¾¾å¼è®¡ç®— Tensor å†…çš„æ•°æ®ã€‚å…¶API ç¬¬ä¸€ä¸ªä¼ å…¥å‚æ•°æ˜¯ Tensor çš„ shapeï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ª lambda è¡¨è¾¾å¼ï¼Œè¡¨æ˜ Tensor çš„æ•°æ®æ˜¯å¦‚ä½•è®¡ç®—æ¥çš„ï¼Œè¿”å›ä¸€ä¸ª`te.tensor.Tensor` ã€‚ å…¶ C++ éƒ¨åˆ†æ•°æ®ç»“æ„å®šä¹‰å…·ä½“å¦‚ä¸‹ï¼š

    ```c++
    class TVM_DLL ComputeOpNode : public BaseComputeOpNode {
    public:
      Array<PrimExpr> body; //compute expression
      // override functions
      int num_outputs() const final;
      Array<Tensor> InputTensors() const final;

      static constexpr const char* _type_key = "ComputeOp";
      TVM_DECLARE_FINAL_OBJECT_INFO(ComputeOpNode, BaseComputeOpNode);
    };
    ```
    
    å¯ä»¥çœ‹åˆ°å…¶å…³é”®æˆå‘˜åªæœ‰ä¸€ä¸ª bodyï¼Œæ˜¯ä¸€ä¸ª PrimExpr æ•°ç»„ï¼Œå› æ­¤æ¥ä¸‹æ¥çœ‹ python ç«¯æ˜¯æ€æ ·æŠŠä¸€ä¸ª lambda è¡¨è¾¾å¼è¡¨è¾¾çš„è®¡ç®—è½¬æ¢æˆäº† PrimExprï¼š

    ```py
    def compute(shape, fcompute, name="compute", tag="", attrs=None, varargs_names=None):
      out_ndim = len(shape)

      argspec = inspect.getfullargspec(fcompute)
      arg_names = argspec.args
      
      # 1. å°†ä¼ å…¥çš„ shape è½¬æˆ tir.IterVar åˆ—è¡¨
      dim_var = [tvm.tir.IterVar((0, s), x, 0) for x, s in zip(arg_names, shape[:out_ndim])]
      
      # 2. æ„é€ è®¡ç®—çš„ AST
      body = fcompute(*[v.var for v in dim_var])

      # 3. è°ƒç”¨ C++ API è·å– compute_op_node
      body = convert(body) # å°†List[PrimExpr] è½¬æˆ Array<PrimExpr>
      op_node = _ffi_api.ComputeOp(name, tag, attrs, dim_var, body)

      return op_node.output
    ```

    è¿™é‡Œçš„å…³é”®ç‚¹åœ¨äº `IterVar` ç»§æ‰¿äº† `ExprOp`ï¼ˆä½äº `python/tvm/tir/expr.py`ï¼‰ï¼Œ è€Œ `ExprOp` é‡è½½äº†å››åˆ™è¿ç®—ï¼Œç§»ä½è¿ç®—ï¼Œæ¯”è¾ƒè¿ç®—ç­‰æ“ä½œç¬¦ï¼Œå› æ­¤å¯ä»¥ç›´æ¥è¢« lambda è¡¨è¾¾å¼æ“ä½œï¼Œè°ƒç”¨é‡è½½çš„è¿ç®—ç¬¦è¿”å›è¡¨ç¤ºè¿ç®—ä¹‹åçš„è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ï¼š

    ```py
    Z = te.compute((n,), lambda i: X[i]*Y[i])
    ```

    é¦–å…ˆå¯¹äº `X[i]`å’Œ`Y[i]` ä¼šåˆ†åˆ«æ„é€ ä¸€ä¸ª `tir.expr.ProducerLoad` è¡¨è¾¾å¼ï¼Œæ¥ç€è¿™ä¸¤ä¸ªè¡¨è¾¾å¼çš„ä¹˜æ³•ä¼šè°ƒç”¨åˆ° `ExprOp` é‡è½½çš„ä¹˜æ³•è¿ç®—ç¬¦ï¼š

    ```py
    def multiply(lhs, rhs, span=None):
      return _ffi_api._OpMul(lhs, rhs, span)
    ```

    è¿”å›ä¸€ä¸ª `tir.Mul` Exprï¼›å…¶å®ƒçš„è¿ç®—ç±»ä¼¼ã€‚**å€¼å¾—æ³¨æ„çš„æ˜¯** TE ä¸­ä¹Ÿæä¾›äº†æ§åˆ¶ç±» PrimExpr çš„å°è£…ï¼Œä¾‹å¦‚ `te.if_then_else`ï¼›ä»¥åŠ `te.extern_primfunc` ç”¨å…¶å®ƒPrimFunc æ¥åˆ›å»º PrimExpr

3. åœ¨æ„é€ è¡¨ç¤ºè®¡ç®—çš„ AST ä¹‹åï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªä»lambdaæ„é€ å‡ºæ¥çš„ PrimExpr æ•°ç»„ä¼ å…¥äº†`_ffi_api.ComputeOp`ï¼Œ è¿”å›ä¸€ä¸ª ComputeOp ç±»å‹ï¼Œ `ComputeOp` æ˜¯ `te::Operation` çš„ä¸€ä¸ªå­ç±»ï¼Œå› æ­¤å¯ä»¥ä»å…¶`output`æ–¹æ³•æ‹¿åˆ°è®¡ç®—ç»“æœ `Tensor` ä½œä¸º `te.compute` çš„è¿”å›ç»“æœ

ğŸ’¡**åœ¨è¿™é‡Œæ€»ç»“ä¸€ä¸‹ TE çš„æ„æˆå…³é”®å…ƒç´ **ï¼š å³ `Tensor` å’Œ `Operation` ä¸¤ä¸ªç±»å‹ã€‚å‰è€…è¡¨å¾ä¸€ä¸ªå¼ é‡çš„å½¢çŠ¶ã€å…ƒç´ ç±»å‹ã€source operationç­‰ï¼›åè€…è¡¨ç¤ºäº§ç”Ÿ Tensor çš„ä¸€ç§æ“ä½œï¼ŒåŒ…æ‹¬è¡¨ç¤ºè¾“å…¥çš„æ“ä½œ PlaceholderOpï¼Œ è¡¨ç¤ºè®¡ç®—é€»è¾‘çš„æ“ä½œ ComputeOp ç­‰ã€‚

<div class="autocb" style="text-align:center;"><img src="./tvm-lowering.assets\autocb_1.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>


---

æ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹å…³æ³¨ `te.create_prim_func` æ˜¯å¦‚ä½• å°† TE lower åˆ° PrimFunc çš„ï¼Œå¯¹åº”åˆ°`src/te/operation/create_prim_func.cc` ï¼š

```c++
PrimFunc CreatePrimFuncWithConstants(const Array<te::Tensor>& arg_list,
                                     const Array<runtime::NDArray>& constants,
                                     const Optional<Array<tir::Var>>& tir_var_list,
                                     std::optional<DataType> index_dtype_override) {
  // Infomations used in CreatePrimFunc and its sub-functions.
  CreateFuncInfo info(arg_list);
  // Root body stmts.
  Array<Stmt> root_stmts;
  // Analyzer
  arith::Analyzer analyzer;

  // Step 1. åç»­éå†è®¡ç®—å›¾ï¼š placeholderè¢«æ”¾åˆ°å‰é¢ï¼Œæœ€ç»ˆcomputeOpåœ¨æœ€å
  Array<te::Operation> order = CollectOrderedOps(arg_list);

  // Step 2. è¿™æ®µé€»è¾‘æ˜¯å¤„ç† TE ä¸­åæ¥å¼•å…¥çš„ ExternOpï¼Œåœ¨æœ¬ä¾‹ä¸­æ— éœ€è€ƒè™‘
  InitializeBufferBinds(order, &info);

  // Step 3. å¦‚æœæ˜¯ placeholderï¼Œåˆ›å»ºå¯¹åº”çš„ tir.Bufferï¼›å¦‚æœæ˜¯computeï¼Œåˆ›å»ºå¯¹åº”çš„ tir.Stmtï¼›å¦‚æœæ˜¯ externï¼Œåˆ™åˆ›å»ºä¸€ä¸ª tir.Stmt èŠ‚ç‚¹
  for (const te::Operation& op : order) {
    RewriteStageToBlock(op, &info, &root_stmts, &analyzer);
  }

  // åˆ›å»ºå¹¶è¿”å› PrimFunc
  auto func = GenerateAndCompletePrimFunc(arg_list, root_stmts, &info, tir_var_list);

  return func;
}
```

å›é¡¾ä¸‹ PrimFunc çš„ç»“æ„ï¼š

```c++
class PrimFuncNode : public BaseFuncNode {
 public:
  Array<tir::Var> params;
  tir::Stmt body;
  Type ret_type;
  Map<tir::Var, Buffer> buffer_map;

  static constexpr const char* _type_key = "tir.PrimFunc";
  TVM_DECLARE_FINAL_OBJECT_INFO(PrimFuncNode, BaseFuncNode);
};
```

`GenerateAndCompletePrimFunc` ä¼šå°† `arg_list`(`Array<te::Tensor>`) è½¬æˆä¸€ä¸ª `Array<tir::Var>` ä½œä¸ºå‡½æ•°çš„ paramsï¼› è€Œå‡½æ•° body å’Œ `buffer_map` å·²ä»å»ºç«‹çš„è®¡ç®—å›¾ä¸­å¾—åˆ°ï¼›å› æ­¤åˆ°è¿™é‡Œæˆ‘ä»¬å°±å®Œæˆäº†ä» TE åˆ° `PrimFunc` çš„è½¬æ¢ã€‚

ğŸ’¡æ€»ç»“æ•´ä¸ªè¿‡ç¨‹ï¼Œæœ€æ ¸å¿ƒçš„éƒ¨åˆ†åœ¨äºåç»­éå† TE è¡¨ç¤ºçš„è®¡ç®—å›¾ï¼ˆOperationä½œä¸ºèŠ‚ç‚¹ï¼Œé€šè¿‡ `InputTensors` æ–¹æ³•æ‹¿åˆ°æ‰€æœ‰å…¥è¾¹ï¼Œåš DFSï¼‰ï¼Œè½¬æ¢æˆå¯¹åº”çš„ `tir.Buffer` å’Œ `tir.Stmt` å³ tir ä¸­çš„ AST


## 3. Lower Relay
è¿™ä¸€èŠ‚å…³æ³¨ä¸€ä¸ªæ›´é«˜å±‚æ¬¡çš„æŠ½è±¡ï¼Œ TVM ä¸­å›¾çº§åˆ« IR Relay çš„ lowering

<div class="autocb" style="text-align:center;"><img src="./tvm-lowering.assets\autocb_2.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

ä»è¿™å¼ å›¾å¯ä»¥çœ‹åˆ°ï¼Œ Relay è¦åˆ° TIR æœ‰2æ¡è·¯å¾„ï¼Œç¬¬ä¸€æ¡å°±æ˜¯ç›´æ¥åˆ° TIRï¼Œ æ¯”å¦‚ PrimExpr æ´¾ç”Ÿçš„èŠ‚ç‚¹ IntImmNode å¯ä»¥ç›´æ¥æ˜ å°„åˆ° TIR ï¼Œå¦å¤–ä¸€æ¡å°±æ˜¯ Relay é‡Œé¢ç±»ä¼¼ Conv çš„ Op çš„è®¡ç®—é€»è¾‘æ˜¯ç”¨ TOPI æ¥è¡¨è¾¾çš„ï¼Œ TOPI æ˜¯ TVM ä¸­ç”¨ TE æ¥è¡¨ç¤ºå¸¸ç”¨ç®—å­çš„é¢„å®šä¹‰åº“ã€‚

### 3.1. TOPI
è€ƒè™‘è¿™ä¸ªä¾‹å­ï¼š

```py
n = te.var("i", dtype="int32")
m = te.var("j", dtype="int32")
A = te.placeholder((n, m), name="A")
k = te.reduce_axis((0, m), "k")
B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k), name="B")

# ä½¿ç”¨ TOPI: è¿™ä¸€å¥ä¸ä¸Šé¢ä¸¤å¥ç­‰ä»·
C = topi.sum(A, axis=1) 

# å¯ä»¥åˆ©ç”¨ä¸Šä¸€èŠ‚çš„ä»£ç ï¼Œ å°† TE lower åˆ° tir æŸ¥çœ‹è¡¨ç¤ºï¼š
prim_func1 = te.create_prim_func([A,B])
prim_func2 = te.create_prim_func([A,C])
print(f"\033[31m{'='*20}prim_func1{'='*20}\033[0m\n{prim_func1}")
print(f"\033[31m{'='*20}prim_func2{'='*20}\033[0m\n{prim_func2}")

# Build ä¹‹åè¿è¡Œç»“æœå½“ç„¶ä¹Ÿæ˜¯ç›¸åŒçš„ï¼š
prim_func1 = te.create_prim_func([A,B])
prim_func2 = te.create_prim_func([A,C])

lib1, lib2 = tvm.build(prim_func1), tvm.build(prim_func2)

a_np = np.random.rand(128, 1024).astype("float32")
expected_res = np.sum(a_np, axis=1)
a_nd1, a_nd2 = tvm.nd.array(a_np), tvm.nd.array(a_np)
test_res1 = tvm.nd.empty((128,), dtype="float32")
test_res2 = tvm.nd.empty((128,), dtype="float32")

lib1(a_nd1, test_res1)
lib2(a_nd2, test_res2)
np.testing.assert_allclose(expected_res, test_res1.numpy(), rtol=1e-5)
np.testing.assert_allclose(expected_res, test_res2.numpy(), rtol=1e-5)
print("ğŸ‰\033[32mTest Pass...\033[0m")
```

å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºæ±‚å’Œè¿™æ ·å¸¸è§çš„ç®€å•æ“ä½œï¼Œå¦‚æœåªä½¿ç”¨ te çš„è¯ï¼Œä¹Ÿéœ€è¦å…ˆå®šä¹‰ä¸€ä¸ª reduce_axis, å†ç”¨ lambda é£æ ¼å»å£°æ˜ï¼Œæ¯”è¾ƒç¹çï¼Œæ›´ä¸å¿…è¯´åœ¨ç¥ç»ç½‘ç»œä¸­å¤§é‡ä½¿ç”¨çš„å…¶å®ƒæ›´å¤æ‚ä¸€ç‚¹çš„ç®—å­ï¼Œå¦‚æœç”¨ te å»å†™çš„è¯ï¼Œä¼šæ¯”è¾ƒéº»çƒ¦ã€‚ å› æ­¤ TVM æä¾›äº†ä¸€äº›ä½¿ç”¨ TE å®šä¹‰çš„ç°æˆçš„ç®—å­ï¼ŒæŠŠä»–ä»¬æ”¾åˆ°ä¸€èµ·ï¼ˆä»¥åŠä¸€äº› schedule æ“ä½œï¼‰ï¼Œç§°ä½œ TOPI(Tensor operator inventory)

TOPI ä¸­åŒ…å«äº†å¦‚ å·ç§¯ï¼Œ softmaxï¼Œ çŸ©é˜µä¹˜ ç­‰å¸¸è§çš„ç®—å­ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½çš„è¡¨è¾¾ DL ä¸­å¸¸è§çš„è®¡ç®—è¿‡ç¨‹ã€‚

åœ¨çœ‹å…·ä½“å®ç°å‰å¯ä»¥é¦–å…ˆè§‚å¯Ÿä¸€ä¸‹ TOPI çš„ç›®å½•ç»“æ„ï¼ŒåŒ…å«äº† `nn.cc`ï¼Œ`elemwise.cc`ï¼Œ`schedule.cc`ç­‰ï¼Œåˆ†åˆ«å¯¹åº”äº†æœºå™¨å­¦ä¹ å¸¸è§ç®—å­ï¼Œelement wise è¿ç®—ï¼ˆå¦‚ä¸Šé¢ä¾‹å­ä¸­çš„ `topi.sum`ï¼‰ï¼Œ ç®—å­è°ƒåº¦ï¼›åœ¨è¿™äº›æ–‡ä»¶ä¸­ï¼Œä½¿ç”¨ `TVM_REGISTER_GLOBAL` æ³¨å†Œäº† TOPI ä¸­çš„å…·ä½“å®ç°ã€‚


ğŸ’¡è¿™é‡Œæˆ‘ä»¬ä»¥ `topi.matmul` ä¸ºä¾‹çœ‹ä¸€ä¸‹å®ç°ã€‚


`topi.matmul` å¯¹åº” C++ æ³¨å†Œåœ¨`src/topi/nn.cc` ä¸­ï¼Œå®ç°åœ¨`include/tvm/topi/nn/dense.h`ä¸­ï¼Œç®€åŒ–åå¦‚ä¸‹ï¼š

```c++
inline tvm::te::Tensor matmul(const tvm::te::Tensor& A, const tvm::te::Tensor& B,bool trans_a = false, bool trans_b = false,std::string name = "T_matmul", std::string tag = kMatMul) {
  tvm::Array<tvm::PrimExpr> output_shape{A->shape[trans_a ? 1 : 0], B->shape[trans_b ? 0 : 1]};
  auto k = tvm::te::reduce_axis(tvm::Range{0, A->shape[trans_a ? 0 : 1]}, "k");
  auto l = [&](tvm::tir::Var i, tvm::tir::Var j) {
    return tvm::sum((trans_a ? A[k][i] : A[i][k]) * (trans_b ? B[j][k] : B[k][j]), {k});
  };
  return tvm::te::compute(output_shape, l, name, tag);
}
```

å¯ä»¥çœ‹åˆ°å…¶å® `dense` è·Ÿæˆ‘ä»¬æ‰‹å†™çš„æ²¡æœ‰ä»€ä¹ˆä¸åŒï¼Œä½†æ˜¯å¯¹äº logsoftmax ä¹‹ç±»æœ‰å®ç°æŠ€å·§çš„ç®—å­ï¼ŒTOPI æœ‰å¾ˆå¥½çš„å®ç°å¹¶ä¸”æœ‰ç›¸åº”çš„è°ƒåº¦å¯ä»¥ä½¿ç”¨ã€‚

> TOPI ä¸­æœ‰äº›ç®—å­åœ¨C++ å’Œ python ç«¯åˆ†åˆ«å®ç°äº†ä¸€æ¬¡ï¼Œè¿™æ ·åšçš„åŸå› è§ï¼š https://discuss.tvm.apache.org/t/why-topi-is-implemented-in-both-c-and-python/50/7
>
> ç®€å•æ¥è¯´ï¼Œå½“ä¸€ä¸ªç®—å­åœ°è°ƒåº¦æ–¹å¼ç¨³å®šä¹‹åï¼Œä¼šè¢«æ”¾åœ¨C++éƒ¨åˆ†ï¼Œ æ­£åœ¨devåœ°æ”¾åœ¨pythonå®ç°ï¼›è€Œå¯¹äºä¸€äº›ç®€å•çš„ç®—å­ï¼Œå°±ä¿ç•™äº†åœ¨ C++ å’Œ py ä¸¤éƒ¨åˆ†çš„å®ç°


### 3.2. Relay
æœ¬èŠ‚ä½¿ç”¨å¦‚ä¸‹ä¾‹å­ï¼š

```py
import numpy as np
import tvm
from tvm import relay

# 1. å®šä¹‰ Relay Var
data = relay.var("data", shape=(1,784),dtype="float32")
weight1 = relay.var("weight1",shape=(128,784),dtype="float32")
bias1 = relay.var("bias1",shape=(128,),dtype="float32")
weight2 = relay.var("weight2",shape=(10,128),dtype="float32")
bias2 = relay.var("bias2",shape=(10,),dtype="float32")

# 2. ä½¿ç”¨ Relay Op å®šä¹‰ç½‘ç»œç»“æ„
dense1 = relay.nn.dense(data,weight1)
bias_add1 = relay.nn.bias_add(dense1, bias1)
relu1 = relay.nn.relu(bias_add1)
dense2 = relay.nn.dense(relu1,weight2)
bias_add2 = relay.nn.bias_add(dense2, bias2)
relu2 = relay.nn.relu(bias_add2)

# 3. æ„å»ºç½‘ç»œ
func = relay.Function([data,weight1,bias1,weight2,bias2],relu2)
print(f"\033[31m{'='*20}relay func{'='*20}\033[0m\n{func}")

# 4. æ„å»º IRModule
ir_mod = tvm.IRModule.from_expr(func)

# 5. è°ƒç”¨ relay.build è¿›è¡Œæ„å»º
# relay.build ç›®å‰æš‚æ—¶æ”¯æŒç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥ `relay.Function`ï¼Œ ä½†æ˜¯ä¹‹åå°†åªæ”¯æŒ IRModule
with tvm.transform.PassContext(opt_level=2):
  rt_lib = relay.build(ir_mod=ir_mod,target="llvm")

```

è¿™é‡Œä»¥ `dense` ç®—å­ä¸ºä¾‹ï¼Œ é¦–å…ˆå…³æ³¨ `dense1 = relay.nn.dense(data,weight1)` ï¼š

```py
def dense(data, weight, units=None, out_dtype=""):
  return _make.dense(data, weight, units, out_dtype)
```

åœ¨ C++ ç«¯ä½¿ç”¨ RELAY_REGISTER_OP æœºåˆ¶ç»Ÿä¸€ç®¡ç† Relay Opï¼Œ `dense`ç®—å­çš„æ³¨å†Œä½äº `src/relay/op/nn/nn.cc` ä¸­ï¼š

```c++
Expr MakeDense(Expr data, Expr weight, IndexExpr units, DataType out_dtype) {
  auto attrs = make_object<DenseAttrs>();
  attrs->units = units;
  attrs->out_dtype = out_dtype;
  static const Op& op = Op::Get("nn.dense");
  return Call(op, {data, weight}, Attrs(attrs), {});
}

TVM_REGISTER_GLOBAL("relay.op.nn._make.dense").set_body_typed(MakeDense);

RELAY_REGISTER_OP("nn.dense")
    .describe(/*çœç•¥*/)
    .set_attrs_type<DenseAttrs>()
    .set_num_inputs(2)
    .add_argument("data", "nD Tensor", "Input data.")
    .add_argument("weight", "2D Tensor", "Weight matrix.")
    .set_support_level(1)
    .set_attr<FInferCorrectLayout>("FInferCorrectLayout", DenseInferCorrectLayout)
    .add_type_rel("Dense", MatmulRel<DenseAttrs>)
    .set_attr<TOpPattern>("TOpPattern", kOutEWiseFusable);
```

å¯ä»¥çœ‹åˆ°æœ€ç»ˆè¿”å›çš„å°±æ˜¯ä¸€ä¸ª `CallNode`ï¼Œ Op æ˜¯ `dense`ã€‚

åŒæ—¶æ³¨æ„åˆ° `RELAY_REGISTER_OP("nn.dense")` åªæ˜¯è®¾ç½®äº†è¾“å…¥è¾“å‡º type å…³ç³»ï¼Œä»¥åŠå…¶å®ƒçš„ä¸€äº›ç›¸å…³å±æ€§ï¼Œ æ²¡æœ‰æŒ‡å®š `dense` è¿™ä¸ªç®—å­å…·ä½“çš„è®¡ç®—å’Œè°ƒåº¦ï¼Œ é‚£ä¹ˆä¸€ä¸ª Relay ç®—å­çš„è®¡ç®—å’Œè°ƒåº¦åœ¨å“ªé‡ŒæŒ‡å®šå‘¢ï¼Ÿ

ç­”æ¡ˆæ˜¯åœ¨ python ç«¯ï¼Œ ä»¥`dense`ä¸ºä¾‹ï¼Œ å…¶è°ƒåº¦å’Œè®¡ç®—çš„æŒ‡å®šä½äº `python/tvm/relay/op/nn/_nn.py` ä¸­ï¼Œ è¯¥æ–‡ä»¶ä¼šåœ¨ `import relay` æ—¶è¢«è‡ªåŠ¨åŠ è½½ï¼š

```py
# dense
reg.register_strategy("nn.dense", strategy.dense_strategy)
```

`strategy.dense_strategy` å®šä¹‰å¦‚ä¸‹ï¼š

```py
@override_native_generic_func("dense_strategy")
def dense_strategy(attrs, inputs, out_type, target):
  """dense generic strategy"""
  logger.warning("dense is not optimized for this platform.")
  strategy = _op.OpStrategy()
  strategy.add_implementation(
    wrap_compute_dense(topi.nn.dense),
    wrap_topi_schedule(topi.generic.schedule_dense),
    name="dense.generic",
  )
  return strategy
```

å…¶ä¸­ `topi.nn.dense` å’Œ `topi.generic.schedule_dense` å°±æ˜¯ ä¸Šä¸€èŠ‚ TOPI ä¸­é¢„å®šä¹‰å¥½çš„ TE è¡¨ç¤ºçš„ç®—å­åº“ä¸­çš„ç®—å­ã€‚ è€Œ `_op.OpStrategy` å¯¹åº” C++ ä¸­çš„æ•°æ®ç»“æ„ä½äº `include/tvm/relay/op_strategy.h` ä¸­ï¼š

```c++
class OpImplementationNode : public Object {
 public:
  FTVMCompute fcompute;
  FTVMSchedule fschedule;
  String name;
  int plevel;

  static constexpr const char* _type_key = "relay.OpImplementation";
  TVM_DECLARE_FINAL_OBJECT_INFO(OpImplementationNode, Object);
};
```

å…¶ä¸­ `FTVMCompute` æ˜¯ `runtime::TypedPackedFunc<Array<te::Tensor>(const Attrs& attrs, const Array<te::Tensor>& inputs, const Type& out_type)>;` ï¼Œ FTVMSchedule æ˜¯ `runtime::TypedPackedFunc<te::Schedule(const Attrs& attrs, const Array<te::Tensor>& outs, const Target& target)>;`ï¼›

ğŸ’¡**è¿™æ ·å°±å°† TOPI å’Œ Relay Op å»ºç«‹äº†è”ç³»**

æ¥ä¸‹æ¥å…³æ³¨ `relay.build`

---

å…ˆæš‚æ—¶å¿½ç•¥ `PassContext(opt_level=2)`ï¼Œ ç›´æ¥çœ‹ `relay.build`ï¼š

```py
def build(ir_mod,target=None, target_host=None,
          executor=Executor("graph"), runtime=Runtime("cpp"),
          workspace_memory_pools=None, constant_memory_pools=None,
          params=None, mod_name="default",
):
  raw_targets = Target.canon_multi_target_and_host(Target.target_or_current(target), target_host)
  target_host = raw_targets[0].host

  # å¦‚æœå½“å‰ dispatch context æ˜¯ fallback context (the default root context),
  # ä» TopHub ä¸­ load pre-tuned parameters
  if isinstance(autotvm.DispatchContext.current, autotvm.FallbackContext):
    tophub_context = autotvm.tophub.context(list(raw_targets))
  else:
    tophub_context = autotvm.utils.EmptyContext()

  with tophub_context:
    bld_mod = BuildModule()
    graph_json, runtime_mod, params = bld_mod.build(
      mod=ir_mod, target=raw_targets, params=params,
      executor=executor, runtime=runtime,
      workspace_memory_pools=workspace_memory_pools,
      constant_memory_pools=constant_memory_pools,
      mod_name=mod_name,
    )
    # build ä¹‹å bld_mod ä¸­åŒ…å« IRModuleï¼ˆå…¶ä¸­ä¸º PrimFuncï¼‰
    func_metadata = bld_mod.get_function_metadata()
    devices = bld_mod.get_devices()
    lowered_ir_mods = bld_mod.get_irmodule()
    executor_codegen_metadata = bld_mod.get_executor_codegen_metadata()
    # è¿™é‡Œåªçœ‹ graph_executor å»æ‰äº†aot ç›¸å…³
    if executor.name == "graph":
      executor_factory = _executor_factory.GraphExecutorFactoryModule(
        ir_mod, raw_targets,
        executor, graph_json, runtime_mod,
        mod_name, params, func_metadata,
      )
    return executor_factory
```

å‡½æ•°è¿”å›ä¸€ä¸ª `tvm.relay.backend.executor_factory.ExecutorFactoryModule` ï¼Œ æ˜¯relayçš„ graph executor factoryï¼ˆç›®å‰æœ‰ graph å’Œ aot ä¸¤ç§ï¼‰

1. TopHub:

    > é¦–å…ˆ Relay ä¼šå¯»æ‰¾æ˜¯å¦æœ‰ AutoTVM é¢„å…ˆ Fintune çš„è®°å½•ï¼Œå¦‚æœæ²¡æœ‰é‚£ä¹ˆå°±ä½¿ç”¨autotvm.FallbackContextè¿™ä¸ªç¯å¢ƒä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚æœæœ‰é‚£ä¹ˆæ¥ä¸‹æ¥çš„æ‰€æœ‰æ“ä½œéƒ½åœ¨ tophub_context çš„ scope ä¹‹ä¸‹ (with tophub_context:)ã€‚å€¼å¾—ä¸€æçš„æ˜¯ Relay è€ƒè™‘äº†å¼‚æ„æƒ…æ™¯ä¸‹çš„ä»£ç ç”Ÿæˆï¼Œç”¨æˆ·å¯ä»¥æŒ‡å®šå¤šä¸ªç”Ÿæˆä»£ç çš„ç›®æ ‡ (target)ã€‚


2. åœ¨ `tophub_context`ä¸­ï¼Œåˆ›å»ºäº†ä¸€ä¸ª `BuildModule` ï¼Œç„¶åè°ƒç”¨äº† `build` å‡½æ•°ç”Ÿæˆä¸€ä¸ªç¡¬ä»¶å¯æ‰§è¡Œçš„æ›´åº•å±‚çš„ IRï¼Œä»¥åŠåŒ…å«å„ç§å¿…éœ€è¿è¡Œæ—¶åº“çš„tvm.Moduleå’Œä¼˜åŒ–åçš„è®¡ç®—å›¾çš„å‚æ•°ã€‚è¿™é‡Œè¿˜æœ‰ä¸€ä¸ª_graph_executor_factory.GraphExecutorFactoryModuleå‡½æ•°ï¼Œå®ƒçš„åŠŸèƒ½å°±æ˜¯å°†ä¸Šé¢çš„ IRï¼Œè¿è¡Œæ—¶åº“ä»¥åŠå‚æ•°æ‰“åŒ…æˆä¸€ä¸ªtvm.Moduleï¼Œè¿™æ ·ç”¨æˆ·åªéœ€è¦æŠŠè¿™ä¸ªtvm.Moduleå­˜ä¸‹æ¥ï¼Œä¸‹æ¬¡å°±å¯ä»¥çœå»ç¼–è¯‘è¿‡ç¨‹ç›´æ¥åœ¨ç¡¬ä»¶ä¸Šæ‰§è¡Œäº†ã€‚

3. `BuildModule()` å¯¹åº”åœ¨ C++ ä¸­çš„ `src/relay/backend/build_module.cc` ä¸­:

    ```c++
    class RelayBuildModule : public runtime::ModuleNode {
     public:
      RelayBuildModule() = default;

      PackedFunc GetFunction(const std::string& name, const ObjectPtr<Object>& sptr_to_self) final {
        if (name == "get_graph_json") {
          return PackedFunc(
              [sptr_to_self, this](TVMArgs args, TVMRetValue* rv) { *rv = this->GetGraphJSON(); });
        } else if (name == "build") {
          return PackedFunc([sptr_to_self, this](TVMArgs args, TVMRetValue* rv) {
            this->Build(args[0], args[1], args[2], args[3], args[4], args[5], args[6], args[7]);
          });
        } else if (name == "get_irmodule") {
          return PackedFunc([sptr_to_self, this](TVMArgs args, TVMRetValue* rv) {
            *rv = this->executor_codegen_->GetIRModule();
          });
        } else if (name == "optimize") {
          return PackedFunc([sptr_to_self, this](TVMArgs args, TVMRetValue* rv) {
            *rv = this->Optimize(args[0], args[1]);
          });
        }
      }

     protected:
      // return The updated Relay IR module after optimization.
      IRModule Optimize(IRModule relay_module, const Array<Target>& raw_targets);

      // Compile a Relay IR module to runtime module. ç»“æœä¿å­˜åœ¨ `this->ret_`
      void BuildRelay(IRModule relay_module, const String& mod_name);

     protected:
      std::unique_ptr<ExecutorCodegen> executor_codegen_;
      Executor executor_;     // Executor to build for
      Runtime runtime_;       // Runtime to codegen for
      WorkspaceMemoryPools workspace_memory_pools_;
      ConstantMemoryPools constant_memory_pools_;
      std::unordered_map<std::string, runtime::NDArray> params_; // parameters
      BuildOutput ret_; // building output
      CompilationConfig config_;
    };
    ```


## 4. Lower Relax
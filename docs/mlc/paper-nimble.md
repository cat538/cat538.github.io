# Nimble: Efficiently Compiling Dynamic Neural Networks for Model  Inference

文章发表在2021 MLSys， 后续工作有 cortex， dietcode等

## 0. 摘要
现代深度神经网络越来越多地利用**控制流、动态数据结构和动态张量形状**。 现有的深度学习系统侧重于优化和执行静态神经网络， 优化动态神经网络比静态神经网络更具挑战性， 必须考虑所有可能的执行路径和张量形状。 本文提出了 Nimble，这是一种高性能且灵活的系统，**用于在多个平台上优化、编译和执行动态神经网络**。 Nimble 通过引入动态类型系统、一组面向动态的优化和轻量级虚拟机运行时来处理模型动态。 我们的评估表明，在 Intel CPU、ARM CPU 和 Nvidia GPU 等硬件平台上，**Nimble 的性能比现有的动态神经网络解决方案高出 20 倍**。

## 1. Intro
随着基于深度学习的应用程序变得无处不在(ubiquitous)，用于优化、执行和部署此类应用程序的系统也变得无处不在。 许多系统研究项目专注于提高 DL 研究人员在各种平台上生成的预训练模型子集的性能。 这些模型表示为静态数据流图，其中每个输入和输出（即 tensor）的大小是先验已知的，确保执行路径在每次调用时保持不变，我们将这种称为静态模型。 神经网络，尤其是 NLP 的进步，在模型中引入了更多动态性： **控制流、动态数据结构 和 动态形状**。 我们将表现出这些行为的模型称为动态模型。

随着动态模型的成熟并继续从研究转向生产，它需要一个**高效的跨平台推理系统**。 这对深度学习从业者提出了新的挑战，因为动态模型引入了依赖于输入的图拓扑结构，打破了现有的系统假设并使为纯静态数据流图设计的优化无效。 然而，没有现有的解决方案满足这些要求。

一种解决方案是扩展现有的深度学习框架。 然而，框架针对训练做了很多优化，在推理方面有一定限制，且依赖第三方算子加速库(如 OpenBLAS、cuDNN 和 oneDNN)，算子库的发展无法适应动态模型需要的大量具有不同数据类型和形状的算子。

另一种方案是使用 深度学习编译器(DLC) 对神经网络进行端到端优化，如 XLA(XLA 团队，2017 年)、 Glow(Rotem 等人, 2018)、 TVM(Chen 等人，2018) 和 MLIR(Lattner 等人，2021)。 DLC 不同于传统的 DL 框架，将执行分为编译阶段和运行时阶段。 编译阶段支持图形级别的整体模型优化，以及针对多个硬件平台的工作负载特定内核代码生成，而运行时执行编译后的模块。

然而，由于缺乏对动态的支持， DLC 主要限于静态模型。 具体来说，为了编译和执行动态模型，系统需要 **一个可以静态表示动态构造的 IR ， 一个可以为动态变化的数据形状生成 kernel 的代码生成器， 以及一个动态处理执行与 kernel dispatch 的运行时**。 此外，动态特定的优化，例如动态内存规划，静态优化动态分配的过程，对于实现理想的性能是必要的。 当前的 DLC 中不存在这些功能。

为此，我们提出 Nimble，这是一种高性能的便携式系统，用于在多个平台上编译、优化和执行动态神经网络。 据我们所知，这是第一次尝试从 compiler 的角度系统地处理动态模型。 首先，我们引入类型系统扩展来处理未知维度的数据，这在动态模型中很常见，通过使用 Any 对形状执行类型检查和推理。 其次，我们设计了几种特定于动态模型的优化，包括动态形状感知代码生成、内存规划和设备放置。 第三，我们提出了一个基于虚拟机 (VM) 的运行时，它将独立于平台的控制逻辑和依赖于平台的内核实现分离，以实现可移植、轻量级，最重要的是，能够执行动态模型。 对 LSTM（Hochreiter & Schmidhuber，1997）、Tree-LSTM（Tai 等人，2015）和 BERT（Devlin 等人，2018）的评估表明，与最佳解决方案相比，在云端（Intel CPU 和 Nvidia GPU）和边缘（ARM CPU）的主流硬件平台上， Nimble 将延迟降低了 1.05 倍至 19.9 倍 。

综上所述，本文有以下三个核心贡献：

- 提出并构建一个端到端系统，实现跨多个硬件平台的高效动态模型推理，包括对结果进行基准测试的实证研究；
- 设计多种编译和优化技术，包括动态类型系统、内存规划 pass、用于放置计算和数据的异构放置机制，及符号内核代码生成和基于形状的调度算法；
- 设计和实现基于张量的VM，具有独立于平台的指令集，以高效灵活地跨平台执行动态模型。

剩余部分结构如下： 第 2 节回顾了现有 DLC 的局限性，并对 Nimble 进行了概述。 第 3 节介绍了 Nimble 编译流程的设计和实现，随后是第 4 节中基于 VM 的运行时。第 5 节提供了在不同硬件平台上使用各种模型的评估结果。 第 6 节涵盖相关工作，第 7 节总结本文。

## 7. Conclusion

本文提出了 Nimble，一种针对动态神经网络的端到端编译器和运行时解决方案。 Nimble 是第一个支持动态神经网络的深度学习编译器，通过一个轻量级和可移植的基于 VM 的运行时在多个平台上执行编译模型。 实验结果表明，与最先进的技术相比，Nimble 在多个平台上有效地执行了流行的动态模型，具有更好的性能和更广泛的覆盖范围。 未来的工作包括在新兴人工智能加速器上启用动态模型推理和动态模型的高性能训练。

## 2. CHALLENGES AND OUR APPROACH

### 2.1. DLC 的局限
由于缺少以下特定于动态的功能，当前的深度学习编译器无法很好地处理动态模型:

1. **An IR for representing dynamism**. 对静态模型执行 data type and shape inference 非常简单，因为它们在编译期间已知并且在运行时保持不变。 然而，输入张量的形状在动态模型中的不同输入样本之间可能变化很大。 控制流结构的出现使这个问题进一步复杂化，因为不同的执行路径可以发出截然不同的数据。 因此，完全静态的 IR 不足以应对这些模型的动态特性。

2. **A set of dynamic-oriented optimizations**. 现有的深度学习编译器，例如 TVM (Chen et al., 2018a) 和 Glow (Rotem et al., 2018) 期望每次优化的静态输入。 每个张量的内存空间都是预先分配的，它们的生命周期是使用专用优化 pass 确定的。 它们还确保了整个模型的均匀执行，因为所有内核都在同一设备上执行。 然而，当出现动态时，这些优化可能会完全中断，其中不同的执行路径可能需要不同数量的内存，并且在运行前大小不确定。 因此，可能会引入某些 IR 节点来帮助运行时类型推断和内存分配。 这些节点中的操作本质上对 CPU 更友好，如果放置不当会导致严重的性能问题 （对应文章中提的 "异构设备放置机制" ）。

    !!! warning "疑问"
        "The operations in these nodes are intrinsically more CPU friendly" 什么意思？
    

3. **A symbolic kernel code generator**. 代码生成（codegen）负责为算子生成高性能的可执行内核。 最近的研究（TVM 2018; Flextensor 2020; Adams et al., 2019; Ansor 2020a）在多后端静态形状的内核性能方面取得了令人瞩目的成果。 尽管如此，具有符号形状的代码生成中的挑战仍未得到探索。 在应用同一组循环优化后，如果循环边界处理不当，使用符号形状生成的内核仍然可能表现不佳。 同时，随着搜索空间呈指数增长，符号形状设置下的内核调整变得更具挑战性。

4. **A light-weight and cross-platform runtime**. 为了提高效率，静态模型的运行时可以简单地设计为一个顺序执行器，它按拓扑顺序遍历输入数据流图，一个一个地调用操作符。 然而，动态模型的执行路径可能只在运行时才知道，并且某些算子的内核必须根据运行时确定的数据形状进行调度，使得简单的图遍历运行时不够用。

### 2.2. 我们的方案
Nimble 的设计目标：

1. **支持动态模型**. Nimble 的目标模型具有所有类型的动态性，包括控制流、动态数据结构和各种数据形状。
2. **可移植且轻量**. Nimble 生成的模块应该可以在云端（高端 CPU 和 GPU）和边缘（低功耗 CPU 和 GPU）的多个平台上执行。 运行时应该足够轻，以在具有最小计算能力和内存容量的设备上运行。
3. **性能表现良好**. Nimble 应该在跨平台动态的背景下表现出色。

图 1 显示了 Nimble 架构。 由两个主要组件组成，即编译器和运行时。 Nimble 采用主流 DL 框架格式的模型，将其转换为统一的 IR ，然后将 IR 优化并编译为包含 **平台无关字节码** 和 **平台相关内核代码** 的可执行文件 ，最后加载可执行文件以在基于 VM 的运行时中执行。 **字节码由 Nimble 的运行时解释器执行**，可跨平台共享。 这种设计有效地使我们能够只维护一个版本的执行逻辑，而更多地关注性能关键的算子内核。 **内核针对特定硬件平台进行了优化，以实现高性能**。

<div class="autocb" style="text-align:center;"><img src="./paper-nimble.assets\autocb_0.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

为了提高性能，编译器中引入了各种分析和优化技术。 首先，设计了 **a set of IR extensions 来表示动态形状 和 动态分配**， 用于动态程序行为的静态优化（第 3.1 节）。 其次，**形状函数附加到算子以动态计算输出形状并在运行时执行类型检查**（第 3.2 节）。 第三，采用内存规划优化来减少内存消耗量（第 3.3 节）。 第四，异构设备放置机制旨在将 IR 节点放置在 "最佳" 设备上，以减少昂贵的跨设备数据传输和同步（第 3.4 节）。 最后，编译器具有一个代码生成器，能够专门化某些可能形状的代码生成器（第 3.5 节）。 一旦编译了具有动态行为的可执行文件，基于 VM 的运行时就可以使用 动态内核调度加载和解释它（第 4 节）。 我们将在后续部分详细介绍这些功能的设计和实现。

## 3. COMPILER SUPPORT FOR DYNAMISM
阻止现有 DLC 处理 DYNAMISM 的一个关键挑战是缺乏统一的动态表示。 例如，现有 IR(如 TVM)的优化和运行时始终假定存在静态形状信息。 这些假设为动态优化带来了相当多的挑战。

为了处理动态性，我们设计了一组 IR 扩展，它暴露优化动态程序所需的基本语义。 Nimble 在 TVM(0.6 版) 上实现，以利用其从各种 DL 框架到 TVM IR 的前端转换器。 TVM 的前端减轻了对前端特定细节的需求，使我们的工作能够专注于 IR 扩展 和 优化等贡献。 要使用 Nimble，只需为其提供一个预训练模型，进行编译，然后进行推理。 Nimble 适用于其他编译器工作。

本节描述我们**如何将标准 TVM 程序转换为我们的动态方言**，以便轻松地将静态优化应用于动态程序，就像我们在传统编译器优化中所做的那样。 特别是，我们详细介绍了编译动态模型所需的三个关键组件：

1. 一个扩展的类型系统，可以对动态形状进行静态跟踪
2. 一系列使动态输出形状、分配和设备布局明确的优化过程
3. 一组代码生成技术，用于生成具有动态输入和输出形状的内核代码


### 3.1. Typing
DLC 使用类型系统来表示、检查和推断张量的数据类型和形状。 在某些框架和编译器中，这分为两个步骤，形状推断和数据类型推断。 TVM 同时执行这两种形式，并将它们称为类型推断（Relay 2019），我们将在本节中使用这个术语。

*Tensor Type* 由 n 维形状（定义为描述张量维度的整数元组）和数据类型（例如 float32 或 int64）指定。 当前的深度学习 IR 仅在编译时知道张量形状的所有维度时才支持代码生成，即静态形状对于类型推断和检查是强制性的。

在动态模型的上下文中，许多数据形状只能在运行时确定。 为了支持动态数据形状，Nimble 引入了一个名为 `Any` 的特殊维度来表示静态未知维度。 例如，我们可以将张量类型表示为 `Tensor[(1, 10, Any), float32]`，其中该张量中第三维的大小未知，而其他两个维度具有具体值。 JANUS (Jeong 等人，2019) 使用类似的表示法来表示动态维度，但仅用于类型统一，而 Nimble 扩展类型推断以处理 `Any`，如下所述:

**Operator Type Relation** 类型关系描述了算子输入和输出类型之间的关系。 TVM 的类型系统依赖于类型关系(Relay的论文中有更详细的例子) 来推断和双向传播整个网络中算子的输入和输出之间的类型和形状关系。

必须泛化类型关系才能正确处理动态形状。 例如，如果没有适当的类型系统支持，一个在每次循环迭代中改变 tensor形状 的程序（许多 NLP 模型的解码器中都存在这种情况）是不可能进行类型化和编译的。 通过 Any 的引入，我们能够改进现有的类型关系以支持动态模型。

这里有两种动态类型关系的特定情况： 第一种是 输出 shape 取决于 输入 tensor 的算子，例如 `arange` 和 `unique` ， 将使用 Any 来描述这些形状。  另一种是，当输入形状包含 Any 维度时，类型关系需要将 Any 正确地传播到输出类型，并在必要时放宽静态情况下的类型约束。 例如，具有Any的两个维度之间的 broadcast4 类型关系的规则定义如下：

**Type Inference** `Any` 维度的一个警告是未知维度将在类型推断期间传播，从而减少形状特化的机会。 为了限制使用 Any 维度引入的精度损失，我们在类型系统中引入了子整形。 与流行编程语言中使用的子类型非常相似 (Liskov & Wing, 1994; Amadio & Cardelli, 1993)，我们的类型系统扩展使具有具体维度的值成为具有动态维度的张量类型的有效子类型。 例如 Tensor[(128, 128), f32] 是 Tensor[(any, 128), f32] 的有效子类型。 此外，我们使用程序分析来检测和删除不必要的动态，通过传达可用于下游编译的额外形状信息。 我们在两个关键点上这样做：首先，我们使用二次形状分析用静态尺寸改进任何错误的动态尺寸； 其次，在代码生成中，我们使用单个变量维度作为等效的动态维度。

### 3.2. Shape Function
Any维度的引入使得现有深度学习编译器采用的预分配机制失效。 相反，我们现在必须跟踪并行计算所需分配的内存量。 此外，由于动态张量形状，静态类型检查无法消除编译时的所有类型错误。 因此，我们定义了一个形状函数来计算存储分配的输出形状，并根据每个运算符的语义验证类型关系。 形状函数在结构上类似于 3.1 节中描述的类型关系，但存在于运行时而不是编译时。 它支持编译输出形状的计算并将其嵌入到程序中。

根据算子的特点，我们将形状函数分为三种不同的模式：数据独立、数据相关和上界。 数据独立形状函数用于输出形状仅取决于输入形状的运算符，例如普通二维卷积。 数据相关形状函数需要具体的输入值来计算输出形状。 例如，arange 的输出形状取决于 start、stop 和 step 的值。 此外，还有某些运算符，例如非最大抑制 (nms)，其中计算输出形状的复杂性与执行运算符本身的复杂性相当。 为了避免冗余计算，我们使用上限形状函数来快速估计输出的上限形状。 我们要求此类运算符将输出形状与输出值一起返回，以便使用真实形状将输出张量切片为精确的输出形状和布局。

值得注意的是，在存在动态形函数的情况下，需要特别注意算子融合。 运算符融合将基本运算符组合成复合运算符，是性能优化的关键技术，因为它减少了不必要的内存副本并改善了缓存局部性。 当所有形函数都与数据无关时，编译器可以很容易地将基本算子的形函数连接起来形成复合算子的形函数。 然而，具有数据依赖或上限形状函数的基本算子不能与其他算子融合，即，将其他算子的输出作为其输入融合在一起，因为形状函数需要访问复合中的中间结果 操作员。 因此，我们明确定义了融合策略以防止这种情况发生。

### 3.3. Memory Planning
许多深度学习编译器使用一种静态内存规划形式，将内存合并为连续的块并最大限度地减少分配。 对于 GPU 等设备，这些优化对于减少内存碎片和确保分配不会影响内核性能至关重要。 现有的深度学习编译器 IR 将内存分配隐藏在功能接口后面，每个运算符都在其中隐式分配其输出存储。 在执行之前，系统会在数据流图上执行静态内存规划，从而实现输出缓冲区的高效预分配。 由于内存分配的这种“带外”性质，自定义、修改或与其他通道组合内存优化具有挑战性。 例如，如果需要为异构执行调整内存分配，则需要对运行时进行修改。 TVM 的图形运行时就是静态内存规划的一个例子。 由于深度学习模型的粗粒度内存语义，与传统的编译器不同，内存优化必须在适当的高抽象级别进行。 现有工作没有提供对动态内存分配执行静态优化的明确途径。

为了执行我们所说的“动态内存规划”，我们扩展了 TVM 以将其 IR 与隐式内存分配转换为具有显式缓冲区分配和操作的 IR。 此转换的关键是调用约定的过程间更改，每个运算符现在都明确地获取其输出。 转换使得跟踪和优化 IR 中的动态内存分配成为可能。 为了执行此优化，我们引入了四个新的 IR 构造，(a) invoke_mut(op, inputs, outputs) 将输出作为可变输入输出参数，(b) alloc storage(size, alignment, device) 这 分配特定大小的内存区域，
(c) alloc tensor(storage, offset, shape, dtype, attrs) 在特定的存储偏移处分配一个具有形状和数据类型的张量，以及 (d) kill(tensor) 在它的引用计数之前释放一个张量 由于退出框架而变为零。

我们通过转换单个静态形状操作（例如广播加法）的示例来说明这是如何工作的。 请注意，在下面的代码示例中，Tensor<d1, ..., dn> 是形状为 (d1, ..., dn) 的包含浮点值的张量的简写。

<div class="autocb" style="text-align:center;"><img src="./paper-nimble.assets\autocb_1.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

上面的转换替换了运算符调用添加到代码中，该代码首先从偏移量为零的后备存储中分配一个输出张量，然后调用 invoke_mut。 出于空间原因，我们在附录 A 中提供了一个更复杂的示例，它说明了当运算符具有动态整形输入时如何处理内存分配。 关键的见解是将内存分配的概念内化到 IR 中，从而在存在控制和动态形状的情况下实现静态和动态分配的静态优化。 现在所有分配在 IR 中都是显式的，我们可以在动态程序的静态情况下提供类似的优化，例如我们已经实现了存储合并传递以将存储分组到一个更大的区域，这允许将多个张量分配复用到单个 一块存储。 可以对程序应用进一步的优化，如活性分析和图形着色算法，以重用存储。

### 3.4. Heterogeneous Device Placement

### 3.5. Symbolic Codegen

## 4. VIRTUAL MACHINE

## 5. EVALUATION

## 6. RELATED WORK

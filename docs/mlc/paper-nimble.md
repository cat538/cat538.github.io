# Nimble: Efficiently Compiling Dynamic Neural Networks for Model  Inference

文章发表在2021 MLSys， 旨在解决模型编译中动态形状的问题（对应到 TVM 的代码里即 `Any` Expr），介绍了引入 `Any` 遇到的类型推断、代码生成，多后端支持等挑战以及相应的解决方法

后续工作有 cortex， dietcode等

## 0. 摘要
现代深度神经网络越来越多地利用**控制流、动态数据结构和动态张量形状**。 现有的深度学习系统侧重于优化和执行静态神经网络， 优化动态神经网络比静态神经网络更具挑战性， 必须考虑所有可能的执行路径和张量形状。 本文提出了 Nimble，这是一种高性能且灵活的系统，**用于在多个平台上优化、编译和执行动态神经网络**。 Nimble 通过引入动态类型系统、一组面向动态的优化和轻量级虚拟机运行时来处理模型动态。 我们的评估表明，在 Intel CPU、ARM CPU 和 Nvidia GPU 等硬件平台上，**Nimble 的性能比现有的动态神经网络解决方案高出 20 倍**。

## 1. Intro
随着基于深度学习的应用程序变得无处不在(ubiquitous)，用于优化、执行和部署此类应用程序的系统也变得无处不在。 许多系统研究项目专注于提高 DL 研究人员在各种平台上生成的预训练模型子集的性能。 这些模型表示为静态数据流图，其中每个输入和输出（即 tensor）的大小是先验已知的，确保执行路径在每次调用时保持不变，我们将这种称为静态模型。 神经网络，尤其是 NLP 的进步，在模型中引入了更多动态性： **控制流、动态数据结构 和 动态形状**。 我们将表现出这些行为的模型称为动态模型。

随着动态模型的成熟并继续从研究转向生产，它需要一个**高效的跨平台推理系统**。 这对深度学习从业者提出了新的挑战，因为动态模型引入了依赖于输入的图拓扑结构，打破了现有的系统假设并使为纯静态数据流图设计的优化无效。 然而，没有现有的解决方案满足这些要求。

一种解决方案是扩展现有的深度学习框架。 然而，框架针对训练做了很多优化，在推理方面有一定限制，且依赖第三方算子加速库(如 OpenBLAS、cuDNN 和 oneDNN)，算子库的发展无法适应动态模型需要的大量具有不同数据类型和形状的算子。

另一种方案是使用 深度学习编译器(DLC) 对神经网络进行端到端优化，如 XLA(XLA 团队，2017 年)、 Glow(Rotem 等人, 2018)、 TVM(Chen 等人，2018) 和 MLIR(Lattner 等人，2021)。 DLC 不同于传统的 DL 框架，将执行分为编译阶段和运行时阶段。 编译阶段支持图形级别的整体模型优化，以及针对多个硬件平台的工作负载特定内核代码生成，而运行时执行编译后的模块。

然而，由于缺乏对动态的支持， DLC 主要限于静态模型。 具体来说，为了编译和执行动态模型，系统需要 **一个可以静态表示动态构造的 IR ， 一个可以为动态变化的数据形状生成 kernel 的代码生成器， 以及一个动态处理执行与 kernel dispatch 的运行时**。 此外，动态特定的优化，例如动态内存规划，静态优化动态分配的过程，对于实现理想的性能是必要的。 当前的 DLC 中不存在这些功能。

为此，我们提出 Nimble，这是一种高性能的便携式系统，用于在多个平台上编译、优化和执行动态神经网络。 据我们所知，这是第一次尝试从 compiler 的角度系统地处理动态模型。 首先，我们引入类型系统扩展来处理未知维度的数据，这在动态模型中很常见，通过使用 Any 对形状执行类型检查和推理。 其次，我们设计了几种特定于动态模型的优化，包括动态形状感知代码生成、内存规划和设备放置。 第三，我们提出了一个基于虚拟机 (VM) 的运行时，它将独立于平台的控制逻辑和依赖于平台的内核实现分离，以实现可移植、轻量级，最重要的是，能够执行动态模型。 对 LSTM（Hochreiter & Schmidhuber，1997）、Tree-LSTM（Tai 等人，2015）和 BERT（Devlin 等人，2018）的评估表明，与最佳解决方案相比，在云端（Intel CPU 和 Nvidia GPU）和边缘（ARM CPU）的主流硬件平台上， Nimble 将延迟降低了 1.05 倍至 19.9 倍 。

综上所述，本文有以下三个核心贡献：

- 提出并构建一个端到端系统，实现跨多个硬件平台的高效动态模型推理，包括对结果进行基准测试的实证研究；
- 设计多种编译和优化技术，包括动态类型系统、内存规划 pass、用于放置计算和数据的异构放置机制，及符号内核代码生成和基于形状的调度算法；
- 设计和实现基于张量的VM，具有独立于平台的指令集，以高效灵活地跨平台执行动态模型。

剩余部分结构如下： 第 2 节回顾了现有 DLC 的局限性，并对 Nimble 进行了概述。 第 3 节介绍了 Nimble 编译流程的设计和实现，随后是第 4 节中基于 VM 的运行时。第 5 节提供了在不同硬件平台上使用各种模型的评估结果。 第 6 节涵盖相关工作，第 7 节总结本文。

## 7. Conclusion

本文提出了 Nimble，一种针对动态神经网络的端到端编译器和运行时解决方案。 Nimble 是第一个支持动态神经网络的 DLC，通过一个轻量级和可移植的基于 VM 的运行时在多个平台上执行编译模型。 实验结果表明，与最先进的技术相比，Nimble 在多个平台上有效地执行了流行的动态模型，具有更好的性能和更广泛的覆盖范围。 未来的工作包括在新兴人工智能加速器上启用动态模型推理和动态模型的高性能训练。

## 2. CHALLENGES AND OUR APPROACH

### 2.1. DLC 的局限
由于缺少以下特定于动态的功能，当前的 DLC 无法很好地处理动态模型:

1. **An IR for representing dynamism**. 对静态模型执行 data type and shape inference 非常简单，因为它们在编译期间已知并且在运行时保持不变。 然而，输入张量的形状在动态模型中的不同输入样本之间可能变化很大。 控制流结构的出现使这个问题进一步复杂化，因为不同的执行路径可以发出截然不同的数据。 因此，完全静态的 IR 不足以应对这些模型的动态特性。

2. **A set of dynamic-oriented optimizations**. 现有的 DLC ，例如 TVM (Chen et al., 2018a) 和 Glow (Rotem et al., 2018) 期望每次优化的静态输入。 每个张量的内存空间都是预先分配的，它们的生命周期是使用专用优化 pass 确定的。 它们还确保了整个模型的均匀执行，因为所有内核都在同一设备上执行。 然而，当出现动态时，控制流不同执行路径可能需要不同数量的内存，并且在运行前大小不确定。 因此，可能会引入某些 IR 节点来帮助运行时类型推断和内存分配。 这些节点中的操作本质上对 CPU 更友好，如果放置不当会导致严重的性能问题 （对应文章中提的 "异构设备放置机制" ）。

    !!! warning "疑问"
        "The operations in these nodes are intrinsically more CPU friendly" 什么意思？
    

3. **A symbolic kernel code generator**. 代码生成（codegen）负责为算子生成高性能的可执行内核。 最近的研究（TVM 2018; Flextensor 2020; Adams et al., 2019; Ansor 2020a）在多后端静态形状的内核性能方面取得了令人瞩目的成果。 尽管如此，具有符号形状的代码生成中的挑战仍在探索。 在应用同一组循环优化后，如果循环边界处理不当，使用符号形状生成的内核仍然可能表现不佳。 同时，随着搜索空间呈指数增长，符号形状设置下的内核调整变得更具挑战性。

4. **A light-weight and cross-platform runtime**. 为了提高效率，静态模型的运行时可以简单地设计为一个顺序执行器，它按拓扑顺序遍历输入数据流图，一个一个地调用操作符。 然而，动态模型的执行路径可能只在运行时才知道，并且某些算子的内核必须根据运行时确定的数据形状进行调度，使得简单的图遍历运行时不够用。

### 2.2. 我们的方案
Nimble 的设计目标：

1. **支持动态模型**. Nimble 的目标模型具有所有类型的动态性，包括控制流、动态数据结构和各种数据形状。
2. **可移植且轻量**. Nimble 生成的模块应该可以在云端（高端 CPU 和 GPU）和边缘（低功耗 CPU 和 GPU）的多个平台上执行。 运行时应该足够轻，以在具有最小计算能力和内存容量的设备上运行。
3. **性能表现良好**. Nimble 应该在跨平台动态的背景下表现出色。

图 1 显示了 Nimble 架构。 由两个主要组件组成，即编译器和运行时。 Nimble 采用主流 DL 框架格式的模型，将其转换为统一的 IR ，然后将 IR 优化并编译为包含 **平台无关字节码** 和 **平台相关内核代码** 的可执行文件 ，最后加载可执行文件以在基于 VM 的运行时中执行。 **字节码由 Nimble 的运行时解释器执行**，可跨平台共享。 这种设计有效地使我们能够只维护一个版本的执行逻辑，而更多地关注性能关键的算子内核。 **内核针对特定硬件平台进行了优化，以实现高性能**。

<div class="autocb" style="text-align:center;"><img src="./paper-nimble.assets\autocb_0.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

为了提高性能，编译器中引入了各种分析和优化技术。 首先，设计了 **a set of IR extensions 来表示动态形状 和 动态分配**， 用于动态程序行为的静态优化（第 3.1 节）。 其次，**形状函数附加到算子以动态计算输出形状并在运行时执行类型检查**（第 3.2 节）。 第三，采用内存规划优化来减少内存消耗量（第 3.3 节）。 第四，异构设备放置机制旨在将 IR 节点放置在 "最佳" 设备上，以减少昂贵的跨设备数据传输和同步（第 3.4 节）。 最后，编译器具有一个代码生成器，能够专门化某些可能形状的代码生成器（第 3.5 节）。 一旦编译了具有动态行为的可执行文件，基于 VM 的运行时就可以使用 动态内核调度加载和解释它（第 4 节）。 我们将在后续部分详细介绍这些功能的设计和实现。

## 3. COMPILER SUPPORT FOR DYNAMISM
阻止现有 DLC 处理 DYNAMISM 的一个关键挑战是缺乏统一的动态表示。 例如，现有 IR(如 TVM)的优化和运行时始终假定存在静态形状信息。 这些假设为动态优化带来了相当多的挑战。

为了处理动态性，我们设计了一组 IR 扩展，它暴露优化动态程序所需的基本语义。 Nimble 在 TVM(0.6 版) 上实现，以利用其从各种 DL 框架到 TVM IR 的前端转换器。 TVM 的前端减轻了对前端特定细节的需求，使我们的工作能够专注于 IR 扩展 和 优化等贡献。 要使用 Nimble，只需为其提供一个预训练模型，进行编译，然后进行推理。 Nimble 适用于其他编译器工作。

本节描述我们**如何将标准 TVM 程序转换为我们的动态方言**，以便轻松地将静态优化应用于动态程序，就像我们在传统编译器优化中所做的那样。 特别是，我们详细介绍了编译动态模型所需的三个关键组件：

1. 一个扩展的类型系统，可以对动态形状进行静态跟踪
2. 一系列使动态输出形状、分配和设备布局明确的优化过程
3. 一组代码生成技术，用于生成具有动态输入和输出形状的内核代码


### 3.1. Typing
DLC 使用类型系统来表示、检查和推断张量的数据类型和形状。 在某些框架和编译器中，这分为两个步骤，形状推断和数据类型推断。 TVM 同时执行这两种形式，并将它们称为类型推断（Relay 2019），我们将在本节中使用这个术语。

*Tensor Type* 由 n 维形状（定义为描述张量维度的整数元组）和数据类型（例如 float32 或 int64）指定。 当前的深度学习 IR 仅在编译时知道张量形状的所有维度时才支持代码生成，即静态形状对于类型推断和检查是强制性的。

在动态模型的上下文中，许多数据形状只能在运行时确定。 为了支持动态数据形状，Nimble 引入了一个名为 `Any` 的特殊维度来表示静态未知维度。 例如，我们可以将张量类型表示为 `Tensor[(1, 10, Any), float32]`，其中该张量中第三维的大小未知，而其他两个维度具有具体值。 JANUS (Jeong 等人，2019) 使用类似的表示法来表示动态维度，但仅用于类型统一，而 Nimble 扩展类型推断以处理 `Any`，如下所述:

**<u>Operator Type Relation</u>** 类型关系**描述算子输入和输出类型之间的关系**。 TVM 的类型系统依赖于类型关系(Relay的论文中有更详细的例子) 来推断和双向传播整个网络中算子的输入和输出之间的类型和形状关系。

必须泛化类型关系才能正确处理动态形状。 例如，如果没有适当的类型系统支持，一个在每次循环迭代中改变 tensor形状 的程序（许多 NLP 模型的解码器中都存在这种情况）是不可能进行类型化和编译的。 通过 Any 的引入，我们能够改进现有的类型关系以支持动态模型。

这里有两种动态类型关系的特定情况： 第一种是 输出 shape 取决于 输入 tensor 的算子，例如 `arange` 和 `unique` ， 将使用 Any 来描述这些形状。  另一种是，当输入形状包含 Any 维度时，类型关系需要将 Any 正确地传播到输出类型，并在必要时放宽静态情况下的类型约束。 例如，具有 Any 的两个维度之间的 [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting) 类型关系(在本例子中即 输入tensor shape 和输出 tensor shape)的规则定义如下：

<div class="autocb" style="text-align:center;"><img src="./paper-nimble.assets\autocb_2.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

**值得注意的是，由于动态形状的存在，这些类型关系规则不能再排除编译时的所有类型错误**。 例如，对于上面显示的第二条规则，当 Any 在运行时既不是 1 也不是 d 时，它就违反了广播类型约束。 为了解决这个问题，我们采用渐进式类型（Siek & Taha，2006）方法，并在 Any 被具体值实例化后**在运行时保留某些类型检查**（更多详细信息请参见第 3.2 节）。 可以使用更 advanced 的类型系统来消除这些错误，但会增加复杂性。

**<u>Type Inference</u>** `Any` 维度的一个警告是未知维度将在类型推断期间传播，从而减少形状特化的机会。 为了限制使用 Any 维度引入的精度损失，我们在类型系统中引入了 子类型 *sub-shaping* 。 与流行编程语言中使用的子类型非常相似 (Liskov & Wing, 1994; Amadio & Cardelli, 1993)，我们的类型系统扩展 **使具有具体维度的值成为具有动态维度的张量类型的有效子类型** 。 例如 `Tensor[(128, 128), f32]` 是 `Tensor[(any, 128), f32]` 的有效子类型。 此外，我们使用程序分析来检测和删除不必要的动态，通过传达可用于下游编译的额外形状信息。 我们在两个关键点上这样做：首先，我们使用二次形状分析用静态尺寸改进任何错误的动态尺寸； 其次，在 codegen 中， 我们使用单个变量维度作为等效的动态维度。

### 3.2. Shape Function
Any 维度的引入使得现有 DLC 采用的预分配机制失效。 现在我们现在必须在计算的同时跟踪需要分配的内存。 此外，由于动态张量形状的引入，静态类型检查现在无法消除编译时的所有类型错误。 因此，我们定义了一个形状函数 *shape function* 来计算存储分配的输出形状，并根据每个算子的语义验证类型关系。 形状函数在结构上类似于 3.1 节中描述的类型关系，**但存在于运行时而不是编译时**。 它支持编译输出形状的计算并将其嵌入到程序中。

根据算子的特点，我们将形状函数分为三种不同的模式：数据独立、数据相关和上界。 

1. *Data independent* 形状函数用于输出形状仅取决于输入形状的算子，例如普通二维卷积。 
2. *Data dependent* 形状函数需要具体的输入值来计算输出形状。 例如，arange 的输出形状取决于 start、stop 和 step 的值。 
3. 此外，还有某些算子，例如 Non Maximum Suppression (nms)，**计算输出形状的复杂性与执行算子本身的复杂性相当**。 为了避免冗余计算，我们使用 *upper bound* 形状函数来快速估计输出的上限形状。 **我们要求此类算子将输出形状与输出值一起返回**， 以便为输出张量确定输出形状和内存布局。

值得注意的是，**在存在动态形状函数的情况下，需要特别注意算子融合**。 算子融合减少了不必要的内存副本并改善了缓存局部性，是性能优化的关键技术。 当所有形状函数都 Data independent 时，编译器可以将基本算子的形状函数连起来形成复合算子的形状函数。 然而，具有数据依赖或上限形状函数的基本算子不能与其他算子融合，即，将其他算子的输出作为其输入融合在一起，因为**形状函数需要访问复合中的中间结果算子**。 因此，**我们明确定义了融合策略以防止这种情况发生**。

!!! warning "疑问"
    为了引入动态形状需要改动的地方原来有这么多。不仅要引入运行时类型检查，类型（形状）推断，还要引入相应的动态内存分配机制，甚至算子融合策略都要因为动态的引入而受到限制。有一两个例子表示具体如何做吗（比方说哪些算子融合策略会受到具体怎么样的限制）？

### 3.3. Memory Planning
许多 DLC 使用静态内存规划形式，将内存合并为连续的块并最大限度地减少分配。 对于 GPU 等设备，这些优化对于减少内存碎片和确保分配不会影响内核性能至关重要。 现有的 DLC IR 将内存分配隐藏在 函数式 接口后面，每个算子都在其中隐式分配其输出存储。 在执行前，系统会在数据流图上执行静态内存规划，从而实现输出缓冲区的高效预分配。 由于内存分配的这种 "out-of-band" 的性质，自定义、修改或与其他 pass 组合内存优化具有挑战性。 例如，如果需要为异构执行调整内存分配，则需要对运行时进行修改。 TVM 的图形运行时就是静态内存规划的一个例子。 由于深度学习模型的粗粒度内存语义，与传统的编译器不同，内存优化必须在适当的高抽象级别进行。 现有工作没有提供对动态内存分配执行静态优化的明确途径。

为了执行我们所说的“动态内存规划”，我们扩展了 TVM 以将其 IR 与隐式内存分配转换为 具有显式缓冲区分配和操作的 IR。 此转换的关键是函数调用约定的更改： **每个算子现在都明确地获取其输出**。 这个转换使得跟踪和优化 IR 中的动态内存分配成为可能。 为了执行此优化，我们引入了四个新的 IR 构造:
1. `invoke_mut(op, inputs, outputs)` **将输出作为可变输入输出参数**
2. `alloc_storage(size, alignment, device)` 分配特定大小的内存区域
3. `alloc_tensor(storage, offset, shape, dtype, attrs)` 在特定的存储偏移处分配一个具有形状和数据类型的张量
4. `kill(tensor)` 在它的引用计数归零之前， 提前释放一个张量

我们通过转换单个静态形状操作（例如广播加法）的示例来说明这是如何工作的。 请注意，在下面的代码示例中，`Tensor<d1, ..., dn>` 是形状为 `(d1, ..., dn)` 的包含浮点值的张量的简写。

<div class="autocb" style="text-align:center;"><img src="./paper-nimble.assets\autocb_1.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

该代码将 `add` 转换为以下动作： 首先从偏移量为零的后备存储中分配一个输出张量，然后调用 `invoke_mut`。 出于空间原因，我们在附录 A 中提供了一个更复杂的示例，它说明了当算子具有动态整形输入时如何处理内存分配。 **关键的见解是将内存分配的概念内化到 IR 中**，从而在存在 control 和动态形状的情况下实现静态和动态分配的静态优化。 现在所有分配在 IR 中都是显式的，我们可以在动态程序的静态情况下提供类似的优化，例如我们已经实现了存储合并 pass 以将存储分组到一个更大的区域，这允许将**多个张量分配复用到单块存储**。 可以使用如 liveness analysis and graph coloring algorithm 等更进一步的优化算法以重用存储。

!!! warning "疑问"
    图着色算法在这里的应用？


### 3.4. Heterogeneous Device Placement
如 [3.2 节](#32-shape-function)所述，形状函数在运行时执行以计算算子的输出形状。 这些函数应该在 CPU 上执行，因为它们的输出用于计算分配内存的大小。 在异构执行的情况下（即 CPU 和 GPU），必须小心地将 IR 节点的执行放置到适当的设备上。 **否则，如果形状函数和内核的输入需要从 GPU 复制或复制到 GPU，则数据传输和设备同步的大量开销将会发生**。 为了最大限度地减少性能损失，**我们分析程序以将子表达式放置在最合适的设备上**。

我们引入了 a unification based analysis ， 用于根据先前的计算内核调度计算正确的设备放置和分配。 我们设备分析的目标是正确分配每个 IR 节点，以最大限度地减少跨设备 copy 的数量。 我们引入 `DeviceDomain` 的概念来表示设备的域， 包括源和目标。 IR 中的每个表达式默认为空域，这意味着对其设备放置没有限制。 此外，引入了一个新的 IR 结构 `device_copy` 以促进 Nimble 运行时的异构执行。 它**代表不同设备之间的数据传输，并在强制执行跨设备数据复制时插入**。 我们的分析被制定为一组描述设备约束如何流动的设备放置规则(完整的规则集可以在附录 B 中找到)，然后我们使用 unification （一种类型推断和编译器中常见的技术）来计算精确的设备放置。 图 2 描述了分配和传播设备类型的规则的一些要点:

1. shape function 的输入和输出都分配给 CPU
2. `device_copy` 的输出分配给复制到的设备
3. `invoke_mut` 的所有参数都应该有相同的设备域

<div class="autocb" style="text-align:center;"><img src="./paper-nimble.assets\autocb_3.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

基于这些规则，我们使用 并查集 来双向传播和统一每个 IR 节点的设备放置。 我们引入 `union(s, t)` 和 `find(s)` 两个操作来实现整个程序中 `DeviceDomain` 的统一。 `union(s,t)` 当设备类型匹配时，将 s 和 t 的等价设备域合并为一个等价域。 `find(s)` 返回 s 所属设备域的代表。 应用这两个操作，**直到所有 IR 节点都被 annotated 为止**。 如果对设备域没有约束，我们将 compilation target (i.e., GPU) 分配给它以支持更好的内核性能。 异构设备放置的结果由内存规划和形状函数插入组成，从而达到正确地放置分配。

### 3.5. Symbolic Codegen
DLC 在多个平台上展示了与手动调整内核相比具有竞争力的性能。 最近的趋势是应用 ML-Based 搜索来进一步减少或消除复杂的手动性能调整，使用 template based（Chen 等人，2018b；Zheng 等人，2020b）或 search based (Adams 等人，2019； Zheng 等人，2020a) 方法。 然而，现有工作专注于静态形状，在 符号 或 动态形状 方面存在不足。 关于符号形状的代码生成，存在两个固有的挑战：

- 在应用相同的调度时，如何使使用符号形状生成的内核与使用静态形状生成的内核具有相同的性能？
- 如何扩展基于机器学习的方法来调整具有符号形状的内核？

循环并行和循环分块(tiling)是常见的优化技术，它们通过实现数据访问模式来利用多核功能，这些模式对 CPU 和 GPU 都具有内存层次结构感知能力。 然而，这些技术的组合会**导致复杂的 loop boundary conditions**。 在静态情况下，我们可以证明这些 conditions 始终成立，从而消除阻碍进一步优化（如 unrolling）的检查。 但在执行符号代码生成时， 如果处理不当，限制条件（边界）检查将保留下来，导致性能不佳。

为了解决这个问题，我们根据平铺因子的残差模生成多个内核，然后在运行时根据实际形状进行调度。 例如，假设符号维度 $x$ is tiled by a factor of 8，然后我们将生成的内核复制 8 次，并在每次复制中将符号变量 $x$ 替换为 $8k+r$，其中 $k = \lfloor x/8\rfloor$ ， $r \in [0..7]$ 。 通过将此技术与增强的符号表达式简化 Pass 结合使用，我们可以消除大多数边界检查， 以实现与使用单个静态形状编译的内核几乎相同的性能。 最后，我们自动生成一个调度函数，根据残差调用相应的内核。 此外，调度功能可以扩展为调用编译器生成的内核或第三方库，以分析结果中速度更快的为准。 与整体深度学习模型相比，增加的内核大小相对较小。 在资源极其有限的情况下，我们可以生成比平铺因子更少的内核数，或者降低平铺因子以在代码大小和性能之间找到可接受的权衡。

**基于机器学习的调优的一个已知问题是需要很长时间（通常是数小时）才能找到单个内核的最佳调度**。 

!!! note "note"
    这里联想到了ROLLER-OSDI 2022

当涉及到符号形状时，如果我们天真地针对每个可能的形状进行 tuning， 调整时间可能会成倍增加。 在本文中，我们扩展了符号形状的基于模板的调整方法，使调整时间不那么长。 基于模板的调优方法采用人工定义的代码模板和搜索空间， 并使用机器学习算法在搜索空间内搜索最佳配置。 我们观察到， 一种形状的良好配置通常在其他形状上表现良好。 基于这一观察，我们设计了以下机制来调整内核的符号形状：

1. 首先用足够大的值（例如64）替换符号维度，以便搜索空间可以覆盖大多数可能性，并在静态形状上运行调整算法进行足够次数的迭代。
2. 选择前 k 个配置，将它们应用于其他形状的选择，并评估它们的性能。
3. 选择在先前评估的形状中平均表现最好的配置。

根据经验，我们发现 k = 100 涵盖了其他形状的大部分最佳配置。 **当前流行的动态模型通常只需要具有一个符号变量的内核**。 因此，我们在其他形状的交叉评估中选择了至多 $2^{256}$。 **如果有多个符号变量，则可能需要一种更复杂的选择方法来限制 步骤 2 的评估时间**。我们将此留给未来的工作。 此外，如果工作负载分布已知，我们可以在 步骤 3 中调整已知形状的权重。

!!! warning "疑问"
    这几句没看懂？

## 4. VIRTUAL MACHINE
现有 DLC 的传统运行时 naively 按拓扑顺序逐个执行模型算子，这不适用于执行动态模型的编译模块。 需要一个更智能、更强大的执行引擎来**处理 控制流 执行逻辑，并相应地调度不同的内核**。 为了实现这些目标并可移植到不同的平台，我们设计并实现了一个基于虚拟机 (VM) 的运行时。

在 Nimble 中，我们将一个动态模型编译成一个 VM 可执行文件，其中**包含与平台无关的字节码 和 与平台相关的内核代码**。 字节码由一系列指示内核调用顺序**和控制流执行逻辑**的指令组成。 这种设计补充了传统运行时执行高度优化内核但不直接处理内核之间编排的能力。

VM 指令集的设计 is motivated by the simple observation that kernel execution dominates neural network execution time 。 **它与传统语言虚拟机截然不同**，传统语言虚拟机包含许多执行很少工作的指令，导致 profile 出来每条指令的执行时间都很高。 而我们的 ISA 由 CISC-style instructions 构成，**每条指令对应于张量上的原始 IR 表达式，例如分配和内核调用**， 这反过来可能对应于执行多个 "低级" 操作。 例如，`LoadConst idx, $reg` 具有多种寻址模式，因为它首先读取 `idx`，然后将数据从 常量池 加载到 `$reg`。 完整的指令集列表可以在附录 C 中找到。 我们自然地选择了基于寄存器的虚拟机设计以获得紧凑的字节码，便于用户阅读和修改。 我们提供了无限组虚拟寄存器的抽象，因为它显著简化了优化和分配（类似于 SSA），并最大限度地减少了快速原型设计和修改的概念障碍。

指令使用包含操作码和数据有效负载的传统标记联合表示。 这种表示可以实现高效的序列化以及指令解码和分派。 由于在指令中包含可变大小的操作数（例如数据形状），Nimble 使用可变长度指令格式。

在我们从编译阶段生成可执行文件后，可以创建一个解释器来加载它。 当执行开始时，解释器运行一个调度循环，检查操作码并执行适当的逻辑，然后重复。 由于我们的指令是粗粒度的（即，它们可以被视为超级指令）， 调度循环生成的分支数低于传统编程语言 VM， 与 AOT 相比，增加的开销可以忽略不计。

**Discussion** 运行时的替代解决方案是 AOT 以消除 dispatch 开销。 **但是由于操作的粒度，调度时间在执行中只占很小的一部分**。 更重要的是，我们的 VM 提供了传统上归因于虚拟机的灵活性以及清晰的 编译器/运行时划分。 我们看到了 VM 作为运行时模块集成到更大系统中的潜力。 例如，VM 可以提供资源隔离，其中多个推理实例在云中共享相同的硬件。 此外，服务质量 (QoS) 感知系统，例如（Kang 等人，2018 年；Yachir 等人，2009 年）， 可以利用 VM 暂停当前模型的执行，以获得更高优先级或时间关键的模型。 最后，由于 VM 设计的简单性，人们可以出于安全和隐私目的验证 VM 的实现。

## 5. EVALUATION
...

## 6. RELATED WORK
本节将 Nimble 与执行动态神经网络的现有解决方案进行对比。

**DL 框架** 一些框架通过在其 图 IR 中添加原语来支持动态控制流，例如 Tensor-Flow 中的切换和合并（Yu 等人，2018 年）以及 MXNet 中的 foreach、cond 和 while 循环(Zheng , 2018). 控制流的间接编码需要专门的数据流运行时来处理诸如切换之类的操作，或者需要将控制平面和数据平面的执行分开的混合运行时。 两者都严重侵入了框架代码库。 此外，还有许多框架扩展来支持不同类型的动态。TensorFlow Fold（Looks 等人，2017 年）对用户提供的计算图进行分析，以确定可以一起批处理的动态操作。 一旦找到这样的操作，就会生成一个批处理的 TensorFlow 图，它提供形状专用的子图。 虽然这可能会提供加速，但它会引入大量开销，因为每条路径都必须作为单独的子计算图执行，并且会限制进一步的优化。 郑等人。 （Jeong 等人，2018 年）和 JANUS（Jeong 等人，2019 年）也扩展了 TensorFlow 以提高动态模型的性能。 这些扩展是特定于框架的，与我们使用的编译技术没有直接关系。 speculative execution 的使用是对我们技术的补充。

Dynet（Neubig 等人，2017 年）和 PyTorch（Paszke 等人，2019 年）使用 host 语言功能（即 Python 的控制流）来动态展开控制流以生成动态模型的静态trace。 基于trace的方法以提前优化为代价提供了灵活友好的编程模型。 此外，它需要为每个trace创建一个数据流图，引入控制流构造的开销并限制整个程序的优化，这是传统 trace JIT 编译中也面临的挑战。 JAX（Bradbury 等人，2018 年）也支持受限形式的动态网络，但其优化从根本上受到其底层编译器 XLA 的限制。 例如，JAX 的 JIT 模式中 dynamic value dependent control-flow is not supporte。 相比之下，Nimble 在不影响静态子集性能的情况下降低了动态行为的使用成本。

此外，框架依赖于第三方库（Chetlur et al., 2014; Intel, 2020）来实现具有不同数据形状的算子，即只在特定硬件平台上表现较好。 因此，框架以及从它们派生的用于动态模型的运行时系统（Xu et al., 2018; Gao et al., 2018）通常在 ARM CPU 等second tier of support的设备上表现不佳，而在流行的基准测试中找不到算子和形状组合。 相比之下，Nimble 适用于所有平台，并且可以根据需要为新形状和新设备生成高性能代码（第 3.5 节）。

**DLC** 现有的 DLC ， 包括 XLA (XLA Team, 2017)、TVM (Chen et al., 2018a) 和 Glow (Rotem et al., 2018)，可以编译深度学习模型以在多个硬件上运行 具有加速性能的平台。 然而，在优化动态神经网络编译方面的工作很少。 **MLIR (Lattner et al., 2021) 是一个很有前途的方向，其 IR 支持动态形状，但动态优化或端到端性能未知**。 Nimble 的编译和 VM 设计在很大程度上受到 production compiler and VM 的启发，例如 LLVM（Lattner & Adve，2004）、GCC（gcc，2019）和 JVM（jvm，2013），用于处理动态行为的通用解决方案，例如控制流和可变长度输入数组。

# The Deep Learning Compiler: A Comprehensive Survey

## 摘要

在不同的 DL 硬件上部署各种深度学习模型的困难推动了社区对 DLC 的研究和开发。 工业界和学术界已经提出了几种 DLC ，例如 Tensorflow XLA 和 TVM。 同样，DLC 将不同 DL 框架中描述的 DL 模型作为输入，然后为不同的 DL 硬件生成优化代码作为输出。 然而，现有的调查都没有全面分析 DLC 的独特设计架构。 在本文中，我们通过详细剖析普遍采用的设计，对现有的 DLC 进行了全面调查，重点是面向 DL 的多级 IR 和前端/后端优化。 我们对多级 IR 的设计进行了详细分析，并说明了常用的优化技术。 最后，强调了一些见解作为 DLC 的潜在研究方向。 这是第一篇专注于 DLC 设计架构的调查论文，我们希望它能为未来对 DLC 的研究铺平道路。

## 1. Intro

随着CNN、RNN、GAN等多种深度学习模型的出现，简化各种 DL 模型的编程以实现它们的落地至关重要。

随着工业界和学术界的不断努力，已经提出了几种流行的 DL 框架，例如 TensorFlow、PyTorch、MXNet等， 以简化各种 DL 模型的实现。 尽管上述 DL 框架根据其设计的权衡存在优势和劣势，但在跨现有 DL 模型支持新兴 DL 模型时，互操作性对于减少冗余工程工作变得很重要。 为了提供互操作性，已经提出了 ONNX [66]，它定义了表示 DL 模型的统一格式，以促进不同 DL 框架之间的模型转换。

同时，矩阵乘法等独特的计算特性激发了芯片架构师设计定制化深度学习加速器以提高效率的热情。 互联网巨头（例如 Google TPU[44]、海思 NPU [56]、Apple Bonic [49]）、处理器供应商（例如 NVIDIA Turing [72]、Intel NNP [41]）、服务提供商（例如 Amazon Inferentia [ 8]、Alibaba Hanguang [7]），甚至初创公司（例如 Cambricon [57]、Graphcore [43]）正在投入大量人力和资金开发 DL 芯片，以提高 DL 模型的性能。 通常，DL 硬件可分为以下几类：1）具有软硬件协同设计的通用硬件，2）完全为 DL 模型定制的专用硬件，以及 3）受生物脑科学启发的神经形态硬件。 例如，通用硬件（例如 CPU、GPU）添加了特殊的硬件组件，例如 AVX512 矢量单元和张量核心来加速 DL 模型。 而对于 Google TPU 等专用硬件，专用集成电路（例如矩阵乘法引擎和高带宽内存）的设计旨在将性能和能效提升到极致。 **在可预见的未来，DL 硬件的设计将更加多样化**。

为了拥抱硬件多样性，将计算有效地映射到 DL 硬件非常重要。 在通用硬件上，高度优化的线性代数库，如基本线性代数子程序 (BLAS) 库（例如，MKL 和 cuBLAS）作为 DL 模型高效计算的基础。 以卷积运算为例，DL框架将卷积转换为矩阵乘法，然后调用BLAS库中的GEMM函数。 此外，硬件供应商还发布了专门为 DL 计算量身定制的优化库（例如 MKL-DNN 和 cuDNN），包括前向和反向卷积、池化、归一化和激活。 还开发了更高级的工具以进一步加速 DL 操作。 例如，TensorRT [73] 通过大量高度优化的 GPU 内核支持图优化（例如，层融合）和低位量化。 在专用的 DL 硬件上，也提供了类似的库 [43、57]。 然而，**依赖库的缺点是它们通常落后于 DL 模型的快速发展，因此无法有效地利用 DL 芯片**。

为了解决 DL 库和工具的缺点，并减轻在每个 DL 硬件上手动优化 DL 模型的负担，工业界和学术界提出了几种流行的 DLC ，例如 <u>TVM</u>、<u>Tensor Comprehension</u>、<u>Glow</u>、<u>nGraph</u> 和 <u>XLA</u>。 DLC 将 DL 框架中描述的模型定义作为输入，并在各种 DL 硬件上生成高效的代码实现作为输出。 模型定义和具体代码实现之间的转换针对模型规范和硬件架构进行了高度优化。 具体来说，它们结合了面向 DL 的优化，例如层和运算符融合，从而实现高效的代码生成。 此外，现有的 DLC 还利用了来自通用编译器（例如 LLVM [51]）的成熟工具链，这提供了跨不同硬件架构的更好的可移植性。 与传统编译器类似，DLC 也采用分层设计，包括前端、中间表示 (IR) 和后端。 然而，**DLC 的独特之处在于多级 IR 的设计和 DL 特定的优化**。

在本文中，我们通过将编译器设计分解为前端、多级 IR 和后端，对现有 DLC 进行了全面调查，并特别强调了 IR 设计和优化方法。 据我们所知，这是第一篇对 DLC 的设计进行全面调查的论文。 具体来说，本文做出以下贡献：

1. 剖析了现有 DLC 普遍采用的设计架构，并详细分析了关键设计组件，如 多级 IR、 前端优化 和 后端优化
2. 从各个方面对现有的 DLC 进行了全面的分类，这与本次调查中描述的关键组件相对应。 该分类的目标是为从业者考虑其需求的 DLC 选择提供指南，并为研究人员提供 DLC 的全面总结。
3. 提供了 DLC 在 CNN 模型上的定量性能比较，包括完整模型和轻量级模型。比较了端到端和每层（卷积层——主导推理时间）性能以显示优化。 [评估脚本和结果已开源](https://github.com/buaa-hipo/dlcompiler-comparison)供参考。
4. 对 DLC 未来发展的几个见解，包括动态形状和预处理/后处理、高级自动调整、多面体模型、子图划分、量化、统一优化、可微分编程和隐私保护，我们希望推动这些 DLC 社区的研究。

文章 第 2 节介绍了 DLC 的背景，包括 DL 框架、DL 硬件以及硬件 (FPGA) 特定的 DL 代码生成器。
第 3 节描述了 DLC 的通用设计架构。 第 4 节讨论 DLC 的关键组件，包括多级 IR、前端优化和后端优化。 第 5 节介绍了综合分类。 第 6 节提供了定量性能比较。 第 7 节强调了 DLC 研究的未来方向。

## 7. CONCLUSION AND FUTURE DIRECTIONS

论文针对设计原则对现有的 DLC 进行了全面分析。 首先深入研究现有 DLC 采用的通用架构，包括 多级 IR、 前端 和 后端。 我们详细介绍了每个组件的设计理念和参考实现，重点介绍了特定于 DLC 的独特 IR 和优化。 我们总结了本次调查的结果，并强调了 DLC 的未来发展方向，如下所示：

1. **Dynamic shape and pre/post processing**

    动态模型在 DL 领域越来越流行，其输入形状甚至模型本身在执行过程中都可能发生变化。 特别是在 NLP 领域，模型可能接受各种形状的输入，这对 DLC 来说是一个挑战，因为数据的形状在运行时是未知的。 现有的 DLC 需要更多的研究工作才能有效地支持新兴动态模型的动态形状。 此外，随着未来的深度学习模型变得越来越复杂，它们的整个控制流程可能不可避免地包括复杂的预处理/后处理过程。 目前，大多数 DLC 使用 Python 作为其编程语言，当由 Python 解释器执行时，预处理/后处理可能成为性能瓶颈。 现有的 DLC 尚未考虑这种潜在的性能瓶颈。 支持 DLC 中的整个控制流可以表达和优化预处理/后处理以及 DL 模型，这为模型部署中的性能加速开辟了新的机会。

2. **Advanced auto-tuning**

    现有的自动调整技术侧重于individual operators的优化。 但是，局部最优的组合不会导致全局最优。 例如，可以将应用于不同数据布局的两个相邻运算符一起调整，而无需在其间引入额外的内存转换。 此外，随着边缘计算的兴起，执行时间不仅是深度学习编译器的优化目标。 自动调整中还应考虑新的优化目标，例如内存占用和能耗。 特别是，对于基于 ML 的自动调整技术，有几个方向值得进一步探索。 首先，ML 技术可以应用于自动调整的其他阶段，而不是成本模型。 例如，在选择编译器选项和优化计划阶段，可以使用 ML 技术直接预测可能性并开发算法来确定最终配置。 其次，可以基于领域知识改进基于 ML 的自动调整技术。 例如，在自动调整技术中结合特征工程（选择特征来表示程序）[99] 可能是获得更好调整结果的潜在方向。

3. **Polyhedral model**

    在 DLC 的设计中结合多面体模型和自动调整技术以提高效率是一个很有前途的研究方向。 一方面，可以应用自动调整通过重用以前的配置来最小化多面体 JIT 编译的开销。 另一方面，多面体模型可以用来进行自动调度，减少自动调整的搜索空间。

    在 DLC 中应用多面体模型的另一个挑战是支持稀疏张量。 通常，CSF [84] 等稀疏张量的格式用不再线性的索引数组（例如 a[b[i]]）表示循环索引。 这种间接索引寻址导致非仿射下标表达式和循环边界，这禁止了多面体模型的循环优化 [14, 90]。 幸运的是，多面体社区在支持稀疏张量方面取得了进展 [94、95]，并且集成多面体模型的最新进展可以增加 DLC 的性能机会。

4. **Subgraph partitioning**

    支持子图划分的 DLC 可以将计算图划分为多个子图，子图可以以不同的方式进行处理。 子图划分为 DLC 提供了更多的研究机会。 首先，它开辟了集成图形库以进行优化的可能性。 以 nGraph 和 DNNL 为例，DNNL 是一个 DL 库，利用大量高度优化的内核进行图形优化。 DNNL 与 nGraph 的集成使 DNNL 能够加速 nGraph 生成的子图的执行。 其次，它开辟了异构和并行执行的可能性。 一旦计算图被划分为子图，不同子图的执行可以同时分配给异构硬件目标。 以边缘设备为例，它的计算单元可能包括ARM CPU、Mail GPU、DSP，可能还有NPU。 从有效利用所有计算单元的 DLC 生成子图可以显着加快 DL 任务。

5. **Quantization**

    DL 框架中应用的传统量化策略基于一组固定的方案和数据类型，几乎没有针对在不同硬件上运行的代码进行定制。 然而，在 DLC 中支持量化可以在编译期间利用优化机会来导出更有效的量化策略。 例如，Relay [78] 提供了一种量化重写流程，可以为各种方案自动生成量化代码。 为了支持量化，DLC 需要解决几个挑战。 第一个挑战是如何在不进行大量工程工作的情况下实施新的量化运算符。 AWS的尝试指出了一个可能的方向，即利用dialect的概念，在基本算子的基础上实现新的算子，使图级和算子级的优化可以复用。 第二个挑战是编译期间量化和其他优化之间的交互。 例如，确定量化的适当阶段并与算子融合等优化合作需要未来的研究调查。

6. **Unified optimizations**

    尽管现有的 DLC 在计算图优化和硬件特定优化方面采用类似的设计，但每个编译器在某些方面都有自己的优势。 缺少共享最先进的优化以及跨现有编译器支持新兴硬件目标的方法。 我们提倡统一现有 DLC 的优化，以便可以重用每个 DLC 采用的最佳实践。 此外，统一 DLC 的优化可以积累强大的力量来影响通用和专用 DL 加速器的设计，并为 DLC 和硬件的高效协同设计提供环境。

    目前，谷歌 MLIR 是朝这个方向迈出的一项有前途的举措。 它提供了多级 IR 的基础设施，并包含 IR 规范和工具包以在每个级别执行跨 IR 的转换。 它还提供了灵活的方言，因此每个 DLC 都可以为高级和低级 IR 构建其定制的方言。 通过跨方言的转换，一个 DLC 的优化可以被另一个编译器重用。 然而，方言的转变需要进一步的研究努力，以减少对精致设计的依赖。

7. **Differentiable programming**

    可微编程是一种编程范式，其中程序是完全可微的。 用可微分编程范式编写的算法可以自动微分，这对 DL 社区很有吸引力。 许多编译器项目都采用了可微分编程，例如 Myia [89]、Flux [40] 和 Julia [13]。 不幸的是，现有的 DLC 几乎不支持差分编程。

    支持差异化编程对于现有的 DLC 来说是相当具有挑战性的。 困难不仅来自数据结构，还来自语言语义。 例如，要实现从 Julia 到 XLA HLO IR 的转换，挑战之一 [24] 是 Julia 使用的命令式语言与 XLA 使用的符号语言之间的控制流不同。 为了高效地使用 HLO IR，编译器还需要为 Julia 提供操作抽象，以支持 XLA 的特定语义，例如 MapReduce 和广播。 此外，Julia 和 XLA 之间语义差异的区分，也需要进行重大更改

8. **Privacy protection**

    在边缘-云系统中，DL 模型通常被分成两半，每个部分模型分别运行在边缘设备和云服务上，这样可以提供更好的响应延迟并消耗更少的通信带宽。 然而，边缘云系统的缺点之一是用户隐私变得脆弱。 原因是攻击者可以拦截从边缘设备发送到云端的中间结果，然后使用中间结果训练另一个模型，该模型可以揭示偏离原始用户任务的隐私信息。

    为了保护边缘云系统中的隐私，现有方法 [27、67、74] 建议在中间结果中添加具有特殊统计特性的噪声，这可以降低攻击者任务的准确性，而不会严重降低用户任务的准确性。 然而，困难在于确定应该插入噪声的层，这对于确定最佳层是相当费力的。 上述困难为 DLC 提供了一个很好的机会来支持隐私保护，因为编译器保留了 DL 模型的丰富信息，可以自动引导噪声插入跨层。

9. **Training support**

    通常，当前的 DLC 对模型训练的支持要少得多。 如表1所示，nGraph只支持在Intel NNP-T加速器上训练，TC只支持单核自动微分，Glow有有限模型的实验性训练支持，TVM的训练支持正在开发中，XLA 依赖 关于 TensorFlow 的训练支持。 总而言之，当前的 DLC 主要专注于弥合将 DL 模型有效部署到不同硬件上的差距，因此他们选择推理作为他们的主要优化目标。 然而，扩展 DLC 的能力以支持模型训练将开辟大量的研究机会，例如梯度算子的优化和高阶自动微分。

## 4. KEY COMPONENTS OF DL COMPILERS
### 4.1. High-level IR
为了克服传统compiler中采用的 IR 限制 DL 模型中使用的复杂计算的表达的限制，现有的 DL compiler利用具有特殊设计的高级 IR（称为图 IR）来实现高效的代码优化。 为了更好地理解 DL compiler中使用的图 IR，我们描述图 IR 的表示和实现如下。

#### 4.1.1. 图IR的表示

图 IR 的表示方式会影响 IR 对于计算的表现能力，以及 DLC 分析 graph IR 的方式。

- **DAG-based IR**

    基于 DAG 的 IR 是compiler 构建计算图的最传统方法之一，节点和边组织为有向无环图 (DAG)。 在 DL compiler中，DAG 的节点代表算子原语 （卷积、池化等），而边代表张量。 并且该图是非循环的，没有循环，这与通用 compiler 的数据依赖图(DDG) 不同。

    > 无环意味着图中没有一条路径从同一节点开始和结束，并且沿途只访问每个节点一次。
    > 
    > 首先，它确保由图表示的计算可以按照明确定义的顺序执行，并且在执行任何操作之前所有输入都可用。 这使得对图执行拓扑排序成为可能，它提供了一系列可以直接执行的操作。
    > 
    > 其次，它允许清楚地表示操作之间的依赖关系。 DAG 中的每个节点都有一组代表其输入的传入边和一组代表其输出的传出边。 非循环属性可确保不存在循环或循环依赖性，从而易于理解数据如何流经图形。
    > 
    > 最后，非循环属性简化了图的分析和优化。 例如，当图是非循环的时，许多图算法和优化更容易实现和应用，因为无需担心会使计算复杂化的循环或循环。
    > 
    > 总的来说，DAG 的非循环属性是它们在表示计算图方面有用的一个重要方面，并使它们非常适合许多机器学习和其他计算应用程序。
    > 

    借助 DAG 计算图，DLC 可以分析各种算子之间的关系和依赖关系，并使用它们来指导优化。 DDG上已经有很多优化，比如公共子表达式消除（CSE）和死代码消除（DCE）。 通过将 DL 的领域知识与这些算法相结合，可以将进一步的优化应用于 DAG 计算图。 基于DAG的IR非常简单，因此便于编程和编译，**但存在计算范围定义缺失导致的语义歧义等不足**。

- **Let-binding-based IR**

    Let-binding 是一种解决语义歧义的方法，它通过为许多高级编程语言（如 Javascript、F#、Rust等）使用的范围受限的某些函数提供 let 表达式来解决语义歧义。 当使用 let 关键字定义表达式时，会生成一个 let 节点，然后它指向表达式中的运算符和变量，而不是像 DAG 那样只是建立变量之间的计算关系。 

    在基于 DAG 的编译器中，当需要获取某个表达式的返回值时，它首先访问相应的节点并搜索相关节点，也称为递归下降技术。 相反，基于 let-binding 的编译器计算出 let 表达式中变量的所有结果并构建变量映射。 当需要一个特定的结果时，编译器会查找这个映射来决定表达式的结果。 在 DLC 中，TVM 的 Relay IR 采用了基于 DAG 的 IR 和 let-binding-based IR 以获得两者的好处。

- **Representing Tensor Computation**

    不同的图 IR 有不同的方式来表示对张量的计算。 各种 DL 框架的算子根据此类特定表示被转换为图 IR。 并且定制的算子也需要以这种表示形式进行编程。 

    张量计算的表示可以分为以下三类:

    1. **Function-based**: 
    基于函数的表示只是提供了封装的算子，Glow、nGraph 和 XLA 都采用了这种方法。 以High Level Optimizer（HLO，XLA的IR）为例，它由 symbolic programming 中的一组函数组成，而且大部分都没有副作用。 指令分为三个层次，包括 `HloModule` （整个程序）、 `HloComputaion` （一个函数）和 `HloInstruction` （操作）。 XLA使用 HLO IR 同时表示 graph IR 和 operation IR，使得HLO的操作从数据流层面延伸到算子层面。

    2. **Lambda expression**: 
    lambda 表达式是一种索引公式表达式(an index formula expression)，通过变量绑定和替换来描述计算。 使用 lambda 表达式，程序员可以在不实现新函数的情况下快速定义计算。 TVM 表示使用基于 lambda 表达式的 tensor expression 进行张量计算。 在 TVM 中，张量表达式中的计算算子由输出张量的形状和计算规则的 lambda 表达式定义。

    3. **Einstein notation**: 
    爱因斯坦符号，也称为求和约定，是一种表达求和的符号。 它的编程简单性优于 lambda 表达式。 以TC为例，临时变量的索引不需要定义。 IR 可以根据爱因斯坦符号，通过未定义变量的出现来计算出实际表达式。 在爱因斯坦符号中，运算符需要结合和交换。 这种限制保证归约运算符可以按任何顺序执行，从而有可能进一步并行化。

#### 4.1.2. 图IR的实现
图 IR 在 DLC 中的实现完成了数据和操作的管理。

- **Data representation**: 

    DLC 中的数据（例如，输入、权重和中间数据）通常以张量的形式组织，也称为多维数组。 DLC 可以直接用内存指针表示张量数据，或者以更灵活的方式用占位符表示。 占位符包含张量每个维度的大小。 或者，张量的维度大小可以标记为未知。 为了优化，DLC 需要数据布局信息。 此外，应根据占位符推断迭代器的边界。

    1. **Placeholder**: 
   占位符广泛用于符号编程（例如，Lisp、Tensorflow）。 占位符只是一个具有明确形状信息（例如，每个维度的大小）的变量，它将在计算的后期阶段用值填充。 它允许程序员在不考虑确切数据元素的情况下描述操作和构建计算图，这有助于将计算定义与 DLC 中的确切执行分开。 此外，在不改变计算定义的情况下，程序员可以使用占位符方便地改变输入/输出的形状和其他相应的中间数据。

    2. **Unknown (Dynamic) shape representation**: 
    声明占位符时通常支持未知维度大小。 例如，TVM 使用 Any 来表示未知维度（例如，`Tensor ⟨(Any, 3), fp32⟩`）； XLA 使用 None 来实现相同的目的（例如，`tf.placeholder ("float", [None, 3])`）； nGraph 使用其 `PartialShape` 类。 **未知形状表示是支持动态模型所必需的**。 然而，为了完全支持动态模型，应该放宽绑定推理和维度检查。 此外，应该实施额外的机制来保证内存有效性。

    3. **Data layout**: 
    数据布局描述了张量在内存中的组织方式，通常是逻辑索引到内存索引的映射。 数据布局通常包括维度序列（例如，NCHW 和 NHWC）、平铺、填充、步幅等。TVM 和 Glow 将数据布局表示为算子参数，并且需要此类信息进行计算和优化。 然而，将数据布局信息与 算子 而不是张量相结合可以实现某些 算子 的直观实现并减少编译开销。 XLA 将数据布局表示为与其后端硬件相关的约束。 Relay 和 MLIR 将把数据布局信息添加到他们的张量类型系统中。

    4. **Bound inference**: 
    在 DLC 中编译 DL 模型时，应用边界推理来确定迭代器的边界。 尽管 DLC 中的张量表示可以方便地描述输入和输出，但它暴露了推断迭代器边界的特殊挑战。 根据计算图和已知的占位符，边界推理通常以递归或迭代的方式执行。 例如，在 TVM 中，迭代器形成一个有向无环超图，其中图的每个节点表示一个迭代器，每个超边表示两个或多个迭代器之间的关系（例如，split, fuse or rebase）。 一旦根据占位符的形状确定了根迭代器的边界，就可以根据关系递归地推断出其他迭代器。

- **算子支持**

    DLC 支持的算子负责表示 DL 工作负载，它们是计算图的节点。 算子通常包括代数算子（例如 +、×、exp 和 topK）、神经网络算子（例如 卷积 和 池化 ）、张量算子（例如 reshape、resize 和 copy）、广播和约简算子（例如 min 和 argmin），以及控制流算子（例如，条件和循环）。 在这里，我们选择三个在不同 DLC 中经常使用的代表性算子进行说明。 此外，我们讨论了定制算子的情况

    1. **Broadcast**: 
    广播 算子可以复制数据并生成具有兼容形状的新数据。 如果没有广播算子，输入张量的形状会受到更多限制。 例如，对于加法算子，输入张量应具有相同的形状。 XLA 和 Relay 等一些编译器通过提供广播算子放宽了此类限制。 例如，XLA 允许在矩阵和向量上进行逐元素加法，方法是复制它直到其形状与矩阵匹配。

    2. **Control flow**: 
    表示复杂和灵活的模型时需要控制流。 RNN 和强化学习 (RL) 等模型依赖于循环关系和数据依赖条件执行，这需要控制流。 如果不支持 DLC 的 图IR 中的控制流，这些模型必须依赖宿主语言的控制流支持（例如，Python 中的 if 和 while）或静态展开，这会降低计算效率。 Relay 注意到任意控制流可以通过递归和模式来实现，函数式编程已经证明了这一点。 因此，它提供了 if 运算符和递归函数来实现控制流。 相反，XLA represents control flow by special HLO operators such as while and conditional.

    3. **Derivative**: 
    operatorOp 的导数算子以 Op 的输出梯度和输入数据为输入，计算 Op 的梯度。 尽管一些 DLC （例如 TVM 和 TC）支持自动微分，但在应用链式法则时，它们需要高级 IR 中所有运算符的导数。 TVM 正致力于提供代数运算符和神经网络算子的导数运算符。 程序员可以使用这些导数运算符来构建自定义运算符的导数。 相反，PlaidML 可以自动生成衍生算子，甚至是自定义算子。 值得注意的是，**无法支持导数运算符的 DLC 无法提供模型训练的能力**。

    4. **Customized operators**: 
    它允许程序员为特定目的定义他们的算子。 支持自定义算子可提高 DLC 的可扩展性。 例如，在 `Glow` 中定义新的算子时，程序员需要实现逻辑和节点封装。 此外，如果需要，还需要额外的工作，例如 lowering 步骤、 operation IR generation 和 指令生成。 然而，除了描述计算实现之外，TVM 和 TC 需要较少的编程工作。 具体来说，TVM 的用户只需描述计算和调度，并声明输入/输出张量的形状。 而且，自定义的算子通过钩子的方式集成了Python函数，进一步减轻了程序员的负担。

#### 4.1.3. Discussion
几乎所有 DLC 都有其独特的 High-level IR。 但它们有相似的设计理念，例如使用 DAG 和 let-binding 来构建计算图。 此外，它们通常为程序员提供方便的方式来表示张量计算。 High-level IR 中设计的数据和算子具有足够的灵活性和可扩展性，足以支持各种 DL 模型。 更重要的是，High-level IR 是独立于硬件的，因此可以应用于不同的硬件后端。

### 4.2. Low-level IR
#### 4.2.1 Low-Level 的实现
Low-level IR 以比 high-level IR 更细粒度的表示形式描述了 DL 模型的计算，它通过提供接口来调整计算和内存访问来实现目标相关的优化。 在本节中，我们将低级 IR 的常见实现分为三类：基于 Halide-based IR、 基于多面体的 IR 和其他独特的 IR。

- **Halide-based IR**: 

    Halide 首次被提出用于并行化图像处理，并被证明在 DLC（例如 TVM）中具有可扩展性和高效性。 Halide 的基本理念是计算和调度的分离。 采用 Halide 的编译器不是直接给出特定的方案，而是尝试各种可能的方案并选择最佳方案。 Halide 中内存引用和循环嵌套的边界are restricted to bounded boxes aligned to the axes。 因此，Halide 无法用复杂的模式（例如，非矩形）表达计算。 幸运的是，DL 中的计算非常有规律，可以用 Halide 完美表达。 此外，Halide 可以轻松地参数化这些边界并将它们暴露给调整机制。 Halide 的原始 IR 在应用于 DLC 的后端时需要进行修改。 例如，Halide 的输入形状是无限的，而 DLC 需要知道数据的确切形状才能按顺序将算子映射到硬件指令。 一些编译器，例如 TC，需要固定大小的数据，以确保张量数据具有更好的时间局部性。

    TVM通过以下努力将 Halide IR改进为一个独立的符号IR。 它消除了对 LLVM 的依赖，重构了项目模块的结构和 Halide 的 IR 设计，追求更好的组织以及图形 IR 和前端语言（如 Python）的可访问性。 复用性也得到了提升，实现了运行时调度机制，方便添加自定义算子。 TVM 将变量定义从字符串匹配简化为指针匹配，保证每个变量都有一个单一的定义位置（static single-assignment，SSA）。

- **Polyhedral-based IR**: 

    多面体模型是 DLC 采用的一项重要技术。 它使用线性规划、仿射变换和其他数学方法来优化具有边界和分支静态控制流的基于循环的代码。 与Halide不同的是，内存引用和循环嵌套的边界可以是多面体模型中任意形状的多面体。 这种灵活性使得多面体模型广泛用于通用编译器。 然而，这种灵活性也阻碍了与调整机制的集成。 尽管如此，由于能够处理深度嵌套循环，许多 DLC，例如 TC 和 PlaidML（作为 nGraph 的后端），都采用多面体模型作为其低级 IR。 基于多面体的 IR 可以轻松应用各种多面体变换（例如，fusion, tiling, sinking, and mapping），包括与设备相关和与设备无关的优化。 基于多面体的编译器借用了许多工具链，例如 isl、Omega、PIP、Polylib 和 PPL。


    **TC**在 Low-level IR 方面有其独特的设计，它结合了 Halide 和 Polyhedral 模型。 它用 Halide-based IR 来表示计算，用 Polyhedral-based IR 来表示循环结构。 TC通过抽象实例呈现详细表达式，并引入具体的节点类型。 简而言之，TC使用 `domain` 节点来指定索引变量的范围，并使用 `context` 节点来描述与硬件相关的新迭代变量。 它使用 `band` 节点来确定迭代顺序。 过滤器节点表示与语句实例组合的迭代器。 `Set` 和 `sequence` 是指定过滤器执行类型（并行和串行执行）的关键字。 此外，TC使 extension nodes 来描述代码生成的其他必要指令，例如内存移动。

    **PlaidML** 使用 Polyhedral-based IR（称为 Stripe）来表示张量运算。 它通过将并行多面体块的嵌套扩展到多个级别来创建可并行代码的层次结构。 此外，它允许将嵌套的多面体分配给嵌套的内存单元，提供了一种将计算与内存层次结构相匹配的方法。 在 Stripe 中，硬件配置独立于内核代码。 Stripe 中的标签（在其他编译器中称为传递）不会更改内核结构，但会为优化传递提供有关硬件目标的附加信息。 Stripe 将 DL 算子拆分为适合本地硬件资源的块。

- **Other unique IR**: 

    有 DLC 在不使用 Halide 和多面体模型的情况下实现定制的低级 IR。 在定制的低级 IR 上，他们应用特定于硬件的优化并降低到 LLVM IR。

    **Glow** 中的 low-level IR 是一种 instruction-based expression ，它对地址引用的张量进行操作。 Glow 低级 IR 中有两种基于指令的函数： `declare` 和 `program` 。 第一个声明在程序的整个生命周期中存在的常量内存区域的数量（例如，输入、权重、偏差）。 第二个是本地分配区域的列表，包括函数（例如 conv 和 pool）和临时变量。 指令可以在全局内存区域或本地分配的区域上运行。 此外，每个操作数都用限定符之一进行注释： `@in` 表示操作数从缓冲区读取； `@out` 表示操作数写入缓冲区； `@inout` 表示操作数读写缓冲区。这些指令和操作数限定符帮助 Glow 确定何时可以执行某些内存优化。

    **MLIR** 深受 LLVM 的影响，它是一个比 LLVM 更纯粹的编译器基础设施。 MLIR 重用了 LLVM 中的许多思想和接口，并且介于模型表示和代码生成之间。 MLIR 具有灵活的类型系统并允许多个抽象级别，它引入方言(dialects) 来表示这些多个抽象级别。 每个方言都包含一组定义好的不可变操作。 MLIR 目前的方言包括 TensorFlow IR、XLA HLO IR、experimental polyhedral IR、LLVM IR 和 TensorFlow Lite。 还支持方言之间的灵活转换。 此外，MLIR 可以创建新的方言以连接到新的低级编译器，这为硬件开发人员和编译器研究人员铺平了道路。

    **XLA** 的 HLO IR 可以被认为是 both high-level IR and low-level IR，因为 HLO 的粒度足够细，可以表示硬件特定的信息。 此外，HLO 支持特定于硬件的优化，可用于 emit LLVM IR。

#### 4.2.2. Low-Level IR 的代码生成

大多数 DLC 采用的 low-level IR 最终可以降为LLVM IR，并受益于LLVM成熟的优化器和代码生成器。 此外，LLVM 可以从头开始为专用加速器显式设计自定义指令集。 然而，传统的编译器在直接传递给 LLVM IR 时可能会生成糟糕的代码。 为了避免这种情况，DLC 采用了两种方法来实现硬件相关优化：

1. 在 LLVM 的上层 IR 中执行目标特定的循环转换（例如，Halide-based IR and polyhedral-based IR）
2. 提供有关优化过程的硬件目标的附加信息

大多数 DLC 两种方法都会使用，但侧重点不同。 一般来说，更喜欢前端用户的 DLC（例如 TC、TVM、XLA 和 nGraph）可能会关注 1），而更倾向于后端开发人员的 DLC（例如 Glow、PlaidML 和 MLIR）可能会 专注于 2)。

DLC 中的编译方案主要可分为两类：即时（JIT）和提前（AOT）。 对于 JIT 编译器，它可以动态生成可执行代码，并且可以利用更好的运行时信息来优化代码。 AOT 编译器首先生成所有可执行二进制文件，然后执行它们。 因此，它们在静态分析方面比 JIT 编译具有更大的范围。 此外，AOT 方法可以应用于嵌入式平台的交叉编译器（例如，C-GOOD），并支持在远程机器（TVM RPC）和定制加速器上执行。

#### 4.2.3. Discussion
在 DLC 中，low-level IR 是 DL 模型的细粒度表示，它反映了 DL 模型在不同硬件上的详细实现。 low-level IR 包括 Halide-based IRs 、polyhedral-based IRs 和 other unique IRs。 尽管它们在设计上有所不同，但它们利用成熟的编译器工具链和基础设施来提供针对特定硬件的优化和代码生成的定制接口。 low-level IR 的设计也会影响新的 DL 加速器（例如 TVM HalideIR 和 Inferentia，以及 XLA HLO 和 TPU）的设计。

### 4.3. Frontend Optimizations

构建计算图后，前端应用图级优化。 许多优化更容易在图级别识别和执行，因为图提供了计算的全局视图。 这些优化仅适用于计算图，而不适用于后端的实现。 **因此它们是独立于硬件的，可以应用于各种后端目标**。

前端优化通常由 passes 定义，可以通过遍历计算图的节点并执行图变换来应用。 前端提供methods to

1. 从计算图中捕获特定特征 
2. 重写图以进行优化 

除了预定义的pass，开发人员还可以在前端定义自定义pass。 一旦 DL 模型被导入并转换为计算图，大多数 DLC 就可以确定每个操作的输入张量和输出张量的形状。 此功能允许 DLC 根据形状信息执行优化。 图3 显示了使用 Tensorflow XLA 进行计算图优化的示例。 在本节中，我们将前端优化分为三类：
1）<u>节点级优化</u>，2）<u>块级（窥孔，局部）优化</u>，以及 3）<u>数据流级（全局）优化</u>

<div class="autocb" style="text-align:center;"><img src="./dlc-survey\autocb_0.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

#### 4.3.1. Node-level optimizations

计算图的 node 足够粗糙，可以在单个节点内进行优化。 Node-level optimizations 包括消除不必要的和用其他成本较低的节点替换节点。 

在通用编译器中，Nop Elimination 去除了占用空间小但不指定任何操作的空操作指令。 在 DLC 中，Nop Elimination 负责消除缺少足够输入的操作。 例如可以剔除只有一个输入张量的 `sum` node ，可以剔除填充宽度为零的 `padding` node 。 

零维张量消除负责删除输入为零维张量的冗余操作。 假设A是零维张量，B是常数张量，那么A和B的求和运算节点可以用已经存在的常数节点B代替而不影响正确性。 假设C为3维张量，但一维的形状为零，如{0,2,3}，因此，C实际上没有任何元素，`argmin`/`argmax`操作节点可以去掉。

#### 4.3.2. Block-level optimizations

- **Algebraic simplification**

    代数简化优化包括 1) <u>代数识别(algebraic identification)</u>，2) <u>strength reduction</u>，我们可以用更便宜的算子替换更昂贵的算子； 3) <u>常量折叠</u>，我们可以用它们的值替换常量表达式。 这种优化考虑一系列节点，然后利用不同类型节点的交换性、结合性和分布性来简化计算。

    除了典型的算子（+、× 等）之外，代数简化还可以应用于 DL 特定算子（例如，reshape, transpose, and pooling）。 算子可以重新排序，有时甚至可以删除，这减少了冗余并提高了效率。 在这里，我们说明可以应用代数简化的常见情况：
    
    1. **计算顺序的优化**，在这种情况下，优化根据特定特征查找和删除 `reshape`/`transpose` 操作。 以矩阵乘法 (GEMM) 为例，有两个矩阵 $A$ 和 $B$，将两个矩阵转置，分别产生 $A^T$ 和 $B^T$，然后将 $A^T$ 和 $B^T$ 相乘。 然而，一种更有效的 GEMM 实现方式是交换 $A$ 和 $B$ 的顺序，相乘，然后转置 GEMM 的输出，这将两个转置减少为一个； 
    2. **节点组合优化**，这种情况下，优化将多个连续转置节点合并为一个节点，并在转置节点实际不移动数据时将转置节点优化为 `reshape` 节点； 
    3. **ReduceMean 节点的优化**，在这种情况下，如果 reduce 算子的输入是 4D 且最后两个维度要减少，则优化将 ReduceMean 替换为 AvgPool 节点（例如，在 Glow 中）。

- **Operator fusion**
  
    算子融合是 DLC 不可或缺的优化。 它可以更好地共享计算，消除中间分配，通过组合循环嵌套促进进一步优化，并减少启动和同步开销。 
    
    在 TVM 中，算子被分为四类：injective(单射), reduction(规约), complex-out-fusible(复杂可融), and opaque(不透明)。 定义了算子后，就确定了它们对应的类别。 针对以上类别，TVM设计了跨运营商的融合规则。 
    
    在 TC 中，基于自动多面体变换以不同方式执行融合。 
    
    然而，如何识别和融合更复杂的图模式，例如具有多个广播和减少节点的块，仍然是一个问题。 最近的工作[61,62]试图解决这个问题并提出一个框架来探索和优化激进的融合计划。 它不仅支持 element-wise 和 reduction 节点，还支持其他具有复杂依赖关系的计算/内存密集型节点。

- **Operator sinking**

    这种优化将转置等操作下沉到 batch normalization、ReLU、sigmoid 和channel shuffle 等操作之下。 通过这种优化，许多相似的操作彼此靠近，为代数简化创造更多机会。

#### 4.3.3. Dataflow-level optimizations

- **Common sub-expression elimination (CSE)**

    如果 E 的值是先前计算的，并且 E 的值自上次计算以来没有改变，则表达式 E 是一个**公共子表达式**。 在这种情况下，E的值被计算一次，并且可以使用已经计算出的E的值来避免在其他地方重新计算。 DLC 在整个计算图中搜索公共子表达式，并将以下公共子表达式替换为先前计算的结果。

- **Dead Code elimination**

    如果不使用其计算结果或副作用，则一组代码是死的。 并且 DCE 优化删除了死代码。 死代码通常不是程序员造成的，而是由其他图优化造成的。 因此，在其他图形优化之后应用 DCE 和 CSE。 其他优化，例如死存储消除 (DSE)，将存储删除到永远不会被使用的张量中，也属于 DCE。

- **Static memory planning**

    执行静态内存规划优化以尽可能多地重用内存缓冲区。 通常，有两种方法：**就地内存共享**和**标准内存共享**。 就地内存共享对于一个操作的输入和输出使用相同的内存，并且在计算之前只分配一份内存。 标准内存共享重用之前操作的内存而不重叠。 静态内存规划是离线完成的，这允许应用更复杂的规划算法。 最近的一项工作 [4] 首先设计并执行内存感知调度，以最小化边缘设备上的峰值激活内存占用，这为内存受限设备上的内存规划提出了新的研究方向。

- **Layout transformation**

    布局转换试图找到最佳数据布局来存储计算图中的张量，然后将布局转换节点插入到计算图中。 请注意，此处并未执行实际转换，而是在编译器后端评估计算图时执行。

    事实上，同一个操作在不同数据布局下的表现是不一样的，在不同的硬件上最好的布局也是不一样的。 例如，NCHW 格式在 GPU 上的操作通常运行得更快，因此在 GPU（例如 TensorFlow）上转换为 NCHW 格式是高效的。 一些 DLC 依靠特定于硬件的库来实现更高的性能，并且这些库可能需要特定的布局。 此外，一些 DL 加速器更喜欢更复杂的布局（例如，tile）。 此外，边缘设备通常配备异构计算单元，不同的单元可能需要不同的数据布局以更好地利用，因此布局转换需要慎重考虑。 因此，编译器需要提供一种跨各种硬件执行布局转换的方法。

    不仅张量的数据布局对最终性能有重要影响，而且转换操作也有很大的开销。 因为它们也消耗内存和计算资源。

    最近一项 based on TVM targeting on CPUs 的工作 [58] 将计算图中所有卷积操作的布局更改为 NCHW[x]c 首先，其中 c 表示通道 C 的拆分子维度，x 表示通道 C 的拆分大小 子维度。 然后，在特定于硬件的优化期间，在提供硬件详细信息（例如缓存行大小、矢量化单元大小和内存访问模式）时，通过自动调整全局探索所有 x 参数。

#### 4.3.4. Discussion

前端是 DLC 中最重要的组件之一，负责从 DL 模型到高级 IR（例如计算图）的转换以及基于高级 IR 的硬件无关优化。 尽管前端的实现在 DLC 的高级 IR 的数据表示和运算符定义方面可能有所不同，但独立于硬件的优化集中在三个级别：节点级别、块级别和数据流级别。 每个级别的优化方法利用 DL 特定和通用编译优化技术，减少计算冗余并提高 DL 模型在计算图级别的性能。

### 4.4. Backend Optimizations
DLC 的后端通常包括各种特定于硬件的优化(hardware-specific optimizations)、 自动调整技术(auto-tuning) 和 优化的内核库(optimized kernel libraries) 。 特定于硬件的优化可以为不同的硬件目标生成高效的代码。 自动调整在编译器后端中是必不可少的，可以减轻手动工作以获得最佳参数配置。 此外，高度优化的内核库也广泛用于通用处理器和其他定制化 DL 加速器。

#### 4.4.1. Hardware-specific Optimization

特定于硬件的优化，也称为目标相关优化(targetdependent optimizations)，用于获得针对特定硬件的高性能代码。 应用后端优化的一种方法是将低级 IR 转换为 LLVM IR，以利用 LLVM 基础设施生成优化的 CPU/GPU 代码。 另一种方法是使用 DL 领域知识设计定制优化，更有效地利用目标硬件。 由于特定于硬件的优化是为特定硬件量身定制的，因此无法在本文中详尽介绍，因此我们提出了5种在现有编译器中广泛采用的方法。 图 4 显示了这些特定于硬件的优化的概览

<div class="autocb" style="text-align:center;"><img src="./dlc-survey\autocb_1.png" style="zoom: 50%;box-shadow: rgba(0, 0, 0, 0.5) 10px 10px 10px; border-radius: 10px;" /></div>

详细说明如下：

1. **Hardware intrinsic mapping**

    硬件指令映射可以将一组特定的 low-level IR 指令转换为已经在硬件上高度优化的kernels。 
    
    在 **TVM** 中，硬件指令映射是通过 extensible tensorization 实现的，可以声明硬件指令的行为和指令映射的降低规则。 这种方法使编译器后端能够将硬件实现以及高度优化的手工微内核应用于特定的操作模式，从而显着提高性能。 
    
    而 **Glow** 支持硬件指令映射，例如量化。 它可以估计神经网络每个阶段可能的数值范围，并支持profileguided optimization自动进行量化。 
    
    此外，Halide/TVM 将特定的 IR 模式映射到每个体系结构上的 SIMD 操作码，以避免 LLVM IR mapping 在遇到 vector patterns 时效率低下。

2. **Memory allocation and fetching**

    内存分配是代码生成中的另一个挑战，尤其是对于 GPU 和定制加速器。 例如，GPU 主要包含共享内存空间（访问延迟较低，内存大小有限）和本地内存空间（访问延迟较高，容量大）。 这种内存层次结构需要有效的内存分配和获取技术来改善数据局部性。 
    
    为了实现这种优化，**TVM**引入了 memory scope 的调度概念。 内存范围调度原语可以将计算阶段标记为共享或线程本地。 对于标记为共享的计算阶段，TVM 生成具有共享内存分配和协作数据获取的代码，在适当的代码位置插入内存屏障以保证正确性。 
    
    此外， **TC** 还通过扩展 PPCG [97] 编译器提供了类似的功能（称为memory promotion）。 但是，TC 仅支持有限的预定义规则。 特别是，TVM 允许通过 memory scope 调度原语在加速器中启用特殊缓冲。

3. **Memory latency hiding**

    通过重排执行流水线，内存延迟隐藏也是后端使用的一项重要技术。 由于大多数 DLC 都支持 CPU 和 GPU 上的并行化，因此可以通过硬件自然地实现内存延迟隐藏（例如，GPU 上的 warp 上下文切换）。 但对于具有 decoupled access-execute (DAE) 架构的类 TPU 加速器，后端需要执行调度和细粒度同步以获得正确和高效的代码。 为了获得更好的性能并减少编程负担，TVM 引入了虚拟线程调度原语，使用户能够在虚拟化多线程架构上指定数据并行度。 然后 TVM 通过插入必要的内存屏障来降低这些虚拟并行化的线程，并将来自这些线程的操作交织到单个指令流中，从而形成每个线程更好的执行管道以隐藏内存访问延迟。

4. **Loop oriented optimizations**

    面向循环的优化也应用于后端，为目标硬件生成高效代码。 由于 Halide 和 LLVM [51]（与多面体方法集成）已经采用了此类优化技术，因此一些 DLC 在其后端利用了 Halide 和 LLVM。 面向循环的优化中应用的关键技术包括循环融合(loop fusion)、 滑动窗口(sliding window)、 循环分块(loop tiling)、 循环重新排序(loop reording) 和 循环展开(loop unrolling)
    
    1. **Loop fusion** 循环融合是一种循环优化技术，可以融合具有相同边界的循环，以实现更好的数据重用。 对于 PlaidML、TVM、TC 和 XLA 等编译器，此类优化是通过 Halide 调度或多面体方法执行的，而 Glow 通过其运算符堆栈应用循环融合。

    2. **Sliding window** 滑动窗口是 Halide 采用的一种循环优化技术。 它的核心概念是在需要时计算值并动态存储它们以供数据重用，直到不再需要它们为止。 由于滑动窗口交织两个循环的计算并使它们串行，这是并行性和数据重用之间的权衡。

    3. **Tiling** Tiling 将循环拆分为多个 tile，因此循环分为迭代遍历 tile 的外循环和迭代 tile 内部的内循环。 这种转换通过将图块放入硬件缓存来实现图块内更好的数据局部性。 由于图块的大小是特定于硬件的，因此许多 DLC 通过自动调整来确定图块模式和大小。

    4. **Loop reordering** 循环重新排序改变嵌套循环中的迭代顺序，可以优化内存访问，从而增加空间局部性。 它特定于数据布局和硬件功能。 但是，当迭代顺序存在依赖关系时，执行循环重新排序是不安全的。

    5. **Loop unrolling** 循环展开可以将特定循环展开为固定数量的循环体副本，这允许编译器应用积极的指令级并行性。 **通常，循环展开与循环拆分结合应用，它首先将循环拆分为两个嵌套循环，然后将内层循环完全展开**。


5. **Parallelization** 
   
   由于现代处理器通常支持多线程和SIMD并行，编译器后端需要利用并行性来提高硬件利用率。
   
   **Halide** 使用一个称为 `parallel` 的调度原语来指定 **线程级并行化循环** 的并行维数。并且它用一个n宽的向量语句代替了一个大小为n的循环，它可以通过硬件指令mapping 映射到硬件特定的SIMD操作码。 
   
   **Stripe** 开发了多面体模型的变体，称为嵌套多面体模型，它引入平行多面体块作为其迭代的基本执行元素。 在此扩展之后，嵌套多面体模型可以检测平铺和跨越级别之间的层次结构并行化。 此外，一些 DLC 依赖手工库，例如 Glow 或硬件供应商提供的优化数学库（在第 4.4.3 节中讨论）。 同时，Glow 将 向量化工作转移到 LLVM，因为 LLVM auto-vectorizer 在提供张量维度和循环次数信息时运行良好。 然而，完全由编译器后端利用并行性允许应用更多 DL 模型的特定领域知识，从而以更多的工程工作为代价获得更高的性能。

#### 4.4.2. Auto-tuning

由于在特定于硬件的优化中参数调整的巨大搜索空间，因此有必要利用 Auto-tuning 来确定最佳参数配置。 在本次调查研究的 DLC 中，TVM、TC 和 XLA 支持自动调整。 通常，自动调整实现包括4个关键组件，例如参数化、成本模型、搜索技术和加速。

1. **Parameterization**

    1）*Data and target*: 数据参数描述数据的规格，如输入形状。 目标参数描述了在优化调度和代码生成期间要考虑的特定于硬件的特性和约束。 例如，对于 GPU 目标，需要指定共享内存和寄存器大小等硬件参数。 
    
    2）*Optimization options*: 优化选项包括优化调度和相应的参数，例如面向循环的优化和 tile size 。 在 TVM 中，预定义和用户定义的调度以及参数都被考虑在内。 而 TC 和 XLA 更喜欢参数化优化，这与性能有很强的相关性，并且可以在以后以低成本更改。 例如，minibatch 维度是 CUDA 中通常映射到 grid dimensions 的参数之一，可以在自动调整期间进行优化。

2. **Cost model**

    应用于自动调整的不同成本模型的比较如下。 
    
    1）*Black-box model*：该模型只考虑最终执行时间，不考虑编译任务的特性。 建立黑盒模型很容易，但如果没有任务特征的指导，很容易导致更高的开销和不太理想的解决方案。 **TC** 采用这种模式。 
    
    2) *ML-based cost model*：基于机器学习的成本模型是一种使用机器学习方法预测性能的统计方法。 它使模型能够随着新配置的探索而更新，这有助于实现更高的预测准确性。 **TVM** 和 **XLA** 采用了这种模型，例如 gradient tree boosting model（GBDT）和前馈神经网络（FNN）。 
    
    3) *Pre-defined cost model*：一种基于预定义成本模型的方法期望一个建立在编译任务特征上的完美模型，并且能够评估任务的整体性能。 与基于 ML 的模型相比，预定义模型在应用时产生的计算开销较小，但需要大量的工程工作来在每个新的 DL 模型和硬件上重新构建模型。

3. **Searching technique**

    1）*Initialization and searching space determination*：初始选项可以随机设置，也可以基于已知配置，例如用户给定的配置或历史最优配置。 在搜索空间方面，需要在自整定前指定。 TVM 允许开发人员使用他们的特定领域知识指定搜索空间，并根据计算描述为每个硬件目标提供自动搜索空间提取。 相比之下，TC 依赖于编译缓存和预定义规则。 
    
    2）*Genetic algorithm*：GA将每个调整参数视为基因，将每个配置视为候选者。 新候选者根据适应度值通过交叉、变异和选择迭代生成，这是一种受自然选择过程启发的元启发式算法。 最后，导出最佳候选者。 交叉率、变异率和选择率用于控制探索和开发之间的权衡。 TC 在其自动调整技术中采用了 GA。
    
    3）*Simulated annealing algorithm*： 模拟退化算法也是一种受退火启发的元启发式算法。 它允许我们以递减的概率接受更差的解，这可以在固定的迭代次数中找到近似的全局最优并避免精确的局部最优。 TVM 在其自动调整技术中采用了 SA。 
    
    4) *Reinforcement learning (RL)*：强化学习通过学习在给定环境下通过探索和开发之间的权衡来最大化奖励。 Chameleon [5]（建立在 TVM 之上）在其自动调整技术中采用了 RLRL。

4. **Acceleration**

    1） Parallelization: 加速 Auto-tuning 的一个方向是并行化。 考虑到遗传算法需要评估每一代中的所有候选者，TC 提出了多线程、多 GPU 策略。 首先，它将候选配置排入队列并在多个 CPU 线程上编译它们。 生成的代码在 GPU 上并行评估，每个候选者都拥有父选择步骤使用的适合度。 整个评估完成后，生成新的候选，新的编译作业入队，等待CPU编译。 同样，TVM 支持交叉编译和 RPC，允许用户在本地机器上编译并在多个目标上运行具有不同自动调整配置的程序。 
    
    2） Configuration reuse: 加速 Auto-tuning 的另一个方向是重用之前的 Auto-tuning 配置。 TC 通过编译缓存存储与给定配置相对应的已知最快生成代码版本。 编译过程中每次内核优化前都会查询缓存，如果缓存未命中则触发自动调优。 类似地，TVM 会生成一个日志文件，其中存储所有调度算子的最佳配置，并在编译期间查询日志文件以获取最佳配置。 值得一提的是，TVM 对 Halide IR 中的每个算子（例如 `conv2d` ）执行自动调整，因此分别为每个算子确定最佳配置。

#### 4.4.3. Optimized Kernel Libraries

有几个高度优化的内核库被广泛用于加速各种硬件上的 DL 训练和推理。 Intel 的 DNNL（以前称为 MKL-DNN）、NVIDIA 的 cuDNN 和 AMD 的 MIOpen 是广泛使用的库。 计算密集型原语（例如，conv、GEMM 和 RNN）和内存带宽受限原语（例如，batch normalization、pooling和 shuffle ）都根据硬件特性（例如，AVX-512 ISA、张量核心）进行了高度优化。 并且支持可定制的数据布局，可以轻松集成到 DL 应用程序中，避免频繁的数据布局转换。 此外，还支持低精度训练和推理，包括 FP32、FP16、INT8 和非 IEEE 浮点格式 bfloat16 [45]。 其他定制的 DL 加速器也维护其特定的内核库 [43、57]。

现有的 DLC ，如 TVM、nGraph 和 TC，可以在代码生成期间生成对这些库的函数调用。 但是，如果 DLC 需要利用现有的优化内核库，他们应该首先将数据布局和融合样式转换为内核库中预定义的类型。 这种转换可能会破坏最优控制流。 此外，DLC 将内核库视为黑盒。 因此，他们无法在调用内核库时跨 算子 应用优化（例如，算子融合）。 总而言之，当特定高度优化的原语可以满足计算时，使用优化的内核库可以实现显着的性能提升，否则它可能会受到进一步优化的限制，并遭受不太理想的性能。

#### 4.4.4. Discussion

backend 负责基于 low-level IR 的裸机优化和代码生成。 尽管后端的设计可能因各种 low-level IR 而不同，但它们的优化可以分为硬件特定的优化：自动调整技术和优化的内核库。 这些优化可以单独执行或组合执行，以通过利用硬件/软件特性实现更好的数据局部性和并行化。 最终，DL 模型的高级 IR 被转化为在不同硬件上的高效代码实现。
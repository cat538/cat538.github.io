# IO 多路复用

## 前置知识

### 文件描述符

Linux 中一切都可以看作文件，包括普通文件、链接文件、Socket 以及设备驱动等，对其进行相关操作时，都可能会创建对应的文件描述符。文件描述符（file descriptor）是内核为了高效管理已被打开的文件所创建的索引，用于指代被打开的文件，对文件所有 I/O 操作相关的系统调用都需要通过文件描述符。

<img src="https://pic3.zhimg.com/80/v2-8f13aacb6715c1a1452440f92633a06e_720w.jpg" alt="img" style="zoom: 50%;" />

文件描述符与文件是什么关系呢？下图可以体现：

![img](https://pic1.zhimg.com/80/v2-57c1b1809b414ecf1dc38c927bc081e8_720w.jpg?source=1940ef5c)

- **进程级别的文件描述符表**：内核为每个进程维护一个文件描述符表，该表记录了文件描述符的相关信息，包括文件描述符、指向打开文件表中记录的指针。
- **系统级别的打开文件表**：内核对所有打开文件维护的一个进程共享的打开文件描述表，表中存储了处于打开状态文件的相关信息，包括文件类型、访问权限、文件操作函数(file_operations)等。
- **系统级别的 `i-node` 表**：每个文件系统都会为其中所有文件建立一个`inode`表。`i-node` 结构体记录了文件相关的信息，包括文件长度，文件所在设备，文件物理位置，创建、修改和更新时间等，`ls -i`命令可以查看文件 `i-node` 节点

进程A中文件描述符1和20都指向同一个打开的文件句柄(标号为23)，这可能是通过调用`dup`、`dup2`或`fcntl`形成的。

进程A的文件描述符2和进程B的文件描述符2都指向同一个打开的文件句柄(标号为73)，这可能是调用fork后出现的，即进程A与进程B是父子关系。

进程A的文件描述符0和进程B的文件描述符3分别指向不同的打开文件句柄，但是这些句柄均指向`inode`表中的相同条目(1976)，即指向同一文件。这可能因为每个进程各自对同一个文件发起了open调用，同一个进程两次打开同一文件，也会发生类似情况。

### 理解IO

用一个`socket`的例子来理解IO：

```c
//创建socket 
int s = socket(AF_INET, SOCK_STREAM, 0);    
//绑定 
bind(s, ...) 
//监听 
listen(s, ...) 
//接受客户端连接 
int c = accept(s, ...) 
//接收客户端数据 
recv(c, ...); 
//将数据打印出来 
printf(...) 
```

先新建 Socket 对象，依次调用`Bind`、`Listen`与`Accept`，最后调用`recv`接收数据。接下来我们只关心`recv`的过程，`recv`是如何把数据从客户端接收到进程空间中进行处理的呢？

`recv`是个**阻塞**方法，当程序运行到`Recv`时，它会一直等待，直到接收到数据才往下执行。

![20191025213749309](https://github.com/cat538/images-auto/raw/main/img/20191025213749309.png)

上图展示了数据接收的全过程，进程在`Recv`阻塞期间处于阻塞态，不在就绪队列中：

1. 计算机收到了对端传送的数据(步骤 ①)
2. 数据经由网卡传送到内存，由DMA等执行，不需要CPU参与(步骤 ②)
3. 然后网卡通过**中断信号通知 CPU** 有数据到达，CPU 执行**中断程序**(步骤 ③)

此处的**中断程序**主要有两项功能：

1. 将网络数据写入到对应 Socket 的接收缓冲区里面(步骤 ④)
2. 唤醒进程 A(步骤 ⑤)，重新将进程 A 放入工作队列中

可以看到，如果按照上述过程，则一个进程(线程)同时只能监视一个socket，因为在`recv`把进程阻塞，直到该socket可用。试想如果服务程序就是这样一个单进程，面对到来的1000个连接，只考虑`recv`，服务器只能按到达顺序，调用1000次`recv`依次接收数据，则必然会导致饥饿的问题(可能最后一个连接只传输很少的数据，但是被前面大量连接阻塞)。总结以下三点：

1. 进程在 `recv` 的时候大概率会被阻塞掉，导致一次进程切换
2. 当连接上数据就绪的时候进程又会被唤醒，又是一次进程切换
3. 一个进程同时只能等待一条连接，如果有很多并发，则需要很多进程

有没有一种方式能够让进程同时监视多个socket呢？

其实我们可以把`read`(即`recv`)设置为非阻塞，即如果文件描述符处于ready，则返回之，否则返回一个错误值；这样一来，我们就可以在代码里把所有描述符放到一个数组理，写一个循环使用`read`不断轮询这个数组。这样我们就实现了一个进程监视多个socket。

```c
int fd_set[1000];
//...
while(1){
    for(int fd : fd_set){
        if(read(fd)){
            //对文件描述符操作
        } else{ continue; }
    }
}
```

问题似乎得到了解决，但是这种方式存在严重的效率问题，因为`read`是一个系统调用，每次调用`read`都产生一次OS在用户态和内核态切换的开销。

假如我们能够向内核预先传入一个 socket 列表(而不是1个socket)，内核帮我们遍历这个列表，如果列表中的 socket 都没有数据，挂起进程，直到有任何一个socket 收到数据，唤醒进程，开销就很小了——这就是系统调用`select`的做法。

![20191025214848328](https://github.com/cat538/images-auto/raw/main/img/20191025214848328.png)

## select

接口如下：

```c
#include <sys/select.h>
/** 
 * 遍历检查n个文件描述符，是否处于IO就绪状态(可读或可写)，并把符合条件的文件描述符(比如一个套接字)
 * 加到相应(读、写、异常)集合中(即设置相应fdset的相应bit为1，不符合条件设置为0)。调用者负责检查集合
 *
 * @param n		    使用时记住这个值为待检测的最大文件描述符加1即可；
 * @param readfds	指定了被读监控的文件描述符集；
 * @param writefds	指定了被写监控的文件描述符集；
 * @param exceptfds	指定了被例外条件监控的文件描述符集；
 * @param timeout	很常用的一个timeeval结构，起定时器的作用：到指定的时间，无论是否有fd准备好，都返回调用
 */
int select(int maxfd, fd_set *readfds, fd_set *writefds, fe_set *exceptfds, const struct timeval *timeout);
```

详细的解释：

- 测试指定的`fd`(可以是很多个`fd`)可读？可写？有异常条件待处理？  
- `maxfd`：需要检查的文件描述符的个数（即检查到`fd_set`的第几位），数值应该比三组`fd_set`中所含的最大`fd`值更大，一般设为三组`fd_set`中所含的最大`fd`值加1（如在`readset`,`writeset`,`exceptset`中所含最大的`fd`为5，则`nfds=6`，因为`fd`是从0开始的）。这个值是为提高效率，使函数不必检查`fd_set`的所有1024位。
- `readfs`：`fd_set* readfds`是指向`fd_set`结构的指针，这个集合中应该包括文件描述符，我们是要监视这些文件描述符的**读**变化的，即我们关心是否可以从这些文件中读取数据了，如果这个集合中有一个文件可读，`select`就会返回一个大于0的值，表示有文件可读，如果没有可读的文件，则根据timeout参数再判断是否超时，若超出timeout的时间，`select`返回0，若发生错误返回负值。可以传入`NULL`值，表示不关心任何文件的读变化。
- `timeout`：限定了`select`的阻塞上限，即最久轮询时间；如果要非阻塞，则设置为0即可
- 返回值：返回对应位为1的`fd`的总数

与之相关的几个宏：

```c
void FD_SET(int fd, fd_set *fdset);//设置文件描述符集fdset中对应于文件描述符fd的位(设置为1)，即将fd加入fdset集的操作；
void FD_CLR(int fd, fd_set *fdset);//清除文件描述符集fdset中对应于文件描述符fd的位(设置为0)，与上相反操作；
void FD_ISSET(int fd, fd_set *fdset);//检测文件描述符集fdset中对应于文件描述符fd的位是否被设置,如果置1，就进行相应操作；
void FD_ZERO(fd_set *fdset);//清除文件描述符集fdset中的所有位(既把所有位都设置为0)
```

使用`select`的过程一般是：

先调用宏FD_ZERO将指定的`fd_set`清零，然后调用宏FD_SET将需要测试的`fd`加入`fd_set`(即把对应位置1)，接着调用函数select测试`fd_set`中的所有`fd`(如果`fd`符合条件，则不会被置0，否则会被置0)，最后用宏FD_ISSET检查某个`fd`在函数select调用后，相应位是否仍然为1。

> **注意：**上述过程中省略了一个问题，即内核是如何得知**如果这个集合中有一个文件可读**这一事件的呢？在`select`中，事实上内核只是简单的轮询所有描述符，看起是否处于可读状态，因此`select`不会让出CPU！！！

再一次总结`select`解决同时监视多个socket问题的思想：我们构建一个兴趣列表(即添加了描述符的`fd_set`)包含要监视的所有socket，然后通过`select`向内核传入这个socket列表，如果列表中的socket都没有数据，挂起进程，直到有一个socket收到数据，**中断程序**将唤起进程。可以看到，这种方式，既做到了一个线程处理多个客户端连接socket，又减少了系统调用的开销（多个socket只有一次 select 的系统调用 + n 次就绪状态的文件描述符的 read 系统调用）。但是注意到`select`的三个限制：

- 每次调用`select`，都需要把`fd_set`从用户态拷贝到内核态，这个开销在`fd`很多时会很大
- `select`在内核层仍然是通过遍历的方式检查文件描述符的就绪状态，是个同步过程，只不过无系统调用切换上下文的开销(相比于我们把`read`设置为非阻塞，然后在用户层不断轮循多个描述符)。
- 对于`select`每次的返回值(如果不为0)，我们都需要遍历从内核返回来的`fd_set`，这个开销在`fd`很多时很大
- `select`只支持有限文件描述符数量，默认是1024

基于以上不足，Linux2.6之后提供的另一个解决方案`epoll`针对上述缺点进行了改进，性能更好。

![epoll](https://github.com/cat538/images-auto/raw/main/img/epoll.jpg)

## epoll

参考 [epoll](./epoll.md)

## event-driven

I/O多路复用可以用做并发事件驱动(event-driven)程序的基础，在事件驱动程序中，某些事件会导致流向前推进。一般的思路是将逻辑流模型化为状态机。不严格地说，一个状态机就是一组状态、输入事件和转移。其中转移是将状态和输人事件映射到状态。每个转移是将一个(输入状态，输人事件)对映射到一个输出状态。自循环(self-loop)是同一输入和输出状态之间的转移。通常把状态机画成有向图，其中节点表示状态，有向弧表示转移，而弧上的标号表示输入事件。一个状态机从某种初始状态开始执行。每个输入事件都会引发一个从当前状态到下一状态的转移。

对于每个新的客户端$k$，基于I/O多路复用的并发服务器会创建一个新的状态机$s_k$，并将它和已连接描述符$d_k$联系起来。如图12-7所示，每个状态机$s_k$,都有一个状态(“等待描述符$d$,准备好可读”)、一个输人事件(“描述符$d_k$,准备好可以读了”)和一个转移(“从描述符$d_k$,读一个文本行”)。

<img src="https://github.com/cat538/images-auto/raw/main/img/image-20220114015925864.png" style="zoom: 80%;" />

服务器使用I/О多路复用，借助`select`函数检测输入事件的发生。当每个已连接描述符准备好可读时，服务器就为相应的状态机执行转移，在这里就是从描述符读和写回一个文本行。

图`12-8`展示了一个基于I/O多路复用的并发事件驱动服务器的完整示例代码。一个`pool`结构里维护着活动客户端的集合(第3～11行)。在调用`init_pool`初始化池(第27行)之后，服务器进人一个无限循环。在循环的每次迭代中，服务器调用select函数来检测两种不同类型的输入事件:

- a)来自一个新客户端的连接请求到达，
- b)一个已存在的客户端的已连接描述符准备好可以读了。

当一个连接请求到达时(第35行)，服务器打开连接(第37行)，并调用`add_client`函数，将该客户端添加到池里(第38行)。最后，服务器调用`check_clients`函数，把来自每个准备好的已连接描述符的一个文本行回送回去(第42行)。

![image-20220114020229654](https://github.com/cat538/images-auto/raw/main/img/image-20220114020229654.png)![image-20220114020311644](https://github.com/cat538/images-auto/raw/main/img/image-20220114020311644.png)



## IO多路复用 v.s. 多线程

IO多路复用的优势在于，当处理的消耗对比IO几乎可以忽略不计时，可以处理大量的并发IO，而不用消耗太多CPU/内存。对比多线程，省去了线程切换的开销，以及每个线程单独维护的栈资源等。这就像是一个工作很高效的人，手上一个todo list，他高效的依次处理每个任务。这比每个任务单独安排一个人要节省（雇人是要发工资的……）。典型的例子是`nginx`做代理，代理的转发逻辑相对比较简单直接，那么IO多路复用很适合。相反，如果是一个做复杂计算的场景，计算本身可能是个复杂的东西，IO不是瓶颈。那么怎么充分利用CPU或者显卡的核心多干活才是关键。

